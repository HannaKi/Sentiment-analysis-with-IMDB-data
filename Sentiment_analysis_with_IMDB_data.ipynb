{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_with_IMDB_data.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Sentiment-analysis-with-IMDB-data/blob/main/Sentiment_analysis_with_IMDB_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oSt2eIvPfYW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\n",
        "\n",
        "from pprint import pprint\n",
        "# from time import time\n",
        "# import logging"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeF_rZZMRRUX"
      },
      "source": [
        "## Load and inspect data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYMQPYZ5jUM0"
      },
      "source": [
        "%%bash\n",
        "wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1QE3NTLmAHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c5acff-78a2-4d06-f6aa-e964935a7b9e"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb\n",
        "cat README | head -1000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Large Movie Review Dataset v1.0\n",
            "\n",
            "Overview\n",
            "\n",
            "This dataset contains movie reviews along with their associated binary\n",
            "sentiment polarity labels. It is intended to serve as a benchmark for\n",
            "sentiment classification. This document outlines how the dataset was\n",
            "gathered, and how to use the files provided. \n",
            "\n",
            "Dataset \n",
            "\n",
            "The core dataset contains 50,000 reviews split evenly into 25k train\n",
            "and 25k test sets. The overall distribution of labels is balanced (25k\n",
            "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
            "documents for unsupervised learning. \n",
            "\n",
            "In the entire collection, no more than 30 reviews are allowed for any\n",
            "given movie because reviews for the same movie tend to have correlated\n",
            "ratings. Further, the train and test sets contain a disjoint set of\n",
            "movies, so no significant performance is obtained by memorizing\n",
            "movie-unique terms and their associated with observed labels.  In the\n",
            "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
            "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
            "more neutral ratings are not included in the train/test sets. In the\n",
            "unsupervised set, reviews of any rating are included and there are an\n",
            "even number of reviews > 5 and <= 5.\n",
            "\n",
            "Files\n",
            "\n",
            "There are two top-level directories [train/, test/] corresponding to\n",
            "the training and test sets. Each contains [pos/, neg/] directories for\n",
            "the reviews with binary labels positive and negative. Within these\n",
            "directories, reviews are stored in text files named following the\n",
            "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
            "the star rating for that review on a 1-10 scale. For example, the file\n",
            "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
            "example with unique id 200 and star rating 8/10 from IMDb. The\n",
            "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
            "omitted for this portion of the dataset.\n",
            "\n",
            "We also include the IMDb URLs for each review in a separate\n",
            "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
            "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
            "are unable to link directly to the review, but only to the movie's\n",
            "review page.\n",
            "\n",
            "In addition to the review text files, we include already-tokenized bag\n",
            "of words (BoW) features that were used in our experiments. These \n",
            "are stored in .feat files in the train/test directories. Each .feat\n",
            "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
            "data.  The feature indices in these files start from 0, and the text\n",
            "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
            "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
            "(the) appears 7 times in that review.\n",
            "\n",
            "LIBSVM page for details on .feat file format:\n",
            "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
            "\n",
            "We also include [imdbEr.txt] which contains the expected rating for\n",
            "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
            "rating is a good way to get a sense for the average polarity of a word\n",
            "in the dataset.\n",
            "\n",
            "Citing the dataset\n",
            "\n",
            "When using this dataset please cite our ACL 2011 paper which\n",
            "introduces it. This paper also contains classification results which\n",
            "you may want to compare against.\n",
            "\n",
            "\n",
            "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "  month     = {June},\n",
            "  year      = {2011},\n",
            "  address   = {Portland, Oregon, USA},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "  pages     = {142--150},\n",
            "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "}\n",
            "\n",
            "References\n",
            "\n",
            "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
            "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
            "636-659.\n",
            "\n",
            "Contact\n",
            "\n",
            "For questions/comments/corrections please contact Andrew Maas\n",
            "amaas@cs.stanford.edu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSPALgVntPKT"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb/test/neg\n",
        "for f in *txt; do echo $f>> /content/neg_file_names.txt; done # appends filenames to file"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM3iXWLB6qwj"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb/test/pos\n",
        "for f in *txt; do echo $f>> /content/pos_file_names.txt; done # appends filenames to file"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1wqR79f0woZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d9721a-c9e5-4b16-b18b-9321887dc436"
      },
      "source": [
        "def open_file_by_looping(path, file):\n",
        "  f=open(file)\n",
        "  filenames = f.readlines()\n",
        "  reviews = []\n",
        "  for filename in filenames:\n",
        "    fname = path + filename\n",
        "    fname =fname.rstrip()\n",
        "    f = open(fname)\n",
        "    review = f.readlines()\n",
        "    # print(type(review))\n",
        "    reviews.append(review) # or extend?\n",
        "    f.close()\n",
        "  f.close()\n",
        "  flat_list = [item for sublist in reviews for item in sublist] # we have list of lists, it needs to be flattened\n",
        "  return flat_list\n",
        "\n",
        "neg_reviews = open_file_by_looping(\"aclImdb/test/neg/\", \"neg_file_names.txt\")\n",
        "print(\"Number of negative reviews:\", len(neg_reviews))\n",
        "\n",
        "pos_reviews = open_file_by_looping(\"aclImdb/test/pos/\", \"pos_file_names.txt\")\n",
        "print(\"Number of positive reviews:\", len(pos_reviews))\n",
        "\n",
        "reviews=neg_reviews+pos_reviews\n",
        "len(reviews)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of negative reviews: 12500\n",
            "Number of positive reviews: 12500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkqVNVyfVZ9L"
      },
      "source": [
        "Data quality affects the performance of all machine learning algorithms and neural networks. Poor data can not be improved even with a sophisticated algorithm. In this case our data is balanced (classes have equal 50 % share) and well behaving in many aspects.\n",
        "\n",
        "This is seldom true in real life applications. In these cases data balance must be taken care of with for example stratification or giving different weigths to different classes. If data is grouped or datarecords are not independent, even more caution should be given to the training process since this might lead to test data \"leaking\" into training data and thus highly optimistic model performance measeures. \n",
        "\n",
        "For these reasons one should familiarize her with a new dataset before rushing into further steps of modeling. If it is discovered that the data is imbalanced, grouped etc. we can fix the issues uprising from the nature of the data before we feed it to the algorithm, or at least take it in account when analysing the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSktZXAh7QMt",
        "outputId": "2e98f004-5796-43f9-9034-fb03637aa5b4"
      },
      "source": [
        "reviews=neg_reviews+pos_reviews\n",
        "len(reviews)\n",
        "\n",
        "# make labels for the reviews:\n",
        "labels = ['neg']*len(neg_reviews) + ['pos']*len(pos_reviews)\n",
        "print(len(labels))\n",
        "\n",
        "# make shuffled indices and shuffle both of labels and reviews with them\n",
        "indices = list(range(len(labels)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "labels = [labels[index] for index in indices]\n",
        "reviews = [reviews[index] for index in indices]\n",
        "\n",
        "for label, text in zip(labels[:10], reviews[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "label: pos \n",
            "text: 'Captain Corelli's Mandolin' is a fantastic film in itself. It is nothing like the book, which may disapoint its ardent followers. Yet, viewed on it's own, the film is a masterpiece. The views are spectacular and the acting isn't too bad either!! Nicolas Cage was brilliant-so different from his usual action hero type characters. Penelope cruz is superb and really holds the film together. I think that this film has to be judged as an indivdual project-not related to the book. Louis de Bernieres gave up rights to the film script, so the film is an interpretation of the director, john maddon. Go and see this film with an open mind-you'll love it; because underneath is the touching story of love and war. \n",
            "\n",
            "label: neg \n",
            "text: When I saw 6.0 on IMDb, I was rather impressed and excited to watch this movie, as a 6 for a horror movie should be rather entertaining. At first I thought it was going to be some disturbing, unseen evil force (having not read the book) to terrify the audience -- but it turns out to be something rather mundane -- killer plants. Regardless, I am a rather open-minded individual when it comes to movies so I thought perhaps the movie would bring some kind of breakthrough spin on carnivorous vines.<br /><br />Unfortunately, it failed to meet my expectations due to the excessive amounts of cliché and lack of any originality. To top it off, the female lead character continues to annoy you off with her stupidity. Unless the movie is intentionally a bad B-rated movie that is entertaining in the hilarity of badness, no movie should ever ever ever have a main character irritate you if one expects the audience to care about the character. Such roles should be reserved for secondary characters. Characters were undeveloped, the monsters (plants in this case) left unexplained, and clichés were dripping all over.<br /><br />The only thing that is mildly effective are some of the bloody/gory scenes, although the gore pales in comparison with movies like High Tension or Ichi The Killer. Consistently failed logic (such as why would a character not watching the top of a rope during a second attempt at descending into the ruins when it just snapped and almost killed someone), even if minor, adds up and just continues to anger the audience. The movie could've saved itself by using characters or some kind of story device to reprimand or \"redeem\" idiot characters who just did something stupid (or at least let the character recognize or regret her own mistakes). But to continue to allow idiocy to preside will certainly cause the audience to abandon all care for the character, in turn taking away the terror of the movie.<br /><br />Overall this is a poorly done movie. An example of a well done movie involving pretty twenty-something's getting killed is the Texas Chainsaw Massacre remake (and the prequel too) that certainly instilled fear and had much less character logic flaws. In summary, if you have a lot of time to kill, go watch it if there's nothing else. Otherwise, don't waste your time with this sub-par flick and go see something actually scary and highly satisfying like The Hills Have Eyes remake. \n",
            "\n",
            "label: neg \n",
            "text: I just can't imagine any possible reasons why Madsen and Hopper wanted to be in this movie after reading the script. They got blackmailed maybe? Or are they that badly out of money? The main problem with the movie is that it's boring. The conversations between Madsen's and Hopper's character are pointless, just like the bored chatting between two buddies while drinking beer on a saturday night. You never feel for any of the characters (although Madsen's psycho killer is very likeable, comparing to the other characters). Hopper always was a good actor, and Madsen does a fine job as the serial killer, otherwise the acting is almost laughable. There are about three scenes in the whole movie where something is actually happening, each of them last about three minutes. Although the \"talking and thinking about murder and the nature of murderers\" scenes would have been interesting, if they were scenes in a book. The whole concept would've been interesting for a novel, but a movie just can't bare with a story with that much inner thinking and so little action. \n",
            "\n",
            "label: neg \n",
            "text: I thought it will be a Ok movie after seeing the commercials about it. It was funny at some parts and some very nasty. The only person I felt sorry for is Horatio Sans who got a hot wife who is cheating on him with other women. But he never got a chance to have a threesome with until the and that was good but they should have made more bigger thru out the film. \n",
            "\n",
            "label: pos \n",
            "text: Between sweeping, extraordinary scenes within a plethora of Indian locations and a plot which is Bollywood inspired yet grounded in reality, this highly moving film is a must-see for those who love a good romance/adventure. And by this, I mean \"good\", not a Hollywood or Bollywood easy and contrived happy ending. East meets West in a different take on the buddy film. The performances are inspired and brilliant, the cinematography epic and the plot entertaining, engaging and poignant without a trace of schmaltz or cheap tugging at heartstrings. HARI OM is a sweet film that won't rot your teeth. See it! \n",
            "\n",
            "label: pos \n",
            "text: Good movie, all elements of a good movie was there, story, actors, script, and direction. I was on the edge of my seat the whole time.<br /><br />No question about it, is a low budget film, but I liked it more than many big budget films.<br /><br />Andres Bagg plays Martin Sanders, who is dealing with his unfaithful wife. Then a voice in the telephone and then just fear.<br /><br />Virginia Lustig is beautiful and brings a powerful performance. She is an excellent part to the film.<br /><br />I liked the increasing ambiguity near the end, even though we know that the main character can be involved, we continued seeing everything from his point of view and asking: Who is the killer? \n",
            "\n",
            "label: pos \n",
            "text: I'm an opera buff, and operas are full of sex, blood and death. It may help to know the librettos of the operas the arias are from to really appreciate this film -- my mileage is very different than Tug-3. I am a classical music lover, and I liked this film.<br /><br />I loved Ken Russell's \"Nessun Dorma\" segment, and would actually like to see him produce Turandot, because opera is supposed to be overwhelming, truly multi-media experience , but then I loved Lisztomania. I love *Turandot* and knowing the libretto so well may be why I don't find this segment the travesty that Tug-3 did.<br /><br />The Buck Henry/ Rigoletto segment is probably the most approachable for the average viewer -- they are likely to recognize the tunes, and its a classic bedroom farce. I like bedroom farces, so the silliness didn't upset me.<br /><br />The \"Liebestod\" segment is so outstanding that I recommend people watch this for that piece alone. \"Depuis la Jour\" was, for me, beautifully spiritual. And the Caruso recording of \"Vesti la Giubba\" (aka I Pagliacci) with John Hurt as the clown was wonderful. But people just wanting naked women may feel there is too much music and not enough bare flesh and sex. \n",
            "\n",
            "label: neg \n",
            "text: The worst movie ever made. If anyone asks you what is the worst movie you've ever seen - tell them Plump Fiction. Of all the movies I've ever seen this gotta be the most lame experience. Even the poorest sequels are pure masterpieces compared \n",
            "\n",
            "label: pos \n",
            "text: A group of young travelers that just ran out of gas go into a weird wax museum called \"Saluesen's Lost Oasis\" owned by a strange man named Slausen (Chuck Conners) as the dummies are controlled by some mysterious force and a madman with special powers wants them dead.<br /><br />One of the most under-appreciated horror movies of the late 70's! This Charles Band (producer of \"Re-Animator\")production has became one of the scariest and most unique low budget horror productions of it's day combining some psychological themes along without having to result some gore like the usual slasher movie. The movie keeps the viewer on the edge of their seats with tension and some scares, the movie has became a cult diamond in the rough for the genre since then and this is well worth watching.<br /><br />Also recommended: \"Pin\", \"The Texas Chainsaw Massacre ( 1974)\", \"The Hills have Eyes ( 1977)\", \"Maniac ( 1980)\", \"Magic\" ( 1978), \"Dolls\", \"May\", \"Just Before Dawn\", \"House of 1000 Corpses\", \"The Devil's Rejects\", \"Sleepaway Camp\", \"Mother's Day\", \"A Nightmare on Elm Street\", \"Friday The 13th\", \"Halloween 1 & 2\", \"Puppet Master\", \"House of Wax ( 1953 and 2005)\", \"Jeepers Creepers\", \"High Tension\", \"Evil Dead II\", \"From Dusk Till Dawn\", \"Waxwork\", \"Nothing But Trouble\" and \"Psycho ( 1960)\". \n",
            "\n",
            "label: pos \n",
            "text: I thought the movie started out a bit slow and disjointed for the first hour. However, it became more absorbing, fascinating, and surprising in its last two hours. So, while it starts out like a cheap horror film, it evolves into a beautiful and wonderful fantasy film.<br /><br />Bridget Fonda stands out as the Snow Queen. This was her best performance and it is sad that this apparently was her last performance, as she has not acted in the last 7 years. She absolutely personifies both the beauty and coldness of Winter.<br /><br />My daughter, age 14, found the film a bit frightening, so if you are showing it as family entertainment, please stay with your child and reassure her or him that it is just a fairy tale fantasy and not to take it too seriously.<br /><br />It is really one of the best fantasy films that I have seen in a long time, slightly better than \"Eragon\" or any of the \"Lord of the Rings.\" It is about as good as \"The Golden Compass\". \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XCLyD7zNxoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f632f3e6-bbec-4c46-f9d7-90523ee9f2e1"
      },
      "source": [
        "# Remove HTML tags\n",
        "\n",
        "import re\n",
        "pattern1=r\"<br /><br />\" \n",
        "reviews = [re.sub(pattern1, \" \", item) for item in reviews]\n",
        "\n",
        "for label, text in zip(labels[:10], reviews[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: pos \n",
            "text: 'Captain Corelli's Mandolin' is a fantastic film in itself. It is nothing like the book, which may disapoint its ardent followers. Yet, viewed on it's own, the film is a masterpiece. The views are spectacular and the acting isn't too bad either!! Nicolas Cage was brilliant-so different from his usual action hero type characters. Penelope cruz is superb and really holds the film together. I think that this film has to be judged as an indivdual project-not related to the book. Louis de Bernieres gave up rights to the film script, so the film is an interpretation of the director, john maddon. Go and see this film with an open mind-you'll love it; because underneath is the touching story of love and war. \n",
            "\n",
            "label: neg \n",
            "text: When I saw 6.0 on IMDb, I was rather impressed and excited to watch this movie, as a 6 for a horror movie should be rather entertaining. At first I thought it was going to be some disturbing, unseen evil force (having not read the book) to terrify the audience -- but it turns out to be something rather mundane -- killer plants. Regardless, I am a rather open-minded individual when it comes to movies so I thought perhaps the movie would bring some kind of breakthrough spin on carnivorous vines. Unfortunately, it failed to meet my expectations due to the excessive amounts of cliché and lack of any originality. To top it off, the female lead character continues to annoy you off with her stupidity. Unless the movie is intentionally a bad B-rated movie that is entertaining in the hilarity of badness, no movie should ever ever ever have a main character irritate you if one expects the audience to care about the character. Such roles should be reserved for secondary characters. Characters were undeveloped, the monsters (plants in this case) left unexplained, and clichés were dripping all over. The only thing that is mildly effective are some of the bloody/gory scenes, although the gore pales in comparison with movies like High Tension or Ichi The Killer. Consistently failed logic (such as why would a character not watching the top of a rope during a second attempt at descending into the ruins when it just snapped and almost killed someone), even if minor, adds up and just continues to anger the audience. The movie could've saved itself by using characters or some kind of story device to reprimand or \"redeem\" idiot characters who just did something stupid (or at least let the character recognize or regret her own mistakes). But to continue to allow idiocy to preside will certainly cause the audience to abandon all care for the character, in turn taking away the terror of the movie. Overall this is a poorly done movie. An example of a well done movie involving pretty twenty-something's getting killed is the Texas Chainsaw Massacre remake (and the prequel too) that certainly instilled fear and had much less character logic flaws. In summary, if you have a lot of time to kill, go watch it if there's nothing else. Otherwise, don't waste your time with this sub-par flick and go see something actually scary and highly satisfying like The Hills Have Eyes remake. \n",
            "\n",
            "label: neg \n",
            "text: I just can't imagine any possible reasons why Madsen and Hopper wanted to be in this movie after reading the script. They got blackmailed maybe? Or are they that badly out of money? The main problem with the movie is that it's boring. The conversations between Madsen's and Hopper's character are pointless, just like the bored chatting between two buddies while drinking beer on a saturday night. You never feel for any of the characters (although Madsen's psycho killer is very likeable, comparing to the other characters). Hopper always was a good actor, and Madsen does a fine job as the serial killer, otherwise the acting is almost laughable. There are about three scenes in the whole movie where something is actually happening, each of them last about three minutes. Although the \"talking and thinking about murder and the nature of murderers\" scenes would have been interesting, if they were scenes in a book. The whole concept would've been interesting for a novel, but a movie just can't bare with a story with that much inner thinking and so little action. \n",
            "\n",
            "label: neg \n",
            "text: I thought it will be a Ok movie after seeing the commercials about it. It was funny at some parts and some very nasty. The only person I felt sorry for is Horatio Sans who got a hot wife who is cheating on him with other women. But he never got a chance to have a threesome with until the and that was good but they should have made more bigger thru out the film. \n",
            "\n",
            "label: pos \n",
            "text: Between sweeping, extraordinary scenes within a plethora of Indian locations and a plot which is Bollywood inspired yet grounded in reality, this highly moving film is a must-see for those who love a good romance/adventure. And by this, I mean \"good\", not a Hollywood or Bollywood easy and contrived happy ending. East meets West in a different take on the buddy film. The performances are inspired and brilliant, the cinematography epic and the plot entertaining, engaging and poignant without a trace of schmaltz or cheap tugging at heartstrings. HARI OM is a sweet film that won't rot your teeth. See it! \n",
            "\n",
            "label: pos \n",
            "text: Good movie, all elements of a good movie was there, story, actors, script, and direction. I was on the edge of my seat the whole time. No question about it, is a low budget film, but I liked it more than many big budget films. Andres Bagg plays Martin Sanders, who is dealing with his unfaithful wife. Then a voice in the telephone and then just fear. Virginia Lustig is beautiful and brings a powerful performance. She is an excellent part to the film. I liked the increasing ambiguity near the end, even though we know that the main character can be involved, we continued seeing everything from his point of view and asking: Who is the killer? \n",
            "\n",
            "label: pos \n",
            "text: I'm an opera buff, and operas are full of sex, blood and death. It may help to know the librettos of the operas the arias are from to really appreciate this film -- my mileage is very different than Tug-3. I am a classical music lover, and I liked this film. I loved Ken Russell's \"Nessun Dorma\" segment, and would actually like to see him produce Turandot, because opera is supposed to be overwhelming, truly multi-media experience , but then I loved Lisztomania. I love *Turandot* and knowing the libretto so well may be why I don't find this segment the travesty that Tug-3 did. The Buck Henry/ Rigoletto segment is probably the most approachable for the average viewer -- they are likely to recognize the tunes, and its a classic bedroom farce. I like bedroom farces, so the silliness didn't upset me. The \"Liebestod\" segment is so outstanding that I recommend people watch this for that piece alone. \"Depuis la Jour\" was, for me, beautifully spiritual. And the Caruso recording of \"Vesti la Giubba\" (aka I Pagliacci) with John Hurt as the clown was wonderful. But people just wanting naked women may feel there is too much music and not enough bare flesh and sex. \n",
            "\n",
            "label: neg \n",
            "text: The worst movie ever made. If anyone asks you what is the worst movie you've ever seen - tell them Plump Fiction. Of all the movies I've ever seen this gotta be the most lame experience. Even the poorest sequels are pure masterpieces compared \n",
            "\n",
            "label: pos \n",
            "text: A group of young travelers that just ran out of gas go into a weird wax museum called \"Saluesen's Lost Oasis\" owned by a strange man named Slausen (Chuck Conners) as the dummies are controlled by some mysterious force and a madman with special powers wants them dead. One of the most under-appreciated horror movies of the late 70's! This Charles Band (producer of \"Re-Animator\")production has became one of the scariest and most unique low budget horror productions of it's day combining some psychological themes along without having to result some gore like the usual slasher movie. The movie keeps the viewer on the edge of their seats with tension and some scares, the movie has became a cult diamond in the rough for the genre since then and this is well worth watching. Also recommended: \"Pin\", \"The Texas Chainsaw Massacre ( 1974)\", \"The Hills have Eyes ( 1977)\", \"Maniac ( 1980)\", \"Magic\" ( 1978), \"Dolls\", \"May\", \"Just Before Dawn\", \"House of 1000 Corpses\", \"The Devil's Rejects\", \"Sleepaway Camp\", \"Mother's Day\", \"A Nightmare on Elm Street\", \"Friday The 13th\", \"Halloween 1 & 2\", \"Puppet Master\", \"House of Wax ( 1953 and 2005)\", \"Jeepers Creepers\", \"High Tension\", \"Evil Dead II\", \"From Dusk Till Dawn\", \"Waxwork\", \"Nothing But Trouble\" and \"Psycho ( 1960)\". \n",
            "\n",
            "label: pos \n",
            "text: I thought the movie started out a bit slow and disjointed for the first hour. However, it became more absorbing, fascinating, and surprising in its last two hours. So, while it starts out like a cheap horror film, it evolves into a beautiful and wonderful fantasy film. Bridget Fonda stands out as the Snow Queen. This was her best performance and it is sad that this apparently was her last performance, as she has not acted in the last 7 years. She absolutely personifies both the beauty and coldness of Winter. My daughter, age 14, found the film a bit frightening, so if you are showing it as family entertainment, please stay with your child and reassure her or him that it is just a fairy tale fantasy and not to take it too seriously. It is really one of the best fantasy films that I have seen in a long time, slightly better than \"Eragon\" or any of the \"Lord of the Rings.\" It is about as good as \"The Golden Compass\". \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-bnE6jxnusp"
      },
      "source": [
        "# s = r\"\\s\\tWord\"\n",
        "# prog = re.compile(s)\n",
        "# prog\n",
        "\n",
        "# re.sub(some_regex, some_replacement.replace('\\\\', '\\\\\\\\'), input_string)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B9giOL-MsY9"
      },
      "source": [
        "# import re\n",
        "# pattern1=r\"<br /><br />\" # something fishy is going on with HTML tags. Get rid of them\n",
        "# s = \"\\\\'\"\n",
        "# print(s)\n",
        "# # pattern2=re.escape(r\"\\'\")\n",
        "# # print(\"p2:\", pattern2)\n",
        "# pattern3 = s\n",
        "# print(pattern3)\n",
        "# # print(pattern3.replace('\\\\', '\\\\\\\\'))\n",
        "# print(chr(39))\n",
        "\n",
        "# fixed = re.sub(pattern3, chr(39), reviews[8])\n",
        "# fixed_n = re.sub(pattern1, \" \", fixed)\n",
        "# fixed_n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ-yYOydG00p"
      },
      "source": [
        "## UDPipe segmenting (is this needed?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov90X2iQErxH"
      },
      "source": [
        "Let's try to tokenize and sentence split the IMDB data with UDPipe machine learned segmenter!\n",
        "Documentation: https://ufal.mff.cuni.cz/udpipe/users-manual\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq9agXTJAzmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b12649b-eeb7-430d-8ab3-b45cd97b90d9"
      },
      "source": [
        "!wget -nc https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "\n",
        "!pip3 install ufal.udpipe\n",
        "\n",
        "import ufal.udpipe as udpipe\n",
        "\n",
        "model = udpipe.Model.load(\"en.segmenter.udpipe\")\n",
        "pipeline = udpipe.Pipeline(model,\"tokenize\",\"none\",\"none\",\"horizontal\") # horizontal: returns one sentence per line, with words separated by a single space\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-01 15:14:47--  https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/TurkuNLP/intro-to-nlp/master/Data/en.segmenter.udpipe [following]\n",
            "--2021-04-01 15:14:47--  https://raw.githubusercontent.com/TurkuNLP/intro-to-nlp/master/Data/en.segmenter.udpipe\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17394186 (17M) [application/octet-stream]\n",
            "Saving to: ‘en.segmenter.udpipe’\n",
            "\n",
            "en.segmenter.udpipe 100%[===================>]  16.59M  37.5MB/s    in 0.4s    \n",
            "\n",
            "2021-04-01 15:14:48 (37.5 MB/s) - ‘en.segmenter.udpipe’ saved [17394186/17394186]\n",
            "\n",
            "Collecting ufal.udpipe\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 8.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626612 sha256=cf33a9c9716b45951462a00ccce6c40897a4bdd648ce1c0a2b6f1cc13637fc5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe\n",
            "Successfully installed ufal.udpipe-1.2.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiB-UB2zFWxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772e33cb-d10c-4c66-ae27-58cec8129cae"
      },
      "source": [
        "segmented_document = pipeline.process(reviews[8])\n",
        "print(segmented_document)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well every scene so perfectly presented .\n",
            "Never before had I seen such a movie that has meaning in almost every scene .\n",
            "Well while I was watching this movie I remembered watching \" Amilie \" .\n",
            "Amilie is also a similar kind of movie with more fun and fantasy touch .\n",
            "These movies are both based on a same plot line .\n",
            "But both of them are Perfectly perfect in there own way and being able to create a world of their own .\n",
            "Red is able to provoke lots of thoughts in people about destiny and dejavu s .\n",
            "And I am still thinking about it .\n",
            "The story is great and the ending is a bit funny .\n",
            "I was laughing in the end scene .\n",
            "Well funny how this three movie Red White and Blue are connected .\n",
            "All in all a great work of art .\n",
            "A work of a masterman .\n",
            "10/10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha_E-71iwCAX"
      },
      "source": [
        "Preprocessing exhausts resources so let's cut down the input and take 100 st words from each review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8zOyPrwwSUg"
      },
      "source": [
        "cut = [item[:1000] for item in reviews]\n",
        "\n",
        "# for i in range(10):\n",
        "#   print(len(cut[i]))\n",
        "#   print(cut[i])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOQxhIobMFGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "58e054b2-3657-4b36-a97c-18bbc48a832c"
      },
      "source": [
        "segmented  = [pipeline.process(text) for text in cut]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d2745d767eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msegmented\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-d2745d767eec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msegmented\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ptvgdHxAbL"
      },
      "source": [
        "for i in range(30):\n",
        "  print(segmented[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox8EQgz3IEM1"
      },
      "source": [
        "## train dev test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbPsc3SWIyKd",
        "outputId": "28a4ac8a-5182-44e0-def0-2fe55a339890"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.33)\n",
        "\n",
        "print(len(X_train), len(y_train))\n",
        "\n",
        "for label, text in zip(y_train[:10], X_train[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16750 16750\n",
            "label: neg \n",
            "text: To start off, I love Steven Seagal, the man is a genius. But recent movies leave me to wonder, Is he trying anymore? His latest movies show almost no effort on Seagal's part. In Out of Reach, its too obvious that his lines are dubbed over. . What Seagal does in this movie is not only a slap on the face to his fans, but even more to Jean-Claude Van Damme and his fans. In the 2nd scene or so, when he prepares to zip-line into the drug dealers penthouse to steal the jewels and money, it shows him set-up the gun and hook it on a neon sign. Your might be saying to yourself, 'Yah, so what does this have to do with Van Damme?'. Well that scene was stolen from a Jean-Claude Van Damme movie called the Order. Rent both, watch both, compare both and you will lose respect for Seagal. Not only was Today You Die garbage, but it was, dare I say, an insult. Seagal's Aikido moves are still good, but why isn't he doing great movies like Marked for Death or Above the law, hes still got the moves and the attitude, I'm just left wondering 'Why Seagal, Why?'. There are such idiotic scenes in his newer movies that have nothing to do with the storyline, and such idiotic story lines on top of that. I hope the up-coming Black Dawn movie will be another Exit Wounds or Beyond Justice, because these last chain of movies he made, especially Today You Die, really made me wonder if he has the stuff to make more great action movies like Double Team. Please, don't watch this movie unless you hate Seagal, if you love his movies don't watch this, it WILL make you question his future in the action film genre. \n",
            "\n",
            "label: neg \n",
            "text: On a flight back from London, I watched She's the Man; apparently Air Canada has a crap movie policy. Perhaps that's not the best way to start a review of this movie. Amanda Bynes plays a girl who loves soccer so much that she pretends to be her twin brother to get on a team at a boarding school across town. Even if you check your mind at the door (on a 6 hour flight you have to), the story is implausible and ridiculous. There are some moments of humor, mostly from comedian David Cross as the principle, but the intricate love polygon doesn't really inspire emotion, although is is cleverly mixed (with the caveat of mindless plausibility). The ending is just as ridiculously mindless as the rest. I guess if I was a 12-year-old girl, I might have really enjoyed this one. \n",
            "\n",
            "label: pos \n",
            "text: I wish there were more films about middle aged people. The intellectual journey and the twists and turns of life's moral highway make interesting viewing. There seems to be a different standard of judgement on women who have extra marital affairs than on men. Amy Watson's hurtful and humiliating behaviour towards her husband seems to pass without comment. Reverse the roles and one could expect a torrent of condemnation towards the man. If she found her husband boring and judgmental she could could have told him so, left and waited for a no doubt large financial settlement upon divorce. The country and London scenes are wonderfully authentic and rich while the autumnal weather adds to the melancholy background superbly. The ending is perfect, so in tune with real adult life. \n",
            "\n",
            "label: pos \n",
            "text: This is definitely a good movie unlike what other people are saying. Peline and I have both seen the movie 3x and the end is cheesy but that is part of the package... to experiment with death is the same as trying to defy it, so the movie is cool. For those who wish to see a classic Kevin Bacon movie, see this please. take care greeting from Istanbul. we will write 10 lines if you really want us 2 but we think it is a waste of time bye. The rest of this review should not be taken seriously because we just wrote it to fill the 10 line requirement (which is crazy) but we are kind people and will therefore adapt to the rules that are set out for us. So people watch this movie not because we wrote 10 lines, but because it is good. and by the way we wrote 12 lines!! \n",
            "\n",
            "label: neg \n",
            "text: Contains Major Spoilers, on the off chance you would actually care about the story line. OK, we have storms that destroy a city and a computer hacker who clobbers the power grid. Predictable schlock from the start, and if that weren't enough, the 5 second action bumps between the movie and the commercials kill what little suspense there might have been. For example: will they make it to the airport in time? Things look dim as we go to a commercialand the action shot before the ad shows them bouncing around inside the plane! Well, I guess they're gonna make it after allbut then again, they had to because they're good guys. The acting wasn't any too impressive (exception and welcome relief: Randy Quaid as Tornado Tommy) , the effects were kinda lame, the bad guys got it, and the good guys came through. The real disaster of this movie was the script, especially the ending. Not only did they wrap things up happily as quickly as a soap opera given 24 hours notice of a cancellation, but they glorified the hacker as well-intentioned. So he caused a bazillion deathshe meant well. And, of course, an uplifting final TV report about people coming together. Barf. It was everything I expected from the commercials, and I'm glad I wasted my time watching it. It will make great conversation at the lunch table tomorrow. Is CBS insulting us by making this? Surebut we watched it, didn't we? Did you count many ads there were for home backup generators during this pig? Here's hoping for the next Plan 9 from Outer Space (which gets better with each viewing). This isn't it. 1 star. \n",
            "\n",
            "label: neg \n",
            "text: Ken Burns' \"Baseball\" is a decent documentary... it presents a clear origin of the game, a great depiction of baseball's early years and heroes. There's plenty in this movie for any baseball fan... that said, the film has several glaring flaws. 18 hours is simply too long for the human attention span. It's clear that Burns stretched his film out to fit his \"nine inning\" concept. It's not even a tight 18 hours... the pace on every segment is slow, almost morose... the music always nostalgic and wistful. Isn't baseball ever exciting and fun? Why is every player and their accomplishments presented in the form of a tragedy? Talking head after talking head turn every pitch into an emotional heartbreak, yakking about baseball as a metaphor, baseball as Americana, the psychology and theology of baseball... enough! This is syrupy, mawkish drivel. Billy Crystal is here to sell us all the Yankee hokum he's sold us before. Ken Burns uses the National Anthem as the series' theme song, and manages to play \"Take Me Out To The Ballgame\" so many times you might vomit. We get it, dude. Clearly Burns is a neo-Hollywood faux-liberal, so he spends probably a third of the film on the Negro leagues... these segments are spent chastising whites of yesterday for not being as open-minded as Kenny is today. For shame! He chides baseball for being segregated in the thirties and forties but fails to realize that America was segregated in those times! Burns falls head over heels in love with Buck O'Neil, a former negro-league player, and drools over every piece of footage in which the elderly O'Neil waxes poetic about his playing days. Nonsense... Burns would have been better off with an adult to help him edit his creation down. \"Baseball\" winds up as mushy, gushy, civil-rights propaganda disguised as Americana. Its clear that Burns is not a baseball fan... otherwise he would know we watch games laughing and cheering, not weeping and reciting soliloquies... are you listening, Mr. Burns? There's no crying in baseball. \n",
            "\n",
            "label: pos \n",
            "text: Ignore the bad reviews on here, this film is awesome! \"Just Before Dawn\" is a great example of what can be done in a film with a minimal budget if you have a dedicated crew, decent script, and a cool idea for a film. It's a hell of a lot of fun. I enjoyed it a lot more than most other 80's slashers because the killer is so unique. \"Wrong Turn\" ripped this movie off something fierce! There's plenty of blood and scares. My girlfriend was freaked out and she watches almost everything with me and doesn't flinch. It's got that creepiness to it. I'd say that \"Just Before Dawn\" is the best early 80's slasher out there. I really enjoyed it. 8 out of 10, kids. \n",
            "\n",
            "label: neg \n",
            "text: How many times must I write the words boring and not funny on this page until I get to ten lines. George is about original an actor as Alfalfa from the little Rascals. Although that is probably a slight to Alfalfa. How many times do I have sneak into these overpaid actors crappy films before I will learn that there is but one law in the movie industry. Take the money and make a movie - even if it is crap. They spend millions making this movie but can't take a few moments to watch the end result. And Renee GoAwayZigger should stop taking the illegal drugs she is on and seek medical help for the facial problems she is sporting. At least I hope its drugs as she is quite unattractive whilst talking out the side of her mouth. This movie was neither funny nor dramatic and these hacks should just stop making crap like this. George and Renee, you should be ashamed of yourselves and stop being stop stinking greedy. \n",
            "\n",
            "label: neg \n",
            "text: \"Mame\" is a disgrace to many things--to Lucille Ball, to a story which has been told better many times over, and to the musical genre altogether. Ms. Ball does not understand her character at all and she seems to be heavily sedated. Bea Arthur is good, but it is not enough. The production is very shoddy and cheap looking, the songs are sub-par, and nearly every joke misfires. Also, Lucy couldn't dance well, so the music had to be slowed down to a funerial pace. Avoid at all costs, but DO see the delightful \"Auntie Mame.\" \n",
            "\n",
            "label: neg \n",
            "text: Mickey Rourke hunts Diane Lane in Elmore Leonard's Killshot It is not like Mickey Rourke ever really disappeared. He has had a steady string of appearances before he burst back on the scene. He was memorable in: Domino, Sin City, Man on Fire, Once Upon a Time in Mexico, and Get Carter. But in his powerful dramatic performance in The Wrestler (2008), we see a full blown presentation of the character only hinted at in Get Carter. Whenever we get to know him, Rourke remains a cool, but sleazy, muscle bound slim ball. This is an Elmore Leonard story, and production. Leonard wrote such notable movies as taunt western thriller 3:10 to Yuma, Be Cool, Jackie Brown, Get Shorty, 52 Pick-Up, and Joe Kidd. This means that we get tough guys, some good, some not so good. It also means we get tight, realistic plots with characters doing what is best for them in each situation, weaving complications into violent conclusions. Killshot is no different. Tough, slim ball killer Rourke stalks unhappily married witness Lane. Think History of Violence meets No Country for Old Men. It is not as intense, bloody or gory as those two, but it is almost as good. If you like those two, including David Croneberg's equally wonderful Eastern Promises, you will like Killshot also. Director John Madden has not done a lot of movies. His last few were enjoyable, if not successful: Proof, Captain Corelli's Mandolin and Shakespeare in Love. Diana Lane hasn't had a powerful movie role since she and Richard Gere gave incredible performances in Unfaithful. Lately she is charming and appealing in romantic stories such as Nights in Rodanthe, Must Love Dogs, and Under the Tuscan Sun. Here she is right on mark, balancing her sexy appeal with reserved tension. This is a small part for Rosario Dawson. Yet Dawson does a good job with it. You see a lot more of Lane, including an underwear scene to rival Sigourney Weaver in Aliens and Nicole Kidman in Eyes Wide Shut. While you are in the crime drama section, also pick up Kiss, Kiss, Bang, Bang, and Gone Baby Gone, and Before the Devil Knows Your Dead. The last has wonderful performances by Phillip Seymour Hoffman, Ethan Hawke, Marisa Tomei and Albert Finney. Killshot flopped at the box office. More is our luck. It is certainly worth a 3-4 dollar rental, if you like this genre. 6/20/2009 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti6xr4TXGwab"
      },
      "source": [
        "## Tfidf\n",
        "\n",
        "Since we are dealing with text data we need to transform it to format a basic SVM can handle. For that purpose I use sklearn TfidfVectorizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "nNbQMIaSzdU5",
        "outputId": "a9422cdf-9636-4cc2-9316-b5f9c39c997c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=50000)\n",
        "fm_train = vectorizer.fit_transform(X_train)\n",
        "fm_test = vectorizer.transform(X_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-340a7c7b2b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfm_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfm_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                    \"be removed in 0.24.\")\n\u001b[1;32m   1897\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mof\u001b[0m \u001b[0municode\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'filename'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuuJbYX_1-8x"
      },
      "source": [
        "# input column number is limited by the vectorizer\n",
        "\n",
        "print(f\"We have {fm_train.shape[0]} rows and {fm_train.shape[1]} columns in the training data\")\n",
        "print(f\"And {fm_test.shape[0]} rows and {fm_test.shape[1]} columns in the training data\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPXtqJ9ELYK-"
      },
      "source": [
        "# type of the input data is scipy.sparse.csr.csr_matrix\n",
        "print(type(fm_train))\n",
        "\n",
        "# What does it mean? It looks like this:\n",
        "print(fm_train[0:2:])\n",
        "\n",
        "# each row contains tfidf weights for a single review. \n",
        "# Columns are mostly empty because most words in the vovabulary do not appear in every sentence\n",
        "# This is why sparse format is used\n",
        "print(fm_train[0:2].todense())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfeZp_3C0d3B"
      },
      "source": [
        "for (idx, item) in enumerate(vectorizer.vocabulary_.items()):\n",
        "  print(\"Key:\", item[0], \"\\tValue:\",item[1])\n",
        "  if (idx==8):\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rihQsGsqKqOg"
      },
      "source": [
        "## Finding the best model with GridSearch Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "419GUOsFS_Zu"
      },
      "source": [
        "The model can be trained by exploring the hyperparameters one by one or with nested for-loops. It will how ever become a frustrating task to keep up with the hyperparameter combinations and obtained performence values. A more systematic way to do this is by using GridSearch (GS). \n",
        "\n",
        "GridSearchCV allows us set grid (or multiple vectors) of hypermarameters to try with. The idea is to try and find a sweet spot (best performance measure) by adjusting the grid. K-fold CV also introduces a new hyperparameter, which affects the training results, namely the number of folds.\n",
        "\n",
        "GS uses K-fold Cross Validation (CV) to find the best performing model. Depending on the algorithm and chosen parameters the data is \"folded\" (divided into subsets) n times and each of these folds is used once for testing while n-1 folds are used for training. Cross validation is also useful when the data set size is limited and we would like to \"eat the cake and keep it\".\n",
        "\n",
        "For the task I have chosen Linear Support Vector Classifier. When classifying multiple classes and the number of classes in *n* LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training *n* models ([Scikit-learn](https://scikit-learn.org/stable/modules/svm.html#svm-classification)). At prediction time all the classi\ffers \"vote\", and item will be assigned to class with the lowest cost. Other possible models for text classification problem are for example K-Nearest-Neighbors and Multinomial Naive Bayes. Also classifiers can be compared with GS.\n",
        "\n",
        "A simple pipeline is built for both the preprocessor (vectorizer) and the classifier so that we are able to find the best hyperparameters for both of them at once.\n",
        "\n",
        "Sources:\n",
        "\n",
        "[GridSearchCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
        "\n",
        "[GridSearchCV example 1](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html) \n",
        "\n",
        "[GridSearchCV example 2](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html) \n",
        "\n",
        "[SVM documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
        "\n",
        "[GridSearchCV scoring parameters](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95XwkuFwVS8x"
      },
      "source": [
        "#### TfidfVectorizer with SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z5Iu2P5MLSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79ad539-e6cd-4a2e-f0b9-6b0f21ee8158"
      },
      "source": [
        "costs = np.logspace(-1, 1, num=5, endpoint = False)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vec', TfidfVectorizer()),\n",
        "    ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    #'vec__binary': (True, False), # Previous runs revealed this does not seem to matter\n",
        "    'vec__max_features': (30000, 50000),\n",
        "    'vec__ngram_range': ((1, 1), (1, 2), (1, 3)),  \n",
        "    'clf__C': (costs), \n",
        "}\n",
        "# find the best parameters for both the feature extraction and the classifier\n",
        "print(\"Running grid search...\")\n",
        "# n_jobs=-1: use as many cores as possible\n",
        "# cv=3: three folds (this is kind of little but it speeds things up)\n",
        "gridsearch = GridSearchCV(pipeline, parameters, cv=3, verbose=1, n_jobs=-1)\n",
        "gridsearch.fit(X_train, y_train)\n",
        "print(\"Grid search done!\")\n",
        "print()\n",
        "print(f\"Best score: {gridsearch.best_score_:0.2}\")\n",
        "print(\"Best of the observed hyperparameters:\")\n",
        "best_parameters = gridsearch.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "      print(f\"{param_name}: {best_parameters[param_name]}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running grid search...\n",
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  7.9min\n",
            "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 16.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Grid search done!\n",
            "\n",
            "Best score: 0.9\n",
            "Best of the observed hyperparameters:\n",
            "clf__C: 0.6309573444801934\n",
            "vec__max_features: 50000\n",
            "vec__ngram_range: (1, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPNkptft9ixO"
      },
      "source": [
        "So was the selected model clearly the best?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HbVgsom5Wir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae69168-96bd-4d50-fdfd-686484a9c262"
      },
      "source": [
        "# set column visibility in pandas df\n",
        "pd.set_option(\"max_colwidth\", None)\n",
        "\n",
        "# extract mean score for each parameter combination trained\n",
        "means = gridsearch.cv_results_['mean_test_score'] \n",
        "\n",
        "GSCV_results = pd.DataFrame(list(zip(means, gridsearch.cv_results_['params'])), \n",
        "               columns =['Score', 'Parameters']) \n",
        "# sort by the score\n",
        "GSCV_results.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
        "print(GSCV_results.head(7))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Score                                                                              Parameters\n",
            "16  0.902328  {'clf__C': 0.6309573444801934, 'vec__max_features': 50000, 'vec__ngram_range': (1, 2)}\n",
            "17  0.901851  {'clf__C': 0.6309573444801934, 'vec__max_features': 50000, 'vec__ngram_range': (1, 3)}\n",
            "22  0.901194   {'clf__C': 1.584893192461114, 'vec__max_features': 50000, 'vec__ngram_range': (1, 2)}\n",
            "11  0.900716   {'clf__C': 0.251188643150958, 'vec__max_features': 50000, 'vec__ngram_range': (1, 3)}\n",
            "10  0.900239   {'clf__C': 0.251188643150958, 'vec__max_features': 50000, 'vec__ngram_range': (1, 2)}\n",
            "28  0.899582   {'clf__C': 3.981071705534973, 'vec__max_features': 50000, 'vec__ngram_range': (1, 2)}\n",
            "23  0.899403   {'clf__C': 1.584893192461114, 'vec__max_features': 50000, 'vec__ngram_range': (1, 3)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY62tzzG-zVf"
      },
      "source": [
        "Not! This seemd to be a tight race."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJZ2_WEGBCSR"
      },
      "source": [
        "##### Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq6i4BgvTJ4l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "15e8252c-caa6-4c1c-914c-469910c9f338"
      },
      "source": [
        "# classifier=grid_search.best_estimator_\n",
        "# classifier.fit(feature_matrix_train, train_label)\n",
        "\n",
        "predictions = gridsearch.predict(X_test)\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "# conf = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "print(f\"Test accuracy: {acc:0.2f}\")\n",
        "print()\n",
        "# note here we have to feed in the test data not feature matrix since esitimator is a pipeline, not a classifier!\n",
        "plot_confusion_matrix(gridsearch.best_estimator_, X_test, y_test, cmap='Greens', values_format='d')  \n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.show()\n",
        "\n",
        "plot_confusion_matrix(gridsearch.best_estimator_, X_test, y_test, cmap='Blues', normalize='true')  \n",
        "plt.title(\"Normalized confusion matrix\")\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.91\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cAuuKjW6yrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9651d9dd-2ade-4cbf-ef61-b3b34a309ada"
      },
      "source": [
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.92      0.91      0.91      4126\n",
            "         pos       0.91      0.92      0.91      4124\n",
            "\n",
            "    accuracy                           0.91      8250\n",
            "   macro avg       0.91      0.91      0.91      8250\n",
            "weighted avg       0.91      0.91      0.91      8250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuSXvfofI_M1"
      },
      "source": [
        "The model handled both of the classes well. This can be seen from the confusion matrix and from the classification report where precision and recall are in balance for both os the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0eZsazk14xb"
      },
      "source": [
        "#### NN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1nyeGt9wES3"
      },
      "source": [
        "# redo the split to overwrite old variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.33)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEznt0u2KU3"
      },
      "source": [
        "To build a NN we need to\n",
        "\n",
        "1.   turn numpy vectors to tensors\n",
        "2.   know the shape of input layer (number of features)\n",
        "3.   know the shape of output layer (number of classes)\n",
        "\n",
        "TfidfVectorizer gives the 2nd one and LabelEncoder (for example) the 3rd one. (or just len(set(train_labels))\n",
        "\n",
        "\"Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class.\"(https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6vARoH6wv3L"
      },
      "source": [
        "# 1) np vectors to TF tensors\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "    coo = X.tocoo()\n",
        "    indices = np.mat([coo.row, coo.col]).transpose()\n",
        "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utEFKUq0ModO"
      },
      "source": [
        "Since vectorizer affects the shape of the NN, we do not optimize it as a part of the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YooLapRH3ABl",
        "outputId": "44f144dd-dbaa-4bee-d73c-8b478e8b31e8"
      },
      "source": [
        "# 2) size of input \n",
        "vectorizer = TfidfVectorizer(max_features=50000)\n",
        "\n",
        "ft_matrix = vectorizer.fit_transform(X_train)\n",
        "ft_matrix.shape # so we need the second dimension for building the nn\n",
        "input_size = ft_matrix.shape[1]\n",
        "input_size"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HevFwocVx2l6",
        "outputId": "13a9275f-eeb7-4a2a-ff67-23edf713b8fa"
      },
      "source": [
        "# 3) size_of_output_layer\n",
        "# use encoded labels when fitting the model and for testing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder() #Turns class labels into integers\n",
        "class_numbers_train = label_encoder.fit_transform(y_train)\n",
        "\n",
        "print(\"class_numbers shape=\", class_numbers_train.shape)\n",
        "print(\"class labels\", label_encoder.classes_) #this will let us translate back from indices to labels\n",
        "\n",
        "output_size = len(label_encoder.classes_)\n",
        "print(\"Shape of output layer:\", output_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_numbers shape= (16750,)\n",
            "class labels ['neg' 'pos']\n",
            "Shape of output layer: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1GYj-1KZyJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ec2ab8-c6b6-4c94-bbcb-9b9c833d938a"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "def build_sequential_nn(input_size=100, output_size=2, hiddenlayer_size=200, drop_out= 0.3, learning_rate=0.001): \n",
        "  # let's make 200 default hiddenlayersize\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape = (input_size, )))\n",
        "  model.add(Dense(hiddenlayer_size, activation = \"tanh\", ))\n",
        "  model.add(Dropout(rate=drop_out)) # Dropout regularizer to avoid over fitting\n",
        "  model.add(Dense(output_size, activation = \"softmax\"))\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = build_sequential_nn(input_size=input_size, output_size=output_size)\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_243\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_486 (Dense)            (None, 200)               10000200  \n",
            "_________________________________________________________________\n",
            "dropout_243 (Dropout)        (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_487 (Dense)            (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 10,000,602\n",
            "Trainable params: 10,000,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbvQNJQmwpma",
        "outputId": "944c93ba-417a-4380-be34-5baa3828f634"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from keras.callbacks import EarlyStopping\n",
        "import time\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('trans', FunctionTransformer(convert_sparse_matrix_to_sparse_tensor)), # wrapper for custom function\n",
        "    ('clf', KerasClassifier(build_fn=build_sequential_nn)), # wrapper for Keras model\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'clf__hiddenlayer_size': (200, 250), \n",
        "    'clf__input_size': ([input_size]), # GS sets ALL the params\n",
        "    'clf__output_size': ([output_size]),\n",
        "    'clf__batch_size': (16, 32),\n",
        "    'clf__drop_out': (0.2, 0.3, 0.4),\n",
        "    'clf__epochs': (3, 5), # do not use early stopping callback, number of epochs is best treated as a hyper parameter: https://stackoverflow.com/questions/48127550/early-stopping-with-keras-and-sklearn-gridsearchcv-cross-validation\n",
        "    'clf__learning_rate': (0.001, 0.01)\n",
        "}\n",
        "\n",
        "t0=time.time()\n",
        "print(\"Running grid search...\")\n",
        "gridsearch = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=1) # n_jobs=-1: use as many cores as possible OR use GPU\n",
        "gridsearch.fit(ft_matrix, class_numbers_train)\n",
        "print()\n",
        "\n",
        "print(f\"Best score: {gridsearch.best_score_:0.2}\")\n",
        "\n",
        "print(\"Best of the observed hyperparameters:\")\n",
        "best_parameters = gridsearch.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "      print(f\"{param_name}: {best_parameters[param_name]}\")\n",
        "\n",
        "t1=time.time()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running grid search...\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "838/838 [==============================] - 6s 3ms/step - loss: 0.6959 - accuracy: 0.5030\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6027 - accuracy: 0.7058\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3203 - accuracy: 0.8890\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0619 - accuracy: 0.4985\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6947 - accuracy: 0.5114\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5948 - accuracy: 0.7451\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3035 - accuracy: 0.8942\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1481 - accuracy: 0.4907\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6960 - accuracy: 0.4991\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5927 - accuracy: 0.7139\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2990 - accuracy: 0.8991\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1433 - accuracy: 0.4994\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6962 - accuracy: 0.4949\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6034 - accuracy: 0.7214\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3150 - accuracy: 0.8888\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0770 - accuracy: 0.5119\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6956 - accuracy: 0.4945\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5911 - accuracy: 0.7295\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2990 - accuracy: 0.8974\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1286 - accuracy: 0.5018\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7226 - accuracy: 0.5003\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6834 - accuracy: 0.6275\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3783 - accuracy: 0.8333\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1233 - accuracy: 0.5119\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7215 - accuracy: 0.5079\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6763 - accuracy: 0.6324\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3857 - accuracy: 0.8350\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.2495 - accuracy: 0.4872\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7220 - accuracy: 0.5127\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6721 - accuracy: 0.6477\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3612 - accuracy: 0.8431\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.2167 - accuracy: 0.4949\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7260 - accuracy: 0.4928\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6831 - accuracy: 0.6241\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3975 - accuracy: 0.8306\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0998 - accuracy: 0.5134\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7249 - accuracy: 0.4952\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6743 - accuracy: 0.6398\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3796 - accuracy: 0.8341\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1478 - accuracy: 0.4967\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6958 - accuracy: 0.4902\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6101 - accuracy: 0.7179\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3204 - accuracy: 0.8878\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0653 - accuracy: 0.5018\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6967 - accuracy: 0.5019\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5854 - accuracy: 0.7197\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2909 - accuracy: 0.8969\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1854 - accuracy: 0.4878\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6964 - accuracy: 0.4889\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5991 - accuracy: 0.7050\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3010 - accuracy: 0.8999\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1336 - accuracy: 0.4991\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6969 - accuracy: 0.4954\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6087 - accuracy: 0.7035\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3173 - accuracy: 0.8880\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0796 - accuracy: 0.5128\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6950 - accuracy: 0.5123\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5863 - accuracy: 0.7367\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2955 - accuracy: 0.8947\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1588 - accuracy: 0.4973\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7311 - accuracy: 0.4995\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7148 - accuracy: 0.6183\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4120 - accuracy: 0.8217\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0317 - accuracy: 0.5173\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7327 - accuracy: 0.5065\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6972 - accuracy: 0.6367\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3893 - accuracy: 0.8311\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1978 - accuracy: 0.4970\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7315 - accuracy: 0.5028\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6803 - accuracy: 0.6423\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4045 - accuracy: 0.8226\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.2745 - accuracy: 0.4997\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7337 - accuracy: 0.5048\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7091 - accuracy: 0.6211\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4258 - accuracy: 0.8126\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0864 - accuracy: 0.5143\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7293 - accuracy: 0.4996\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7034 - accuracy: 0.6253\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3851 - accuracy: 0.8351\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.2447 - accuracy: 0.4893\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6961 - accuracy: 0.4937\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6025 - accuracy: 0.7089\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3271 - accuracy: 0.8909\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1490 - accuracy: 0.9606\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0707 - accuracy: 0.9855\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.7854 - accuracy: 0.5021\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6962 - accuracy: 0.4924\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6010 - accuracy: 0.7061\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3153 - accuracy: 0.8931\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1366 - accuracy: 0.9637\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0642 - accuracy: 0.9866\n",
            "210/210 [==============================] - 1s 2ms/step - loss: 1.9421 - accuracy: 0.4797\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6957 - accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5911 - accuracy: 0.7274\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2986 - accuracy: 0.8996\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1366 - accuracy: 0.9597\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0614 - accuracy: 0.9876\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.9243 - accuracy: 0.4952\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6969 - accuracy: 0.4959\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5970 - accuracy: 0.7327\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3125 - accuracy: 0.8918\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1456 - accuracy: 0.9564\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0679 - accuracy: 0.9846\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8435 - accuracy: 0.5051\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6961 - accuracy: 0.5044\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5832 - accuracy: 0.7159\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2949 - accuracy: 0.9024\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1381 - accuracy: 0.9592\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0594 - accuracy: 0.9868\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.9092 - accuracy: 0.5003\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7230 - accuracy: 0.5036\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6897 - accuracy: 0.6224\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3980 - accuracy: 0.8229\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1877 - accuracy: 0.9272\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1160 - accuracy: 0.9589\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.5845 - accuracy: 0.4901\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7249 - accuracy: 0.4982\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6732 - accuracy: 0.6352\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3849 - accuracy: 0.8338\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1976 - accuracy: 0.9197\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1009 - accuracy: 0.9632\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.6141 - accuracy: 0.4928\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7237 - accuracy: 0.5027\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6833 - accuracy: 0.6279\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3704 - accuracy: 0.8444\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1862 - accuracy: 0.9271\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1036 - accuracy: 0.9632\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.5626 - accuracy: 0.4949\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7237 - accuracy: 0.4952\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7010 - accuracy: 0.6155\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3871 - accuracy: 0.8363\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1795 - accuracy: 0.9288\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1080 - accuracy: 0.9593\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.2844 - accuracy: 0.5042\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7236 - accuracy: 0.4978\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6795 - accuracy: 0.6352\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3664 - accuracy: 0.8432\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1741 - accuracy: 0.9336\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0880 - accuracy: 0.9687\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.6768 - accuracy: 0.5012\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6967 - accuracy: 0.4854\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6053 - accuracy: 0.7005\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3288 - accuracy: 0.8856\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1501 - accuracy: 0.9570\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0640 - accuracy: 0.9868\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8280 - accuracy: 0.5066\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6964 - accuracy: 0.5021\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5853 - accuracy: 0.7304\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2898 - accuracy: 0.9018\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1270 - accuracy: 0.9637\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0564 - accuracy: 0.9895\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.0914 - accuracy: 0.4851\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6958 - accuracy: 0.5065\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5785 - accuracy: 0.7373\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2889 - accuracy: 0.9054\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1332 - accuracy: 0.9623\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0595 - accuracy: 0.9874\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.9768 - accuracy: 0.4931\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6971 - accuracy: 0.4988\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5971 - accuracy: 0.7085\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3076 - accuracy: 0.8990\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1380 - accuracy: 0.9615\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0657 - accuracy: 0.9828\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8694 - accuracy: 0.5057\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6969 - accuracy: 0.4955\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5889 - accuracy: 0.7111\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2891 - accuracy: 0.9002\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1273 - accuracy: 0.9630\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0573 - accuracy: 0.9864\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.0516 - accuracy: 0.4979\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7328 - accuracy: 0.4903\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7143 - accuracy: 0.6262\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3930 - accuracy: 0.8319\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1994 - accuracy: 0.9230\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1158 - accuracy: 0.9589\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.6002 - accuracy: 0.5110\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7289 - accuracy: 0.4993\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6945 - accuracy: 0.6365\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3812 - accuracy: 0.8364\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1841 - accuracy: 0.9302\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1154 - accuracy: 0.9576\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.9536 - accuracy: 0.4910\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7272 - accuracy: 0.5111\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6965 - accuracy: 0.6413\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3718 - accuracy: 0.8355\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1921 - accuracy: 0.9265\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1153 - accuracy: 0.9581\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 3.1283 - accuracy: 0.4991\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7303 - accuracy: 0.5020\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7152 - accuracy: 0.6166\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4085 - accuracy: 0.8207\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2019 - accuracy: 0.9225\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1247 - accuracy: 0.9543\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.5550 - accuracy: 0.5042\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7306 - accuracy: 0.5048\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6882 - accuracy: 0.6422\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3997 - accuracy: 0.8240\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1924 - accuracy: 0.9250\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1129 - accuracy: 0.9579\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.7254 - accuracy: 0.4934\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6961 - accuracy: 0.5009\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6043 - accuracy: 0.7175\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3238 - accuracy: 0.8885\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0511 - accuracy: 0.4958\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6962 - accuracy: 0.5008\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5791 - accuracy: 0.7423\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2947 - accuracy: 0.9008\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1568 - accuracy: 0.4836\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6960 - accuracy: 0.4973\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5816 - accuracy: 0.7302\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2966 - accuracy: 0.8969\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1633 - accuracy: 0.4973\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6962 - accuracy: 0.4988\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6010 - accuracy: 0.7156\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3192 - accuracy: 0.8891\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0524 - accuracy: 0.5093\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6961 - accuracy: 0.5003\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5896 - accuracy: 0.7398\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3052 - accuracy: 0.8961\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1304 - accuracy: 0.4979\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7289 - accuracy: 0.5078\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6919 - accuracy: 0.6240\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.4056 - accuracy: 0.8179\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0698 - accuracy: 0.5075\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7302 - accuracy: 0.4951\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6677 - accuracy: 0.6492\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3966 - accuracy: 0.8267\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1673 - accuracy: 0.4916\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 3ms/step - loss: 0.7275 - accuracy: 0.5006\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6566 - accuracy: 0.6549\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3807 - accuracy: 0.8301\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1915 - accuracy: 0.5104\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7277 - accuracy: 0.5092\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7064 - accuracy: 0.6145\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.4092 - accuracy: 0.8207\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1323 - accuracy: 0.5137\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7278 - accuracy: 0.5026\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6824 - accuracy: 0.6306\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3754 - accuracy: 0.8383\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0901 - accuracy: 0.5057\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6968 - accuracy: 0.5008\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6112 - accuracy: 0.6922\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3222 - accuracy: 0.8879\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0498 - accuracy: 0.5030\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6962 - accuracy: 0.5002\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5908 - accuracy: 0.7338\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3017 - accuracy: 0.8966\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1656 - accuracy: 0.4854\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6971 - accuracy: 0.4922\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6035 - accuracy: 0.7119\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3141 - accuracy: 0.8888\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0858 - accuracy: 0.5030\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6965 - accuracy: 0.4980\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5955 - accuracy: 0.6986\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3082 - accuracy: 0.8928\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0653 - accuracy: 0.5140\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6967 - accuracy: 0.4928\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5795 - accuracy: 0.7333\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2965 - accuracy: 0.8965\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1693 - accuracy: 0.4934\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7357 - accuracy: 0.5124\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7091 - accuracy: 0.6263\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4340 - accuracy: 0.8129\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0996 - accuracy: 0.5125\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7337 - accuracy: 0.5024\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6764 - accuracy: 0.6522\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3832 - accuracy: 0.8391\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1891 - accuracy: 0.4824\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7353 - accuracy: 0.4968\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6935 - accuracy: 0.6378\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4017 - accuracy: 0.8313\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0854 - accuracy: 0.4925\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7371 - accuracy: 0.5000\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6918 - accuracy: 0.6426\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4147 - accuracy: 0.8217\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1350 - accuracy: 0.5096\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7359 - accuracy: 0.4966\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6834 - accuracy: 0.6451\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4036 - accuracy: 0.8255\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.2131 - accuracy: 0.5042\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6960 - accuracy: 0.4983\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5987 - accuracy: 0.7200\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3166 - accuracy: 0.8906\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1557 - accuracy: 0.9531\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0677 - accuracy: 0.9848\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8214 - accuracy: 0.4991\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6959 - accuracy: 0.4991\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5909 - accuracy: 0.7338\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3041 - accuracy: 0.9018\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1477 - accuracy: 0.9585\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0686 - accuracy: 0.9843\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.9182 - accuracy: 0.4919\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6968 - accuracy: 0.4937\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5903 - accuracy: 0.7256\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3069 - accuracy: 0.8938\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1420 - accuracy: 0.9589\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0680 - accuracy: 0.9850\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8978 - accuracy: 0.4949\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6968 - accuracy: 0.4935\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6019 - accuracy: 0.7145\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3228 - accuracy: 0.8902\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1588 - accuracy: 0.9536\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.0728 - accuracy: 0.9822\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8182 - accuracy: 0.5078\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6967 - accuracy: 0.4991\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.5757 - accuracy: 0.7262\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2906 - accuracy: 0.9021\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1358 - accuracy: 0.9619\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0627 - accuracy: 0.9855\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.9580 - accuracy: 0.5015\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7314 - accuracy: 0.4955\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6982 - accuracy: 0.6208\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4046 - accuracy: 0.8195\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2149 - accuracy: 0.9136\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1185 - accuracy: 0.9569\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.3232 - accuracy: 0.5024\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7311 - accuracy: 0.4897\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6687 - accuracy: 0.6372\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3825 - accuracy: 0.8312\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2012 - accuracy: 0.9193\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1124 - accuracy: 0.9600\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.4396 - accuracy: 0.4955\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7273 - accuracy: 0.4902\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6761 - accuracy: 0.6291\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3965 - accuracy: 0.8269\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2083 - accuracy: 0.9184\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1240 - accuracy: 0.9536\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.4809 - accuracy: 0.5009\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7281 - accuracy: 0.4932\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.6955 - accuracy: 0.6193\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4054 - accuracy: 0.8234\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.2220 - accuracy: 0.9156\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1284 - accuracy: 0.9510\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.4533 - accuracy: 0.5054\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.7283 - accuracy: 0.4952\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6764 - accuracy: 0.6355\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.3853 - accuracy: 0.8298\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1932 - accuracy: 0.9243\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 3ms/step - loss: 0.1186 - accuracy: 0.9575\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.6205 - accuracy: 0.4967\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6962 - accuracy: 0.4924\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5974 - accuracy: 0.7301\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3237 - accuracy: 0.8871\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1562 - accuracy: 0.9536\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0769 - accuracy: 0.9812\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.7667 - accuracy: 0.5078\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6968 - accuracy: 0.4937\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5889 - accuracy: 0.7299\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3039 - accuracy: 0.9014\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1439 - accuracy: 0.9541\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0698 - accuracy: 0.9826\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.9418 - accuracy: 0.4919\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6961 - accuracy: 0.5002\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5847 - accuracy: 0.7277\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3020 - accuracy: 0.8950\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1400 - accuracy: 0.9589\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0645 - accuracy: 0.9861\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.9267 - accuracy: 0.5009\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6973 - accuracy: 0.5002\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6001 - accuracy: 0.7134\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3200 - accuracy: 0.8900\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1529 - accuracy: 0.9554\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0742 - accuracy: 0.9811\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.7910 - accuracy: 0.5104\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6959 - accuracy: 0.5032\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5831 - accuracy: 0.7106\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2992 - accuracy: 0.8980\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1406 - accuracy: 0.9581\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0640 - accuracy: 0.9845\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.9561 - accuracy: 0.5000\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7376 - accuracy: 0.4974\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6893 - accuracy: 0.6388\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4050 - accuracy: 0.8262\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2221 - accuracy: 0.9162\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1392 - accuracy: 0.9460\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.4414 - accuracy: 0.4973\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7373 - accuracy: 0.5032\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6924 - accuracy: 0.6374\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4193 - accuracy: 0.8145\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2206 - accuracy: 0.9129\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1332 - accuracy: 0.9498\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.5564 - accuracy: 0.4949\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7362 - accuracy: 0.4954\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7071 - accuracy: 0.6308\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4172 - accuracy: 0.8192\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2215 - accuracy: 0.9136\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1336 - accuracy: 0.9460\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.3459 - accuracy: 0.5042\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7347 - accuracy: 0.4962\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6921 - accuracy: 0.6466\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4143 - accuracy: 0.8154\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2271 - accuracy: 0.9096\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1332 - accuracy: 0.9516\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.6630 - accuracy: 0.5060\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7366 - accuracy: 0.4937\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6879 - accuracy: 0.6455\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3924 - accuracy: 0.8314\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2017 - accuracy: 0.9181\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1272 - accuracy: 0.9539\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.7597 - accuracy: 0.5051\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6968 - accuracy: 0.4967\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5972 - accuracy: 0.7462\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3233 - accuracy: 0.8876\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0346 - accuracy: 0.5116\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6966 - accuracy: 0.4899\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5823 - accuracy: 0.7508\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3032 - accuracy: 0.8971\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1319 - accuracy: 0.4928\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6963 - accuracy: 0.4961\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5835 - accuracy: 0.7278\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3194 - accuracy: 0.8890\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1160 - accuracy: 0.5063\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6966 - accuracy: 0.4969\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5877 - accuracy: 0.7463\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3127 - accuracy: 0.8938\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0543 - accuracy: 0.5167\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6970 - accuracy: 0.4995\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5837 - accuracy: 0.7263\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3164 - accuracy: 0.8913\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1057 - accuracy: 0.4970\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7316 - accuracy: 0.4982\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6719 - accuracy: 0.6441\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4195 - accuracy: 0.8107\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1250 - accuracy: 0.5012\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7311 - accuracy: 0.5139\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6878 - accuracy: 0.6340\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4224 - accuracy: 0.8083\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1607 - accuracy: 0.4881\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7311 - accuracy: 0.4992\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6897 - accuracy: 0.6262\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4282 - accuracy: 0.8119\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1704 - accuracy: 0.5093\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7329 - accuracy: 0.4971\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6845 - accuracy: 0.6306\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4295 - accuracy: 0.8079\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0534 - accuracy: 0.5060\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7305 - accuracy: 0.5133\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6818 - accuracy: 0.6402\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4188 - accuracy: 0.8111\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1169 - accuracy: 0.4976\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6971 - accuracy: 0.5019\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5953 - accuracy: 0.7246\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3303 - accuracy: 0.8839\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0370 - accuracy: 0.5063\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6950 - accuracy: 0.5140\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5859 - accuracy: 0.7274\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3100 - accuracy: 0.8896\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1067 - accuracy: 0.4916\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6963 - accuracy: 0.4966\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5900 - accuracy: 0.7335\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3146 - accuracy: 0.8883\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1163 - accuracy: 0.4943\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6969 - accuracy: 0.4962\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5971 - accuracy: 0.7308\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3252 - accuracy: 0.8857\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0572 - accuracy: 0.5107\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6964 - accuracy: 0.5067\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5870 - accuracy: 0.7265\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3161 - accuracy: 0.8849\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.1232 - accuracy: 0.5075\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7378 - accuracy: 0.5144\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7199 - accuracy: 0.6252\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4478 - accuracy: 0.8007\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0186 - accuracy: 0.5075\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7411 - accuracy: 0.4953\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6965 - accuracy: 0.6385\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4246 - accuracy: 0.8156\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.2126 - accuracy: 0.4851\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7417 - accuracy: 0.4871\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6956 - accuracy: 0.6281\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4335 - accuracy: 0.8094\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.2687 - accuracy: 0.4931\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7423 - accuracy: 0.4925\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7195 - accuracy: 0.6279\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4391 - accuracy: 0.8073\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.0702 - accuracy: 0.5099\n",
            "Epoch 1/3\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7448 - accuracy: 0.4997\n",
            "Epoch 2/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7014 - accuracy: 0.6425\n",
            "Epoch 3/3\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4219 - accuracy: 0.8146\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.2238 - accuracy: 0.4973\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6967 - accuracy: 0.4889\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5966 - accuracy: 0.7258\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3344 - accuracy: 0.8781\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1690 - accuracy: 0.9507\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0866 - accuracy: 0.9773\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.7618 - accuracy: 0.5057\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6961 - accuracy: 0.5031\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5860 - accuracy: 0.7516\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3154 - accuracy: 0.8885\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1552 - accuracy: 0.9545\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0832 - accuracy: 0.9794\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8239 - accuracy: 0.4943\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6967 - accuracy: 0.4984\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5934 - accuracy: 0.7318\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3215 - accuracy: 0.8855\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1590 - accuracy: 0.9479\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0824 - accuracy: 0.9782\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8237 - accuracy: 0.4967\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6970 - accuracy: 0.5009\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5911 - accuracy: 0.7371\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3185 - accuracy: 0.8906\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1659 - accuracy: 0.9476\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0876 - accuracy: 0.9769\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.7027 - accuracy: 0.5104\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6966 - accuracy: 0.4941\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5894 - accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3156 - accuracy: 0.8840\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1567 - accuracy: 0.9515\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0764 - accuracy: 0.9812\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8808 - accuracy: 0.5015\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7320 - accuracy: 0.4932\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7003 - accuracy: 0.6214\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4329 - accuracy: 0.8119\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2488 - accuracy: 0.8991\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1626 - accuracy: 0.9399\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.2575 - accuracy: 0.5125\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7319 - accuracy: 0.4927\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6829 - accuracy: 0.6407\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4017 - accuracy: 0.8288\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2369 - accuracy: 0.9063\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1520 - accuracy: 0.9434\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.4183 - accuracy: 0.4946\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7336 - accuracy: 0.5065\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6786 - accuracy: 0.6457\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4280 - accuracy: 0.8059\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2296 - accuracy: 0.9124\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1561 - accuracy: 0.9404\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.3608 - accuracy: 0.4997\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7347 - accuracy: 0.4834\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6959 - accuracy: 0.6284\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4237 - accuracy: 0.8162\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2412 - accuracy: 0.9017\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1509 - accuracy: 0.9422\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.1568 - accuracy: 0.5078\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7322 - accuracy: 0.4956\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6842 - accuracy: 0.6319\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4280 - accuracy: 0.8105\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2293 - accuracy: 0.9086\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1411 - accuracy: 0.9458\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.2748 - accuracy: 0.5021\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6973 - accuracy: 0.4870\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6023 - accuracy: 0.7250\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3295 - accuracy: 0.8861\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1640 - accuracy: 0.9503\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0839 - accuracy: 0.9782\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.7336 - accuracy: 0.5057\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6961 - accuracy: 0.5076\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5901 - accuracy: 0.7148\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3115 - accuracy: 0.8893\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1554 - accuracy: 0.9529\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0786 - accuracy: 0.9819\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8527 - accuracy: 0.4964\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6972 - accuracy: 0.5047\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5865 - accuracy: 0.7283\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3113 - accuracy: 0.8880\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1499 - accuracy: 0.9548\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0779 - accuracy: 0.9813\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8858 - accuracy: 0.4970\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6970 - accuracy: 0.4957\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5935 - accuracy: 0.7196\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3183 - accuracy: 0.8902\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1628 - accuracy: 0.9485\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0830 - accuracy: 0.9785\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.7730 - accuracy: 0.5090\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.6969 - accuracy: 0.4992\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.5777 - accuracy: 0.7444\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.3035 - accuracy: 0.8965\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1538 - accuracy: 0.9518\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.0739 - accuracy: 0.9803\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 1.8797 - accuracy: 0.4967\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7395 - accuracy: 0.5048\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6978 - accuracy: 0.6309\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4459 - accuracy: 0.8032\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2397 - accuracy: 0.9075\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1743 - accuracy: 0.9332\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.1539 - accuracy: 0.5027\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7409 - accuracy: 0.5038\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6955 - accuracy: 0.6434\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4330 - accuracy: 0.8096\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2484 - accuracy: 0.9044\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1497 - accuracy: 0.9445\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.5056 - accuracy: 0.4899\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7423 - accuracy: 0.4964\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.6974 - accuracy: 0.6414\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4222 - accuracy: 0.8159\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2432 - accuracy: 0.9052\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1402 - accuracy: 0.9474\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.3088 - accuracy: 0.4994\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7422 - accuracy: 0.5029\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7000 - accuracy: 0.6453\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4513 - accuracy: 0.7967\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2426 - accuracy: 0.9058\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1625 - accuracy: 0.9345\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.2510 - accuracy: 0.4979\n",
            "Epoch 1/5\n",
            "838/838 [==============================] - 4s 4ms/step - loss: 0.7403 - accuracy: 0.5078\n",
            "Epoch 2/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.7027 - accuracy: 0.6345\n",
            "Epoch 3/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.4375 - accuracy: 0.8123\n",
            "Epoch 4/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.2526 - accuracy: 0.8990\n",
            "Epoch 5/5\n",
            "838/838 [==============================] - 3s 4ms/step - loss: 0.1659 - accuracy: 0.9404\n",
            "210/210 [==============================] - 0s 2ms/step - loss: 2.4843 - accuracy: 0.4943\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.4998\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6095 - accuracy: 0.7468\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3261 - accuracy: 0.8921\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0622 - accuracy: 0.5090\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6956 - accuracy: 0.4960\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5916 - accuracy: 0.7449\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3026 - accuracy: 0.9034\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1396 - accuracy: 0.4863\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.4972\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5988 - accuracy: 0.7592\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3102 - accuracy: 0.8950\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1234 - accuracy: 0.4997\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.4925\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5957 - accuracy: 0.7291\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2964 - accuracy: 0.9054\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0957 - accuracy: 0.5093\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5046\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5866 - accuracy: 0.7608\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2923 - accuracy: 0.9022\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1241 - accuracy: 0.5027\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7081 - accuracy: 0.5108\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6385 - accuracy: 0.6507\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3363 - accuracy: 0.8586\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0448 - accuracy: 0.4994\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7117 - accuracy: 0.4974\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6298 - accuracy: 0.6648\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3504 - accuracy: 0.8492\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1823 - accuracy: 0.4842\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7099 - accuracy: 0.5007\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6405 - accuracy: 0.6593\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3357 - accuracy: 0.8584\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1689 - accuracy: 0.4991\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7122 - accuracy: 0.5045\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6455 - accuracy: 0.6407\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3612 - accuracy: 0.8445\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1152 - accuracy: 0.5090\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7130 - accuracy: 0.4852\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6391 - accuracy: 0.6534\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3550 - accuracy: 0.8472\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1409 - accuracy: 0.5003\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5005\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5835 - accuracy: 0.7704\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2931 - accuracy: 0.9070\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1137 - accuracy: 0.5134\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.4951\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5755 - accuracy: 0.7629\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2810 - accuracy: 0.9127\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1840 - accuracy: 0.4875\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5068\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5907 - accuracy: 0.7584\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2869 - accuracy: 0.9086\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1376 - accuracy: 0.4955\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.4927\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5966 - accuracy: 0.7410\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3045 - accuracy: 0.9010\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0615 - accuracy: 0.5191\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5074\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5840 - accuracy: 0.7372\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2919 - accuracy: 0.9069\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1536 - accuracy: 0.5021\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7159 - accuracy: 0.5015\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.6502\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3586 - accuracy: 0.8462\n",
            "105/105 [==============================] - 1s 2ms/step - loss: 1.1011 - accuracy: 0.5024\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7139 - accuracy: 0.5080\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6417 - accuracy: 0.6520\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3582 - accuracy: 0.8419\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.2628 - accuracy: 0.4881\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7147 - accuracy: 0.4963\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6416 - accuracy: 0.6545\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3510 - accuracy: 0.8499\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1118 - accuracy: 0.4943\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7112 - accuracy: 0.5124\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6394 - accuracy: 0.6563\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3745 - accuracy: 0.8375\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0867 - accuracy: 0.5113\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7133 - accuracy: 0.5016\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6436 - accuracy: 0.6590\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3478 - accuracy: 0.8549\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1045 - accuracy: 0.5030\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.4998\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6055 - accuracy: 0.7481\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3173 - accuracy: 0.8979\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1426 - accuracy: 0.9621\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0674 - accuracy: 0.9856\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7041 - accuracy: 0.5042\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6956 - accuracy: 0.5003\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5864 - accuracy: 0.7737\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2927 - accuracy: 0.9052\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1309 - accuracy: 0.9648\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0598 - accuracy: 0.9896\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.8569 - accuracy: 0.4881\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5016\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5894 - accuracy: 0.7312\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3005 - accuracy: 0.9050\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1346 - accuracy: 0.9635\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0613 - accuracy: 0.9877\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.8111 - accuracy: 0.5009\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5035\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6021 - accuracy: 0.7551\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3049 - accuracy: 0.9015\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1330 - accuracy: 0.9657\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0616 - accuracy: 0.9866\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7352 - accuracy: 0.5057\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6956 - accuracy: 0.4944\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5860 - accuracy: 0.7705\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2885 - accuracy: 0.9084\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1258 - accuracy: 0.9678\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0572 - accuracy: 0.9900\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.8214 - accuracy: 0.5069\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7109 - accuracy: 0.5093\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6495 - accuracy: 0.6459\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3556 - accuracy: 0.8456\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1523 - accuracy: 0.9410\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0794 - accuracy: 0.9733\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.4892 - accuracy: 0.5137\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7088 - accuracy: 0.5067\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6482 - accuracy: 0.6466\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3332 - accuracy: 0.8570\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1401 - accuracy: 0.9495\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0769 - accuracy: 0.9747\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5393 - accuracy: 0.4979\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7106 - accuracy: 0.5089\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6356 - accuracy: 0.6528\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3472 - accuracy: 0.8565\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1491 - accuracy: 0.9421\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0827 - accuracy: 0.9719\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5432 - accuracy: 0.5021\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7111 - accuracy: 0.4887\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6525 - accuracy: 0.6424\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3761 - accuracy: 0.8301\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1698 - accuracy: 0.9372\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0861 - accuracy: 0.9711\n",
            "105/105 [==============================] - 1s 2ms/step - loss: 2.2107 - accuracy: 0.5149\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7103 - accuracy: 0.5036\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6203 - accuracy: 0.6723\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3284 - accuracy: 0.8628\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1552 - accuracy: 0.9448\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0818 - accuracy: 0.9708\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5837 - accuracy: 0.4866\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6968 - accuracy: 0.4850\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6071 - accuracy: 0.7353\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3194 - accuracy: 0.8948\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1425 - accuracy: 0.9634\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0656 - accuracy: 0.9867\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7102 - accuracy: 0.5081\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5142\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5823 - accuracy: 0.7558\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2879 - accuracy: 0.9056\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1231 - accuracy: 0.9676\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0573 - accuracy: 0.9897\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.9233 - accuracy: 0.4872\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5008\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5939 - accuracy: 0.7184\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2944 - accuracy: 0.9056\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1309 - accuracy: 0.9625\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0622 - accuracy: 0.9874\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.8342 - accuracy: 0.5024\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6962 - accuracy: 0.4965\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6058 - accuracy: 0.7306\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3120 - accuracy: 0.8953\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1423 - accuracy: 0.9617\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0629 - accuracy: 0.9882\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7404 - accuracy: 0.5104\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5043\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5868 - accuracy: 0.7436\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2843 - accuracy: 0.9086\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1283 - accuracy: 0.9661\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0574 - accuracy: 0.9895\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.8654 - accuracy: 0.4928\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7117 - accuracy: 0.4995\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6386 - accuracy: 0.6627\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3357 - accuracy: 0.8550\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1564 - accuracy: 0.9421\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0922 - accuracy: 0.9659\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.4008 - accuracy: 0.5101\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7134 - accuracy: 0.5063\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6409 - accuracy: 0.6588\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3407 - accuracy: 0.8548\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1552 - accuracy: 0.9415\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0896 - accuracy: 0.9692\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.7399 - accuracy: 0.4916\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7157 - accuracy: 0.4912\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6355 - accuracy: 0.6613\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3306 - accuracy: 0.8593\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1589 - accuracy: 0.9422\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0830 - accuracy: 0.9711\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.4518 - accuracy: 0.4884\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7157 - accuracy: 0.4907\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.6403\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3571 - accuracy: 0.8455\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1750 - accuracy: 0.9331\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0847 - accuracy: 0.9725\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.4656 - accuracy: 0.5048\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7143 - accuracy: 0.4844\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6332 - accuracy: 0.6728\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3187 - accuracy: 0.8658\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1557 - accuracy: 0.9391\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0788 - accuracy: 0.9729\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.7996 - accuracy: 0.5057\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5039\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5881 - accuracy: 0.7597\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3086 - accuracy: 0.8989\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0629 - accuracy: 0.5039\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6956 - accuracy: 0.4959\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5811 - accuracy: 0.7472\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2955 - accuracy: 0.9057\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1669 - accuracy: 0.4818\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.4958\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5777 - accuracy: 0.7685\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2989 - accuracy: 0.9045\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1309 - accuracy: 0.4961\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.4981\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5918 - accuracy: 0.7421\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3117 - accuracy: 0.9002\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0703 - accuracy: 0.5158\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.4965\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5862 - accuracy: 0.7722\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3006 - accuracy: 0.9018\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1117 - accuracy: 0.5012\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7093 - accuracy: 0.5019\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6454\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3678 - accuracy: 0.8405\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0916 - accuracy: 0.5087\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7137 - accuracy: 0.4878\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6286 - accuracy: 0.6641\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3565 - accuracy: 0.8439\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0962 - accuracy: 0.4869\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7148 - accuracy: 0.5051\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6387 - accuracy: 0.6550\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3574 - accuracy: 0.8448\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0928 - accuracy: 0.4940\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7144 - accuracy: 0.5010\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6442 - accuracy: 0.6512\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3485 - accuracy: 0.8476\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0501 - accuracy: 0.5081\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7105 - accuracy: 0.5100\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6181 - accuracy: 0.6759\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3443 - accuracy: 0.8483\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1666 - accuracy: 0.4976\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6956 - accuracy: 0.4995\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5890 - accuracy: 0.7231\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3053 - accuracy: 0.8966\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0639 - accuracy: 0.5087\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.4872\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5785 - accuracy: 0.7752\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2926 - accuracy: 0.9101\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1710 - accuracy: 0.4955\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5063\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5929 - accuracy: 0.7652\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3121 - accuracy: 0.8938\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1178 - accuracy: 0.4943\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.5043\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5967 - accuracy: 0.7512\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3159 - accuracy: 0.8919\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0489 - accuracy: 0.5066\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.4934\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5771 - accuracy: 0.7716\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2983 - accuracy: 0.9008\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1594 - accuracy: 0.5072\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7197 - accuracy: 0.4936\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6501\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3582 - accuracy: 0.8485\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1379 - accuracy: 0.4934\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7188 - accuracy: 0.4996\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6439 - accuracy: 0.6569\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3448 - accuracy: 0.8492\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1051 - accuracy: 0.4875\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7173 - accuracy: 0.5102\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.6444\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3500 - accuracy: 0.8516\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.2320 - accuracy: 0.5030\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7159 - accuracy: 0.4998\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6370 - accuracy: 0.6561\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3701 - accuracy: 0.8408\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0958 - accuracy: 0.5063\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7190 - accuracy: 0.4865\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6210 - accuracy: 0.6750\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3465 - accuracy: 0.8517\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1438 - accuracy: 0.4949\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5054\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5908 - accuracy: 0.7505\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3173 - accuracy: 0.8938\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1442 - accuracy: 0.9606\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0688 - accuracy: 0.9864\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.6939 - accuracy: 0.5087\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5067\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5815 - accuracy: 0.7729\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3028 - accuracy: 0.9004\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1379 - accuracy: 0.9609\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0655 - accuracy: 0.9864\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7833 - accuracy: 0.4943\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.4962\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5907 - accuracy: 0.7524\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3079 - accuracy: 0.8984\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1478 - accuracy: 0.9627\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0697 - accuracy: 0.9882\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7405 - accuracy: 0.4994\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5125\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5832 - accuracy: 0.7680\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2976 - accuracy: 0.9014\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1430 - accuracy: 0.9586\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0695 - accuracy: 0.9830\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7009 - accuracy: 0.5119\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.4978\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5770 - accuracy: 0.7707\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2995 - accuracy: 0.9046\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1391 - accuracy: 0.9592\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0633 - accuracy: 0.9888\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7582 - accuracy: 0.4973\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7120 - accuracy: 0.4972\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6398 - accuracy: 0.6467\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3654 - accuracy: 0.8392\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1785 - accuracy: 0.9327\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1109 - accuracy: 0.9612\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.1677 - accuracy: 0.5093\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7116 - accuracy: 0.5033\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6060 - accuracy: 0.6888\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3389 - accuracy: 0.8567\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1608 - accuracy: 0.9382\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0844 - accuracy: 0.9707\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5603 - accuracy: 0.4863\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7128 - accuracy: 0.5054\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6353 - accuracy: 0.6588\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3502 - accuracy: 0.8501\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1657 - accuracy: 0.9390\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0951 - accuracy: 0.9663\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5112 - accuracy: 0.4958\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7142 - accuracy: 0.4985\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6398 - accuracy: 0.6494\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3665 - accuracy: 0.8409\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1752 - accuracy: 0.9306\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0957 - accuracy: 0.9662\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.3993 - accuracy: 0.5075\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7115 - accuracy: 0.5084\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6172 - accuracy: 0.6762\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3487 - accuracy: 0.8517\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1674 - accuracy: 0.9348\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0874 - accuracy: 0.9689\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5838 - accuracy: 0.4970\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.4978\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5910 - accuracy: 0.7430\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3084 - accuracy: 0.8994\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1417 - accuracy: 0.9617\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0692 - accuracy: 0.9860\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7123 - accuracy: 0.5087\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6962 - accuracy: 0.4974\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5787 - accuracy: 0.7737\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2927 - accuracy: 0.9057\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1346 - accuracy: 0.9633\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0644 - accuracy: 0.9869\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.8691 - accuracy: 0.4860\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.4895\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5860 - accuracy: 0.7603\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3030 - accuracy: 0.8972\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1398 - accuracy: 0.9601\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0677 - accuracy: 0.9867\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7778 - accuracy: 0.4970\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5013\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5908 - accuracy: 0.7549\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3063 - accuracy: 0.8995\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1466 - accuracy: 0.9559\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0650 - accuracy: 0.9862\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7120 - accuracy: 0.5090\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5033\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5729 - accuracy: 0.7776\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2846 - accuracy: 0.9064\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1278 - accuracy: 0.9661\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0625 - accuracy: 0.9878\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.8225 - accuracy: 0.4967\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7157 - accuracy: 0.5037\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6507 - accuracy: 0.6530\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3784 - accuracy: 0.8271\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1826 - accuracy: 0.9323\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1037 - accuracy: 0.9641\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.4381 - accuracy: 0.5006\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7169 - accuracy: 0.5087\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6323 - accuracy: 0.6663\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3436 - accuracy: 0.8507\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1775 - accuracy: 0.9321\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1018 - accuracy: 0.9629\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5026 - accuracy: 0.4815\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7152 - accuracy: 0.5091\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6380 - accuracy: 0.6712\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3525 - accuracy: 0.8528\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1751 - accuracy: 0.9334\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0942 - accuracy: 0.9678\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.6509 - accuracy: 0.4934\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7190 - accuracy: 0.4980\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6296 - accuracy: 0.6652\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3613 - accuracy: 0.8431\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1764 - accuracy: 0.9347\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1022 - accuracy: 0.9622\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.3662 - accuracy: 0.5081\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7170 - accuracy: 0.4956\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6308 - accuracy: 0.6683\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3348 - accuracy: 0.8584\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1680 - accuracy: 0.9370\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0904 - accuracy: 0.9651\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5915 - accuracy: 0.4994\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.4811\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5983 - accuracy: 0.7428\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3282 - accuracy: 0.8819\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0271 - accuracy: 0.5024\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6962 - accuracy: 0.4900\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5825 - accuracy: 0.7593\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3093 - accuracy: 0.8952\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1072 - accuracy: 0.4931\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5006\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5845 - accuracy: 0.7705\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3119 - accuracy: 0.8910\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0895 - accuracy: 0.4991\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.4966\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5955 - accuracy: 0.7527\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3298 - accuracy: 0.8879\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0276 - accuracy: 0.5137\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.4998\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5751 - accuracy: 0.7760\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3089 - accuracy: 0.8969\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1062 - accuracy: 0.5087\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7153 - accuracy: 0.4864\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6379 - accuracy: 0.6543\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3697 - accuracy: 0.8411\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0087 - accuracy: 0.5069\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7134 - accuracy: 0.4935\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6337 - accuracy: 0.6643\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3619 - accuracy: 0.8439\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.2139 - accuracy: 0.4910\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7161 - accuracy: 0.4963\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6401 - accuracy: 0.6494\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3827 - accuracy: 0.8330\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0587 - accuracy: 0.4925\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7175 - accuracy: 0.4964\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6490 - accuracy: 0.6416\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3668 - accuracy: 0.8457\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.9984 - accuracy: 0.5104\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7159 - accuracy: 0.5020\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6276 - accuracy: 0.6695\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3585 - accuracy: 0.8468\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1103 - accuracy: 0.5113\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5048\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5938 - accuracy: 0.7601\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3301 - accuracy: 0.8881\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0338 - accuracy: 0.5027\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5001\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5849 - accuracy: 0.7722\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3106 - accuracy: 0.8937\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1231 - accuracy: 0.4872\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6966 - accuracy: 0.4969\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5839 - accuracy: 0.7594\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3137 - accuracy: 0.8942\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1010 - accuracy: 0.4988\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.4925\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5961 - accuracy: 0.7569\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3263 - accuracy: 0.8886\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0503 - accuracy: 0.5164\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5114\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5753 - accuracy: 0.7760\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3021 - accuracy: 0.8980\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1931 - accuracy: 0.5006\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7206 - accuracy: 0.4958\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6638 - accuracy: 0.6338\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3837 - accuracy: 0.8315\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0772 - accuracy: 0.4976\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7207 - accuracy: 0.5017\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6453 - accuracy: 0.6585\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3569 - accuracy: 0.8468\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1334 - accuracy: 0.4919\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7204 - accuracy: 0.5029\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6382 - accuracy: 0.6599\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3900 - accuracy: 0.8304\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1028 - accuracy: 0.4943\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7178 - accuracy: 0.5005\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6439 - accuracy: 0.6550\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3900 - accuracy: 0.8312\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.2951 - accuracy: 0.5090\n",
            "Epoch 1/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7211 - accuracy: 0.4966\n",
            "Epoch 2/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6282 - accuracy: 0.6719\n",
            "Epoch 3/3\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3690 - accuracy: 0.8409\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1336 - accuracy: 0.4982\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5037\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5943 - accuracy: 0.7595\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3249 - accuracy: 0.8916\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1668 - accuracy: 0.9510\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0799 - accuracy: 0.9831\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.6292 - accuracy: 0.5054\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.4944\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5843 - accuracy: 0.7552\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3155 - accuracy: 0.8925\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1468 - accuracy: 0.9613\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0722 - accuracy: 0.9854\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7580 - accuracy: 0.4860\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5006\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5814 - accuracy: 0.7672\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3180 - accuracy: 0.8919\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1615 - accuracy: 0.9538\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0774 - accuracy: 0.9821\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7002 - accuracy: 0.5009\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5068\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5830 - accuracy: 0.7740\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3206 - accuracy: 0.8924\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1613 - accuracy: 0.9524\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0760 - accuracy: 0.9829\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.6498 - accuracy: 0.5018\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6968 - accuracy: 0.4944\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5917 - accuracy: 0.7400\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3254 - accuracy: 0.8869\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1615 - accuracy: 0.9510\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0808 - accuracy: 0.9815\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7216 - accuracy: 0.5015\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7148 - accuracy: 0.4962\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6600 - accuracy: 0.6375\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3852 - accuracy: 0.8319\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2140 - accuracy: 0.9146\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1184 - accuracy: 0.9547\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.1359 - accuracy: 0.5021\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7140 - accuracy: 0.5064\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6316 - accuracy: 0.6586\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3428 - accuracy: 0.8564\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1872 - accuracy: 0.9286\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1130 - accuracy: 0.9595\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.5096 - accuracy: 0.4887\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7162 - accuracy: 0.4992\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6392 - accuracy: 0.6543\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3728 - accuracy: 0.8376\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1979 - accuracy: 0.9208\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1221 - accuracy: 0.9523\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.2828 - accuracy: 0.4890\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7168 - accuracy: 0.5075\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.6405\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3849 - accuracy: 0.8374\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2005 - accuracy: 0.9230\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1183 - accuracy: 0.9569\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.0324 - accuracy: 0.5149\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7146 - accuracy: 0.4987\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6236 - accuracy: 0.6672\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3553 - accuracy: 0.8434\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1907 - accuracy: 0.9276\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1081 - accuracy: 0.9613\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.3997 - accuracy: 0.4901\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.4942\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5873 - accuracy: 0.7384\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3199 - accuracy: 0.8888\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1590 - accuracy: 0.9554\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0806 - accuracy: 0.9819\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.6399 - accuracy: 0.5075\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5034\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5799 - accuracy: 0.7781\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3098 - accuracy: 0.8955\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1433 - accuracy: 0.9606\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0696 - accuracy: 0.9872\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.8037 - accuracy: 0.4913\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5033\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5815 - accuracy: 0.7683\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3039 - accuracy: 0.8962\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1496 - accuracy: 0.9545\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0734 - accuracy: 0.9853\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7579 - accuracy: 0.4973\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5040\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5905 - accuracy: 0.7647\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3166 - accuracy: 0.8955\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1630 - accuracy: 0.9512\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0729 - accuracy: 0.9836\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.6618 - accuracy: 0.5110\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.4961\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.5841 - accuracy: 0.7591\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3073 - accuracy: 0.9029\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1529 - accuracy: 0.9561\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.0719 - accuracy: 0.9825\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7560 - accuracy: 0.4982\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7204 - accuracy: 0.4932\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6589 - accuracy: 0.6414\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.4018 - accuracy: 0.8196\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2189 - accuracy: 0.9152\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1230 - accuracy: 0.9540\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.2026 - accuracy: 0.4943\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7193 - accuracy: 0.5033\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6173 - accuracy: 0.6757\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3540 - accuracy: 0.8474\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1915 - accuracy: 0.9236\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1284 - accuracy: 0.9539\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.4257 - accuracy: 0.4878\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7251 - accuracy: 0.4936\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6430 - accuracy: 0.6519\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3481 - accuracy: 0.8522\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1963 - accuracy: 0.9247\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1188 - accuracy: 0.9537\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.2032 - accuracy: 0.4913\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7206 - accuracy: 0.4962\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6400 - accuracy: 0.6497\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3749 - accuracy: 0.8362\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2039 - accuracy: 0.9173\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1146 - accuracy: 0.9575\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.0381 - accuracy: 0.5066\n",
            "Epoch 1/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.7205 - accuracy: 0.5048\n",
            "Epoch 2/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.6379 - accuracy: 0.6570\n",
            "Epoch 3/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.3600 - accuracy: 0.8480\n",
            "Epoch 4/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.2080 - accuracy: 0.9185\n",
            "Epoch 5/5\n",
            "419/419 [==============================] - 2s 4ms/step - loss: 0.1235 - accuracy: 0.9546\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 2.3027 - accuracy: 0.4958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 43.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1047/1047 [==============================] - 4s 3ms/step - loss: 0.7311 - accuracy: 0.4994\n",
            "Epoch 2/3\n",
            "1047/1047 [==============================] - 4s 3ms/step - loss: 0.6992 - accuracy: 0.6172\n",
            "Epoch 3/3\n",
            "1047/1047 [==============================] - 4s 3ms/step - loss: 0.4621 - accuracy: 0.7931\n",
            "\n",
            "Best score: 0.51\n",
            "Best of the observed hyperparameters:\n",
            "clf__batch_size: 16\n",
            "clf__drop_out: 0.3\n",
            "clf__epochs: 3\n",
            "clf__hiddenlayer_size: 200\n",
            "clf__input_size: 50000\n",
            "clf__learning_rate: 0.01\n",
            "clf__output_size: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VFeq0r0qxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1df8297-2247-4477-f256-76fc1a498eb8"
      },
      "source": [
        "print(f\"Time elapsed {(t1-t0)/60:0.3} minutes\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed 43.4 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzuE9_f_5HjR",
        "outputId": "bd7e09fc-13bb-4fcc-e20d-27f39d5a9841"
      },
      "source": [
        "means = gridsearch.cv_results_['mean_test_score'] \n",
        "\n",
        "GSCV_results = pd.DataFrame(list(zip(means, gridsearch.cv_results_['params'])), \n",
        "               columns =['Score', 'Parameters']) \n",
        "# sort by the score\n",
        "GSCV_results.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
        "print(GSCV_results.head(7))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Score                                         Parameters\n",
            "9   0.505791  {'clf__batch_size': 16, 'clf__drop_out': 0.3, ...\n",
            "16  0.504896  {'clf__batch_size': 16, 'clf__drop_out': 0.4, ...\n",
            "26  0.503522  {'clf__batch_size': 32, 'clf__drop_out': 0.2, ...\n",
            "3   0.503522  {'clf__batch_size': 16, 'clf__drop_out': 0.2, ...\n",
            "40  0.503403  {'clf__batch_size': 32, 'clf__drop_out': 0.4, ...\n",
            "21  0.503343  {'clf__batch_size': 16, 'clf__drop_out': 0.4, ...\n",
            "29  0.503045  {'clf__batch_size': 32, 'clf__drop_out': 0.2, ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nh6sXJQS4o_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "67e8b71d-3b46-4123-d8a2-f233a9d4ccf6"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# prepare test data\n",
        "ftm_test=vectorizer.transform(test_texts) # model needs to be Sequential for predicting\n",
        "class_numbers_test = label_encoder.transform(test_labels)\n",
        "\n",
        "# predict\n",
        "raw_predictions = gridsearch.predict(ftm_test)\n",
        "predictions=label_encoder.inverse_transform(raw_predictions)\n",
        "\n",
        "# results\n",
        "acc = accuracy_score(test_labels, predictions)\n",
        "print(f\"Test accuracy: {acc:0.2f}\")\n",
        "print()\n",
        "cf_mat = tf.math.confusion_matrix(\n",
        "    class_numbers_test, raw_predictions, num_classes=None, weights=None\n",
        ")\n",
        "\n",
        "def plot_cf_matrix(mat):\n",
        "  sns.heatmap(mat, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Greens')\n",
        "  plt.title(\"Confusion matrix for test data\", fontsize = 16)\n",
        "  plt.ylabel(\"True class\", fontsize = 14)\n",
        "  plt.xlabel(\"Predicted class\", fontsize = 14)\n",
        "\n",
        "plot_cf_matrix(cf_mat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.50\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_kDEsq0t25F",
        "outputId": "eeda1ad8-a707-4344-b81b-9cabd4bc41cd"
      },
      "source": [
        "print(classification_report(test_labels, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.63      0.38      0.47        64\n",
            "         joy       0.74      0.72      0.73       121\n",
            "   not-anger       0.51      0.61      0.55       252\n",
            "     not-joy       0.24      0.20      0.22       157\n",
            "\n",
            "    accuracy                           0.50       594\n",
            "   macro avg       0.53      0.48      0.49       594\n",
            "weighted avg       0.50      0.50      0.49       594\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc8dqxoiLYMc"
      },
      "source": [
        "# From NLP deep learning and ADA courses:\n",
        "\n",
        "#  Additionally accuracy on the permutated test sets are computed for the purpose of \n",
        "# ensuring that performance with actual data differs from performance with junk data.\n",
        "\n",
        " # accuracy on permutated test set\n",
        "    # permutated = np.random.permutation(df_test.reg)\n",
        "    # perm_accuracy, _ = mnb_compute(vocab['test_fm'], permutated, mnb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZyHlbKGUr9M"
      },
      "source": [
        "#### Sidenote: How to tackle randomness?\n",
        "\n",
        "What should be done to make sure our results are not random: resuffle the data, run the CV again. Do this x 10 and take the average.\n",
        "\n",
        "Run with permuted data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW0I7QCLwy78"
      },
      "source": [
        "For joy and not-anger the model finds a bit less than 70 percent of the relevant samples (recall). For joy also precision (relevance of the selected ones) is rather high, close to 80 percent, so the model detects joy the best. This can be seen also from the f1-score. \n",
        "\n",
        "Not-anger, the biggest class in our data, has the second highest f1-score and recall close to 70 percent, but since significant amount of not-joy is assigned under not-anger label, the precision for the class is less than 50 percent. Anger seems to be difficult to detect (recall less than 30 percent) but when the model does so, precision is colse to 60 percent. As already mentioned, not-joy is frequently classified to be not-anger, and that is why all the performance measures for this class are poor.\n",
        "\n",
        "We reach test accuracy around 50 percent. This slightly outperforms our rather naive baseline of guessing the most common class but does not give us results to celebrate with.\n",
        "\n",
        "Initially we had one data set for \"joy\" and \"not-joy\" and other for \"anger\" and \"not-anger\". Test accuracy was around 0.84 while testing with anger and joy data separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yp7UjqWi10b",
        "outputId": "60eefd92-7854-42bc-9055-a89951b80b03"
      },
      "source": [
        "print(classification_report(test_labels, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.59      0.27      0.37        64\n",
            "         joy       0.76      0.68      0.72       121\n",
            "   not-anger       0.49      0.67      0.56       252\n",
            "     not-joy       0.21      0.14      0.17       157\n",
            "\n",
            "    accuracy                           0.49       594\n",
            "   macro avg       0.51      0.44      0.45       594\n",
            "weighted avg       0.48      0.49      0.47       594\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xQlj1JCMNkM"
      },
      "source": [
        "# set the hyperparameter grid\n",
        "costs = np.logspace(-4, 2, num=6, endpoint = False) # array with values: 0.00001, 0.0001, 0.001, 0.01 0.1, 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IADzhlkdvktT"
      },
      "source": [
        "Reminder:\n",
        "\n",
        "Precision: TP/TP+FP --> fraction of true positives of all the items predicted positive. i.e. relevance among the selected items\n",
        "\n",
        "Recall: TP/TP+FN --> fraction of true positives of all the truly positives. i.e relevant items\n",
        "\n",
        "- Optimize precision: To achieve high precision we can set high threshold for predicting positive. But this would lead to discarding many positives and thus high FN and bad recall\n",
        "\n",
        "- Optimize recall: To achieve high recall we must minimize FP. This is achieved by setting a low thrwshold for predicting positive. But this again leads to bad precision!\n",
        "\n",
        "- Goal: Seek for balance. This migt be achieved by setting the metrics used in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY0U--xwTc1X"
      },
      "source": [
        "## Model performance evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyns-K6WDBcg"
      },
      "source": [
        "Let's take a look at the classified test documents. Can we get an insight what went wrong and why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujqZ0R_0G0Ih"
      },
      "source": [
        "df = pd.DataFrame({'true': y_test,\n",
        "                   'prediction':list(predictions),\n",
        "                   'document': X_test})\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ2CXq3qJf5R"
      },
      "source": [
        "df1 = df[(df['true']=='neg') & (df['prediction']=='pos')]\n",
        "print(df1.shape)\n",
        "df1.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKjtl32bOR_N"
      },
      "source": [
        "df2 = df[(df['true']=='pos') & (df['prediction']=='neg')]\n",
        "print(df2.shape)\n",
        "df2.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teYQ21JuTpbS"
      },
      "source": [
        "## Final notes"
      ]
    }
  ]
}
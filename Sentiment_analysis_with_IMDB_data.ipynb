{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_with_IMDB_data.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Sentiment-analysis-with-IMDB-data/blob/main/Sentiment_analysis_with_IMDB_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oSt2eIvPfYW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\n",
        "\n",
        "from pprint import pprint\n",
        "# from time import time\n",
        "# import logging"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeF_rZZMRRUX"
      },
      "source": [
        "## Load and inspect the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9mbjj9vv0Wv"
      },
      "source": [
        "The data is IMBD reviews made for Stanford University research project (https://www.aclweb.org/anthology/P11-1015/). To learn more about the data please visit the web page or read the README file printed below.\n",
        "\n",
        "For my purposes the test data is bi enough and I will use it for training, valdating and testing the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYMQPYZ5jUM0",
        "outputId": "e77b2b69-2d38-4b7b-ca73-286a6f1d6c2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%bash\n",
        "wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘aclImdb_v1.tar.gz’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1QE3NTLmAHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50237b15-7982-47ef-911a-eedf03f5b25d"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb\n",
        "cat README | head -1000"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Large Movie Review Dataset v1.0\n",
            "\n",
            "Overview\n",
            "\n",
            "This dataset contains movie reviews along with their associated binary\n",
            "sentiment polarity labels. It is intended to serve as a benchmark for\n",
            "sentiment classification. This document outlines how the dataset was\n",
            "gathered, and how to use the files provided. \n",
            "\n",
            "Dataset \n",
            "\n",
            "The core dataset contains 50,000 reviews split evenly into 25k train\n",
            "and 25k test sets. The overall distribution of labels is balanced (25k\n",
            "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
            "documents for unsupervised learning. \n",
            "\n",
            "In the entire collection, no more than 30 reviews are allowed for any\n",
            "given movie because reviews for the same movie tend to have correlated\n",
            "ratings. Further, the train and test sets contain a disjoint set of\n",
            "movies, so no significant performance is obtained by memorizing\n",
            "movie-unique terms and their associated with observed labels.  In the\n",
            "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
            "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
            "more neutral ratings are not included in the train/test sets. In the\n",
            "unsupervised set, reviews of any rating are included and there are an\n",
            "even number of reviews > 5 and <= 5.\n",
            "\n",
            "Files\n",
            "\n",
            "There are two top-level directories [train/, test/] corresponding to\n",
            "the training and test sets. Each contains [pos/, neg/] directories for\n",
            "the reviews with binary labels positive and negative. Within these\n",
            "directories, reviews are stored in text files named following the\n",
            "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
            "the star rating for that review on a 1-10 scale. For example, the file\n",
            "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
            "example with unique id 200 and star rating 8/10 from IMDb. The\n",
            "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
            "omitted for this portion of the dataset.\n",
            "\n",
            "We also include the IMDb URLs for each review in a separate\n",
            "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
            "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
            "are unable to link directly to the review, but only to the movie's\n",
            "review page.\n",
            "\n",
            "In addition to the review text files, we include already-tokenized bag\n",
            "of words (BoW) features that were used in our experiments. These \n",
            "are stored in .feat files in the train/test directories. Each .feat\n",
            "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
            "data.  The feature indices in these files start from 0, and the text\n",
            "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
            "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
            "(the) appears 7 times in that review.\n",
            "\n",
            "LIBSVM page for details on .feat file format:\n",
            "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
            "\n",
            "We also include [imdbEr.txt] which contains the expected rating for\n",
            "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
            "rating is a good way to get a sense for the average polarity of a word\n",
            "in the dataset.\n",
            "\n",
            "Citing the dataset\n",
            "\n",
            "When using this dataset please cite our ACL 2011 paper which\n",
            "introduces it. This paper also contains classification results which\n",
            "you may want to compare against.\n",
            "\n",
            "\n",
            "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "  month     = {June},\n",
            "  year      = {2011},\n",
            "  address   = {Portland, Oregon, USA},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "  pages     = {142--150},\n",
            "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "}\n",
            "\n",
            "References\n",
            "\n",
            "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
            "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
            "636-659.\n",
            "\n",
            "Contact\n",
            "\n",
            "For questions/comments/corrections please contact Andrew Maas\n",
            "amaas@cs.stanford.edu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSPALgVntPKT"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb/test/neg\n",
        "for f in *txt; do echo $f>> /content/neg_file_names.txt; done # appends filenames to file"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM3iXWLB6qwj"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb/test/pos\n",
        "for f in *txt; do echo $f>> /content/pos_file_names.txt; done # appends filenames to file"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1wqR79f0woZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c2a7ad-0cc1-4610-fd59-af411752d7fe"
      },
      "source": [
        "def open_file_by_looping(path, file):\n",
        "  f=open(file)\n",
        "  filenames = f.readlines()\n",
        "  reviews = []\n",
        "  for filename in filenames:\n",
        "    fname = path + filename\n",
        "    fname =fname.rstrip()\n",
        "    f = open(fname)\n",
        "    review = f.readlines()\n",
        "    # print(type(review))\n",
        "    reviews.append(review) # or extend?\n",
        "    f.close()\n",
        "  f.close()\n",
        "  flat_list = [item for sublist in reviews for item in sublist] # we have list of lists, it needs to be flattened\n",
        "  return flat_list\n",
        "\n",
        "neg_reviews = open_file_by_looping(\"aclImdb/test/neg/\", \"neg_file_names.txt\")\n",
        "print(\"Number of negative reviews:\", len(neg_reviews))\n",
        "\n",
        "pos_reviews = open_file_by_looping(\"aclImdb/test/pos/\", \"pos_file_names.txt\")\n",
        "print(\"Number of positive reviews:\", len(pos_reviews))\n",
        "\n",
        "reviews=neg_reviews+pos_reviews\n",
        "len(reviews)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of negative reviews: 25000\n",
            "Number of positive reviews: 25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkqVNVyfVZ9L"
      },
      "source": [
        "Data quality affects the performance of all machine learning algorithms and neural networks. Poor data can not be improved even with a sophisticated algorithm. In this case our data is balanced (classes have equal 50 % share) and well behaving in many aspects.\n",
        "\n",
        "This is seldom true in real life applications. In these cases data balance must be taken care of with for example stratification or giving different weigths to different classes. If data is grouped or datarecords are not independent, even more caution should be given to the training process since this might lead to test data \"leaking\" into training data and thus highly optimistic model performance measeures. \n",
        "\n",
        "For these reasons one should familiarize her with a new dataset before rushing into further steps of modeling. If it is discovered that the data is imbalanced, grouped etc. we can fix the issues uprising from the nature of the data before we feed it to the algorithm, or at least take it in account when analysing the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSktZXAh7QMt",
        "outputId": "06b836e8-4ded-44c5-a8d7-8bea0def4f6a"
      },
      "source": [
        "reviews=neg_reviews+pos_reviews\n",
        "len(reviews)\n",
        "\n",
        "# make labels for the reviews:\n",
        "labels = ['neg']*len(neg_reviews) + ['pos']*len(pos_reviews)\n",
        "print(len(labels))\n",
        "\n",
        "# make shuffled indices and shuffle both of labels and reviews with them\n",
        "indices = list(range(len(labels)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "labels = [labels[index] for index in indices]\n",
        "reviews = [reviews[index] for index in indices]\n",
        "\n",
        "for label, text in zip(labels[:10], reviews[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "label: neg \n",
            "text: The murder of the Red Comyn in Grayfriars Abbey was a long way from one of the most horrendous things ever done in the Scottish War of Independence and fights (and killing) in churches wasn't unusual at all. Not that much later Robert Bruces wife, daughter, two of his sisters were captured during a fight in a church in which people were killed. And comparing it to the massacre of Berwick in which the English slaughtered at least 8000 non-combatants (some, yes, in churches) is ridiculous.<br /><br />That said this is not a well-made movie. It is slightly antidote to the absolutely RIDICULOUS sniveling representation of Robert Bruce in Braveheart. Whatever Bruce was, it wasn't a wuss.<br /><br />Too bad that they didn't do a better job of this because someone should make a really GOOD movie of a war that is so amazing that it sounds like something someone made up going from complete defeat at the Battle of Methven to a secret return from hiding to a long guerrilla war to Bannockburn. This isn't it though. Poorly made and to a large extent poorly written and acted. Too bad! \n",
            "\n",
            "label: pos \n",
            "text: Seems to have been made as a vehicle for W.C. Fields and Carol Dempster and they dominate it. Fields already has his film character well developed. Carol Dempster seems to dance through the film and her acting reminds me of Mary Pickford, who also worked a long time under D.W. Griffith. Typical of later Griffith films technically.<br /><br />Later remade as Poppy (the original title) with Fields in the same role. \n",
            "\n",
            "label: neg \n",
            "text: There are rumours that a fourth Underworld is going to happen. If so, than the third part, which is also a prequel, would be in the middle of the franchise. With prequels that succeed the original movies, you always ask yourself in what order should you watch the movies, so that it makes sense ...<br /><br />In this case, I guess it doesn't matter that much. The third Underworld movie isn't up to par with the other two. They had their obvious flaws too, but this one lacks a few things and it feels like a cash in. It seems like it's not going full throttle, which is a shame, because the actors sure could've used better material to work with.<br /><br />The story is OK, but it's nothing special. A nice movie, but Rhona Mitra couldn't fill the shoes of Beckinsale (yes she plays another character, I mean the void, that Kate B. left) ... \n",
            "\n",
            "label: neg \n",
            "text: This is one of the worst movies I have ever seen.<br /><br />What is the purpose of this movie? A bunch of Americans enters Nazi-occupied France and starts slaughtering Germans. You see them scalping their enemies and beating them to death with baseball bats. While making jokes, of course.<br /><br />Some will say that this movie is a parody of a certain genre. For a parody, it is neither witty nor funny. The contents is zero. It is exceptionally brutal and disgusting. Underneath lies a subtle political message, because it is again \"the good guys\" killing \"bad Nazis\". The whole plot is unthinkable if you turn it around. Could you imagine a storyline where Nazis (while making jokes) kill everybody in the Warsaw ghetto with flamethrowers? Probably not, but this movie is exactly about that, with the exception that is satisfies the weird moral expectations of a certain audience: slaughtering people is so cool when done by the right people.<br /><br />This movie only works because of the hidden Nazi-ideology underneath. It does not regard the enemy as people. And if the latter is supposed to be an element of the fun, I am happy to say that this kind of fun will always remain a mystery to me.<br /><br />Another mystery is how such violence can fascinate the American crowd while a bit of nudity will freak them out. But if a naked body is pornography, this movie with all its brutality is pure pornography at its very worst.<br /><br />Inglorious Basterds is a pointless, boring and tasteless waste of time and money. \n",
            "\n",
            "label: pos \n",
            "text: This was easily one of the weirder of the Ernest movies, especially in regards to the production design. What was up with the pink guard uniforms? Sadly, this film probably destroyed the Ernest series, turning the series into a straight-to-video series. However, Jim Varney gave one of his better performances by playing Nash, his criminal alter ego. A misstep in the series, but wasn't too bad in most regards.(the Electro Man routine was classic) \n",
            "\n",
            "label: neg \n",
            "text: this is the worst movie ive ever seen. And i have seen lots of movies. Me and my friends rented this one a wendsday evening. Man we had lots of fun. This movie is the worst most boring crap ive ever seen. But it makes you laugh! U will lay on the floor rolling around tryin to get some air. You wonder why? Just rent it and check for the keyboard playing girl at that sleazy russian bar. My mother would make a 1000 times better movie about her feedin the cats. \n",
            "\n",
            "label: pos \n",
            "text: I've tried to reconcile why so many bad reviews of this film, while the vast majority of reviews are given a rating of between 7 and 10. The reason may be this film is kind of hard to describe in a positive review, although a few have done that quite nicely already. This film is confusing, depressing, and doesn't have a happy ending. I still gave Pola X a rating of 10, because it is basically for me literature and art combined on film. That is really my favorite kind of filmmaking. I've only seen two of Carax's films: this one and Mauvis Sang. As with this film, I'm being somewhat pretentious when I call this one of Carax's best films- but I am. Carax has a minimalist style. If that type of film does not appeal to you and is boring, then it would be best not to watch this. But Pola X was less minimalist than Mauvis Sang, so it had quite a lot of intensity for a thriller- at least for my taste. I found it quite interesting and absorbing. The two lead roles did an excellent job acting. (I mean the lead and the young woman he thought was his half sister.) Catherine D. is always great, but her role was not very large or significant in the story. But everyone did a fine job. I thought the cult stuff was great. It may have not been very believable, but that is due to its being rather abstract. There is a lot going on between the lines in this film. This is a very Freudian psycho-thriller. \n",
            "\n",
            "label: neg \n",
            "text: Very strange screenplay by Cameron Crowe (following on the heels of his \"Fast Times at Ridgemont High\") has little inspiration and flails away at dumb gags. At least \"Fast Times\" had a fair share of satire and sensitivity behind its slapstick (courtesy of a good director, Amy Heckerling, and Crowe's undeniable penchant for capturing letter-perfect teen-speak); here, Chris Penn (Sean's brother, natch) is the goof-off who makes life hell for straight arrow Eric Stoltz, and the filmmakers seem to think he's hilarious. Jenny Wright has some good moments as a mall-worker, but Illan Mitchell-Smith is lost in a head-scratching subplot about a teen who seems to be infatuated with a shell-shocked ex-soldier. Queasy, confused nonsense given a shiny sheen and a soundtrack full of pop-rock tunes, but characters one would hope to avoid. Supporting players Lea Thompson, Rick Moranis, Lee Ving, and Sherilyn Fenn are wasted in stupid roles. * from **** \n",
            "\n",
            "label: pos \n",
            "text: I had such high hopes for Teletoon Retro to air this but instead of having shows such as this, ones that don't get the treatment that they deserve, they air things that I may have seen dozens of times before.<br /><br />The Centurions was the highlight of my pre-teen years. I know that may seem a little bit clichéd but it's true. After Duke from G.I. Joe, Jake Rockewell is another one of those cartoon characters that I really had a crush on.<br /><br />It's too bad that Teletoon Retro doesn't see it the same way people of my generation do. Otherwise Teletoon Retro would be a lot better than it is. \n",
            "\n",
            "label: neg \n",
            "text: Although I had previously watched this one some time ago on Italian TV, I found it to be a surprisingly tolerable potboiler this time round, buoyed by an international cast of familiar faces (including a bemused Joseph Cotten as the Baron) and, contrary to many another film of the Euro-Cult sub-genre, an incident-packed plot in place of lethargic pacing.<br /><br />The creature itself looks a bit dodgy and Cotten is a bit too old to be taken seriously as an eager scientist still dabbling in creating life-forms out of corpses (one would have thought that he would have made himself an army of them by now and not struggling at perfecting his technique still) but Ms. Neri does look good in and out of costume and reliable Herbert Fux probably comes off best as a lecherous grave-robber/blackmailer. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XCLyD7zNxoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae0329f-3da4-46de-802c-50e0ec8da533"
      },
      "source": [
        "# Remove HTML tags\n",
        "\n",
        "import re\n",
        "pattern1=r\"<br /><br />\" \n",
        "reviews = [re.sub(pattern1, \" \", item) for item in reviews]\n",
        "\n",
        "for label, text in zip(labels[:10], reviews[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: neg \n",
            "text: The murder of the Red Comyn in Grayfriars Abbey was a long way from one of the most horrendous things ever done in the Scottish War of Independence and fights (and killing) in churches wasn't unusual at all. Not that much later Robert Bruces wife, daughter, two of his sisters were captured during a fight in a church in which people were killed. And comparing it to the massacre of Berwick in which the English slaughtered at least 8000 non-combatants (some, yes, in churches) is ridiculous. That said this is not a well-made movie. It is slightly antidote to the absolutely RIDICULOUS sniveling representation of Robert Bruce in Braveheart. Whatever Bruce was, it wasn't a wuss. Too bad that they didn't do a better job of this because someone should make a really GOOD movie of a war that is so amazing that it sounds like something someone made up going from complete defeat at the Battle of Methven to a secret return from hiding to a long guerrilla war to Bannockburn. This isn't it though. Poorly made and to a large extent poorly written and acted. Too bad! \n",
            "\n",
            "label: pos \n",
            "text: Seems to have been made as a vehicle for W.C. Fields and Carol Dempster and they dominate it. Fields already has his film character well developed. Carol Dempster seems to dance through the film and her acting reminds me of Mary Pickford, who also worked a long time under D.W. Griffith. Typical of later Griffith films technically. Later remade as Poppy (the original title) with Fields in the same role. \n",
            "\n",
            "label: neg \n",
            "text: There are rumours that a fourth Underworld is going to happen. If so, than the third part, which is also a prequel, would be in the middle of the franchise. With prequels that succeed the original movies, you always ask yourself in what order should you watch the movies, so that it makes sense ... In this case, I guess it doesn't matter that much. The third Underworld movie isn't up to par with the other two. They had their obvious flaws too, but this one lacks a few things and it feels like a cash in. It seems like it's not going full throttle, which is a shame, because the actors sure could've used better material to work with. The story is OK, but it's nothing special. A nice movie, but Rhona Mitra couldn't fill the shoes of Beckinsale (yes she plays another character, I mean the void, that Kate B. left) ... \n",
            "\n",
            "label: neg \n",
            "text: This is one of the worst movies I have ever seen. What is the purpose of this movie? A bunch of Americans enters Nazi-occupied France and starts slaughtering Germans. You see them scalping their enemies and beating them to death with baseball bats. While making jokes, of course. Some will say that this movie is a parody of a certain genre. For a parody, it is neither witty nor funny. The contents is zero. It is exceptionally brutal and disgusting. Underneath lies a subtle political message, because it is again \"the good guys\" killing \"bad Nazis\". The whole plot is unthinkable if you turn it around. Could you imagine a storyline where Nazis (while making jokes) kill everybody in the Warsaw ghetto with flamethrowers? Probably not, but this movie is exactly about that, with the exception that is satisfies the weird moral expectations of a certain audience: slaughtering people is so cool when done by the right people. This movie only works because of the hidden Nazi-ideology underneath. It does not regard the enemy as people. And if the latter is supposed to be an element of the fun, I am happy to say that this kind of fun will always remain a mystery to me. Another mystery is how such violence can fascinate the American crowd while a bit of nudity will freak them out. But if a naked body is pornography, this movie with all its brutality is pure pornography at its very worst. Inglorious Basterds is a pointless, boring and tasteless waste of time and money. \n",
            "\n",
            "label: pos \n",
            "text: This was easily one of the weirder of the Ernest movies, especially in regards to the production design. What was up with the pink guard uniforms? Sadly, this film probably destroyed the Ernest series, turning the series into a straight-to-video series. However, Jim Varney gave one of his better performances by playing Nash, his criminal alter ego. A misstep in the series, but wasn't too bad in most regards.(the Electro Man routine was classic) \n",
            "\n",
            "label: neg \n",
            "text: this is the worst movie ive ever seen. And i have seen lots of movies. Me and my friends rented this one a wendsday evening. Man we had lots of fun. This movie is the worst most boring crap ive ever seen. But it makes you laugh! U will lay on the floor rolling around tryin to get some air. You wonder why? Just rent it and check for the keyboard playing girl at that sleazy russian bar. My mother would make a 1000 times better movie about her feedin the cats. \n",
            "\n",
            "label: pos \n",
            "text: I've tried to reconcile why so many bad reviews of this film, while the vast majority of reviews are given a rating of between 7 and 10. The reason may be this film is kind of hard to describe in a positive review, although a few have done that quite nicely already. This film is confusing, depressing, and doesn't have a happy ending. I still gave Pola X a rating of 10, because it is basically for me literature and art combined on film. That is really my favorite kind of filmmaking. I've only seen two of Carax's films: this one and Mauvis Sang. As with this film, I'm being somewhat pretentious when I call this one of Carax's best films- but I am. Carax has a minimalist style. If that type of film does not appeal to you and is boring, then it would be best not to watch this. But Pola X was less minimalist than Mauvis Sang, so it had quite a lot of intensity for a thriller- at least for my taste. I found it quite interesting and absorbing. The two lead roles did an excellent job acting. (I mean the lead and the young woman he thought was his half sister.) Catherine D. is always great, but her role was not very large or significant in the story. But everyone did a fine job. I thought the cult stuff was great. It may have not been very believable, but that is due to its being rather abstract. There is a lot going on between the lines in this film. This is a very Freudian psycho-thriller. \n",
            "\n",
            "label: neg \n",
            "text: Very strange screenplay by Cameron Crowe (following on the heels of his \"Fast Times at Ridgemont High\") has little inspiration and flails away at dumb gags. At least \"Fast Times\" had a fair share of satire and sensitivity behind its slapstick (courtesy of a good director, Amy Heckerling, and Crowe's undeniable penchant for capturing letter-perfect teen-speak); here, Chris Penn (Sean's brother, natch) is the goof-off who makes life hell for straight arrow Eric Stoltz, and the filmmakers seem to think he's hilarious. Jenny Wright has some good moments as a mall-worker, but Illan Mitchell-Smith is lost in a head-scratching subplot about a teen who seems to be infatuated with a shell-shocked ex-soldier. Queasy, confused nonsense given a shiny sheen and a soundtrack full of pop-rock tunes, but characters one would hope to avoid. Supporting players Lea Thompson, Rick Moranis, Lee Ving, and Sherilyn Fenn are wasted in stupid roles. * from **** \n",
            "\n",
            "label: pos \n",
            "text: I had such high hopes for Teletoon Retro to air this but instead of having shows such as this, ones that don't get the treatment that they deserve, they air things that I may have seen dozens of times before. The Centurions was the highlight of my pre-teen years. I know that may seem a little bit clichéd but it's true. After Duke from G.I. Joe, Jake Rockewell is another one of those cartoon characters that I really had a crush on. It's too bad that Teletoon Retro doesn't see it the same way people of my generation do. Otherwise Teletoon Retro would be a lot better than it is. \n",
            "\n",
            "label: neg \n",
            "text: Although I had previously watched this one some time ago on Italian TV, I found it to be a surprisingly tolerable potboiler this time round, buoyed by an international cast of familiar faces (including a bemused Joseph Cotten as the Baron) and, contrary to many another film of the Euro-Cult sub-genre, an incident-packed plot in place of lethargic pacing. The creature itself looks a bit dodgy and Cotten is a bit too old to be taken seriously as an eager scientist still dabbling in creating life-forms out of corpses (one would have thought that he would have made himself an army of them by now and not struggling at perfecting his technique still) but Ms. Neri does look good in and out of costume and reliable Herbert Fux probably comes off best as a lecherous grave-robber/blackmailer. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-bnE6jxnusp"
      },
      "source": [
        "# s = r\"\\s\\tWord\"\n",
        "# prog = re.compile(s)\n",
        "# prog\n",
        "\n",
        "# re.sub(some_regex, some_replacement.replace('\\\\', '\\\\\\\\'), input_string)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B9giOL-MsY9"
      },
      "source": [
        "# import re\n",
        "# pattern1=r\"<br /><br />\" # something fishy is going on with HTML tags. Get rid of them\n",
        "# s = \"\\\\'\"\n",
        "# print(s)\n",
        "# # pattern2=re.escape(r\"\\'\")\n",
        "# # print(\"p2:\", pattern2)\n",
        "# pattern3 = s\n",
        "# print(pattern3)\n",
        "# # print(pattern3.replace('\\\\', '\\\\\\\\'))\n",
        "# print(chr(39))\n",
        "\n",
        "# fixed = re.sub(pattern3, chr(39), reviews[8])\n",
        "# fixed_n = re.sub(pattern1, \" \", fixed)\n",
        "# fixed_n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox8EQgz3IEM1"
      },
      "source": [
        "## Train-Dev-Test split the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbPsc3SWIyKd",
        "outputId": "240d1409-20f8-47f9-af1f-a620d6f710ad"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.33)\n",
        "\n",
        "print(len(X_train), len(y_train))\n",
        "\n",
        "for label, text in zip(y_train[:10], X_train[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33500 33500\n",
            "label: neg \n",
            "text: Without saying how it ended, it is sufficient to say that the whole thing degenerates from about five minutes before the end. If the standard had been maintained throughout, the movie would be worth a seven. One wonders in a way why a woman was added to the cast. (Well - not really!) The premise is a good one The situation the victims find themselves in is pretty terrifying and it's rather well done, but you get the impression the makers of the film lost interest towards the end, or as a previous contributor said, they changed writers and handed over to someone else. \n",
            "\n",
            "label: neg \n",
            "text: What. Uh... This movie is so dissociative and messed up that I literally lost a bit of my sanity after it was over. I will never be the same person again. I'm trying to put my finger on what, exactly, is so completely insane about it... It's not just the hilarious techno music, or the \"outside of time\" medieval/Blade Runner/wild west/Highlander setting, or the weird CGI \"Grendel\" monster that looks like a man made out of animated sausages, or even the \"Grendel's mother\" monster, which looks like some Alabama table-dancer who grew claws and tentacles when she stayed in the tanning bed too long. All of those things are weird, but what's really the strangest thing in this movie is the acting. I simply can't explain. This script is obviously, hellishly silly, but the actors exude deadly seriousness through it all. Lambert is always weird, and usually kind of boring, but for this one he's gone into Dolph Lundgrin territory: I can't help but just start laughing every time he talks. I will give this movie some credit for being completely scatter-brained and crazy as opposed to conservative and boring. I'll always take a bizarre disaster of a film over an utterly mediocre one. Warning: if you are planning on watching Christopher Lambert as Beowulf, be prepared to spend several hours thereafter wandering the streets in some kind of nightmarish, hyperactive-catatonic daze. It's true. When I was done with my Beowulf spirit journey, I woke up in the middle of the Siberian tundra in a puddle of blood and milk. There was a dead wolf lying next to me, and I later found I had a handful of human teeth in my shirt pocket. My VHS copy of Beowulf was sitting on a hastily-constructed stone altar nearby, enshrined with candles and wilted flowers. The tape told me to walk. I rose and I walked. \n",
            "\n",
            "label: pos \n",
            "text: In the year 2000 (keep in mind, this is two years ago, not four), two men had the motivation to create the most miraculous piece of art on this side of the Mississippi. Thanks to Jere Cunningham and Tom Flynn, the world can now enjoy Second String, a delicious TV movie depicting a tale of a rag-tag gang of second stringers (thus the title) who are thrust into the position of starters due to an order of bad oysters. Because of the motivational direction of both the director Robert Lieberman and the Buffalo Bills' last minute QB, Dan \"Give 'em hell\" Heller (portrayed by Canadian actor, Gil Bellows) the oft Super Bowl snake-bitten Bills find themselves in the ultimate position. With an intriguing mix of internal and external conflict, a love story, comraderie that only the fine sport of football can bring, and an overall theory that the underdog can compete, Second String is an excellent movie worthy of viewing every possible moment that it appears on TNT. The only thing potentially bad about this production is the spelling of the Costume Designer's first name, Jenifur Jarvis.  \n",
            "\n",
            "label: neg \n",
            "text: The operative rule in the making of this film seems to have been \"never make a 1 minute scene when you can make it a 10 minute scene.\" This was a principle set right from the start with an interminably long portrayal of the graduation of the Harvard class of 1870. The point of that scene, I suppose, was to introduce some of the primary figures in the story and give a bit of their background - which is somewhat effective when comparing the idealism of the Harvard graduation ceremony to the realism of life in Johnson County, Wyoming, but it just keeps going and going, and that sets the stage for a film that features repeated stretches of mind-numbing nothingness, made even worse by the fact that I found a significant amount of the dialogue to be almost incoherent. In the end, I couldn't even watch this in one sitting. I got through about half of it and had to set it aside for a couple of days before I could drag myself back to see how it turned out. My reaction to this movie in many ways is a shame, because there are positives here. The performances are generally of a high calibre, especially from Kris Kristofersson as Averill, Christopher Walken as Champion and Isabelle Huppert as Ella. The basic story - interspersed as it is around that ever-present mind-numbing nothingness - is potentially interesting, focusing on the efforts of immigrants to establish themselves in Johnson County and a local cattle company's efforts to stop them by killing a number of them in collaboration with the government and the military. There's also some absolutely breathtaking scenery shots. Having said that, the whole thing could frankly have been done in half the time - and should have been. In the end, all those potential positives are washed out by - again - the mind-numbing nothingness that the movie seems to revolve around. Seriously - 2/10. \n",
            "\n",
            "label: pos \n",
            "text: Loved this movie!! Great acting by Carla Gugino. Interesting story about a kidnapping that goes horribly wrong (don't they all?). Some surprising twists and turns in the film and the plot was easy to follow without being so convoluted as to be totally incomprehensible. It was a totally unexpected delight. More \"Quentin-ish\" than most films try to be. \n",
            "\n",
            "label: pos \n",
            "text: Of course, how could he. He obviously co-opted several aspects from that excellent movie, which was also based on the sensational French case of the self-described \"doctor in the World Health Organization\" who murdered his family and himself when finally unmasked as a fraud. Emilio refers to his son as \"monster,\" he sings to the radio in his car, he hangs out on park benches, and he specializes in investment schemes to defraud his family and friends -- all of this and more directly lifted from \"Time Out,\" which came out the year before \"Nobody's Life.\" It's too bad because this movie is pretty good on its own, with good acting and writing. Whereas Vincent from \"Time Out\" is a much more subtle character who seems to have a sense of ethics even though at times it gets twisted into knots, the protagonist here seems devoid of any character at all save for his winning looks and charm. Seriously, the part where he used X-rays that show his mother-in-law's cancer to bilk more money from his father, then utilizes a subtle twist on the same scam to avoid eviction from his fancy home for failing to pay the lease on time -- it's almost too much. The guy has no shame whatsoever, In fact, he's more like the lead in \"Stepfather\" than some poor schmuck who gets fired and is so humiliated that he can't face the disappointment of his family and friends and feels forced to invent a shiny new life for himself, as Vincent did in \"Time Out.\" Thus, one could feel the tension mounting in \"Nobody's Life\" and the violent conclusion coming. One thing \"Nobody's Life\" has that \"Time Out\" definitely lacked was a love interest apart from the protagonist's trusting wife. It's not hard to understand how the sexy babysitter was able to fascinate and ensnare Emile to the degree that he ignored the danger of her natural curiosity and allowed it to lay bare his less than carefully constructed con. Given the reservations mentioned, this is a pretty good movie that we found entertaining. If you long for something touching on similar elements that goes a might deeper and is more intellectually and spiritually satisfying, I strongly suggest \"Time Out.\" \n",
            "\n",
            "label: neg \n",
            "text: This movie is not realistic at all, more of a comedy than a serious war film. Very old-fashioned too. Maybe I was just expecting it to be on a same level with \"Platoon\". I wonder, why 50% of the voters gave it 10? Something must be wrong. \n",
            "\n",
            "label: pos \n",
            "text: Three ten-year-old children born at the same time during a solar eclipse begin to slyly murder anybody that offends them. While killer kid movies weren't exactly new at the time of this twisted 80's slasher the theme of children as murderers works nicely for this film. Bloody Birthday does deliver some good chills and suspense, while managing to be a competent killer thriller with some strange qualities. It straddles a fine line between cheesy and creepy, but it does remain entertaining throughout with an interesting plot. There's some strong murder scenes, as well as a good bit of nudity to establish this as a solid slasher guilty pleasure. The cast does a fairly good job. Young stars Elizabeth Hoy and K.C. Martel deliver some menacing performances, while rising star Julie Brown does a striptease before a memorable murder scene. Veteran star Susan Strasberg does well as the teacher and Jose Ferrer has a cameo appearance. All around this off-beat slasher entry isn't bad, though it's admittedly not flawless, but it is well worth watching for genre fans. *** out of **** \n",
            "\n",
            "label: pos \n",
            "text: This is a slightly uneven entry with one standout sequence involving an over-the-hill gang reminiscing in the diner that once - thirty years previously - was their hideout; one ho-hum duologue between two ageing rock musos; a noirish kidnap turned on its head and an opening sequence (plus epilogue) involving heist artist wannabe Edward Baer and current 'hot' property Anna Magloulis which has its moments. No movie in which Jean Rochefort appears can be dismissed lightly and here he shines as one of the over-the-hill quintet, indeed the film is worth seeing for Rochefort alone but each of the sequences has something to offer and it's definitely worth a look. \n",
            "\n",
            "label: pos \n",
            "text: This film recreates Lindbergh's historic flight across the Atlantic while touching on episodes in his aviation career through flashbacks. Stewart was about 20 years too old to be playing the young flier, but his fine performance, particularly in the solo flight sequences, makes this a minor quibble. Waxman's rousing score is a big plus. Despite the long running time, Wilder manages to make it quite exciting and is able to sustain the drama even though the outcome is known. What a year 1957 was for Wilder: besides this, he also wrote and directed \"Love in the Afternoon\" and \"Witness for the Prosecution.\" And his next two were \"Some Like it Hot\" and \"The Apartment.\" What a run! \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti6xr4TXGwab"
      },
      "source": [
        "## Preprocessing: Tfidf Vectorizer \n",
        "\n",
        "Since we are dealing with text data we need to transform it to format a basic SVM can handle. For that purpose I use sklearn TfidfVectorizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNbQMIaSzdU5"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=50000)\n",
        "fm_train = vectorizer.fit_transform(X_train)\n",
        "fm_test = vectorizer.transform(X_test)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuuJbYX_1-8x",
        "outputId": "f9f9260c-8ef7-437d-eaae-f594d1223ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# input data size is limited by the vectorizer\n",
        "\n",
        "print(f\"We have {fm_train.shape[0]} rows and {fm_train.shape[1]} columns in the training data\")\n",
        "print(f\"And {fm_test.shape[0]} rows and {fm_test.shape[1]} columns in the training data\")\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 33500 rows and 50000 columns in the training data\n",
            "And 16500 rows and 50000 columns in the training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPXtqJ9ELYK-",
        "outputId": "1cabf923-8089-4693-9611-d5b9b2a96287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# type of the input data is scipy.sparse.csr.csr_matrix\n",
        "print(type(fm_train))\n",
        "\n",
        "# What does it mean? It looks like this:\n",
        "print(fm_train[0:2:])\n",
        "\n",
        "# Each row of the sparse matrix contains the indices of the tfidf matrix \n",
        "#(for example (0, 12247) and the tfidf weight).\n",
        "# The row index is the document index (the number of the review) and the column \n",
        "# index is for the token "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "  (0, 13199)\t0.09941007866240618\n",
            "  (0, 40011)\t0.09720000564462355\n",
            "  (0, 29653)\t0.07463746999238682\n",
            "  (0, 18383)\t0.16408338868089525\n",
            "  (0, 49305)\t0.13276218932659853\n",
            "  (0, 7366)\t0.13785030116220048\n",
            "  (0, 43712)\t0.05078988207012413\n",
            "  (0, 36551)\t0.09847205302615367\n",
            "  (0, 9260)\t0.2506423047981331\n",
            "  (0, 32344)\t0.13157953301868805\n",
            "  (0, 2368)\t0.040067198120549485\n",
            "  (0, 29370)\t0.05194563556205639\n",
            "  (0, 44535)\t0.13083017720105566\n",
            "  (0, 21816)\t0.11904501361315997\n",
            "  (0, 25191)\t0.1099687038650605\n",
            "  (0, 14964)\t0.043764256249960866\n",
            "  (0, 29093)\t0.028977915286299553\n",
            "  (0, 25677)\t0.14185450401662508\n",
            "  (0, 20897)\t0.1416972537134192\n",
            "  (0, 16614)\t0.06403974351093289\n",
            "  (0, 49637)\t0.04386083341751073\n",
            "  (0, 6095)\t0.03683884375959961\n",
            "  (0, 12148)\t0.08953019366645623\n",
            "  (0, 33836)\t0.09601396927197847\n",
            "  (0, 1717)\t0.05690738615936637\n",
            "  :\t:\n",
            "  (1, 2368)\t0.0330230885226977\n",
            "  (1, 29370)\t0.06421981328863342\n",
            "  (1, 25191)\t0.04531769643008442\n",
            "  (1, 14964)\t0.018035088252543168\n",
            "  (1, 29093)\t0.1313586096918897\n",
            "  (1, 49637)\t0.018074887346351466\n",
            "  (1, 6095)\t0.06072460544343168\n",
            "  (1, 12148)\t0.03689506191581317\n",
            "  (1, 1717)\t0.10553095580665552\n",
            "  (1, 33996)\t0.025129327421413817\n",
            "  (1, 28751)\t0.017198795643027252\n",
            "  (1, 48047)\t0.06559109798793558\n",
            "  (1, 20954)\t0.0767892150278685\n",
            "  (1, 29246)\t0.03530220052709239\n",
            "  (1, 3437)\t0.03569796163552166\n",
            "  (1, 27752)\t0.05073701728422769\n",
            "  (1, 18175)\t0.025469350152270268\n",
            "  (1, 20540)\t0.02063971517594968\n",
            "  (1, 558)\t0.02148423843956185\n",
            "  (1, 43739)\t0.032789680377946176\n",
            "  (1, 43611)\t0.1486624642797053\n",
            "  (1, 43600)\t0.027534153188991883\n",
            "  (1, 44188)\t0.06021222162344658\n",
            "  (1, 22232)\t0.06313323118444283\n",
            "  (1, 22300)\t0.06336502098728282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RonQzooNynZJ",
        "outputId": "d4621aa2-ffd0-42a6-d850-0baa59a383ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Columns are mostly empty because most words in the vocabulary do not appear in every sentence\n",
        "# This is why sparse format is used instead of dense:\n",
        "print(fm_train[0:2].todense())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfeZp_3C0d3B",
        "outputId": "d21f0178-48ca-4977-831c-59559a06cae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# In the vectorizer vocabulary we have the original words as key value pairs, where the \n",
        "# word is the key and matrix index is the value:\n",
        "\n",
        "for (idx, item) in enumerate(vectorizer.vocabulary_.items()):\n",
        "  print(\"Key:\", item[0], \"\\tValue:\",item[1])\n",
        "  if (idx==8):\n",
        "    break"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Key: without \tValue: 48956\n",
            "Key: saying \tValue: 36990\n",
            "Key: how \tValue: 20045\n",
            "Key: it \tValue: 22300\n",
            "Key: ended \tValue: 13422\n",
            "Key: is \tValue: 22232\n",
            "Key: sufficient \tValue: 41989\n",
            "Key: to \tValue: 44188\n",
            "Key: say \tValue: 36985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rihQsGsqKqOg"
      },
      "source": [
        "## Finding the best model with GridSearch Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "419GUOsFS_Zu"
      },
      "source": [
        "The model can be trained by exploring the hyperparameters one by one or with nested for-loops. It will how ever become a frustrating task to keep up with the hyperparameter combinations and obtained performence values. A more systematic way to do this is by using GridSearch (GS). \n",
        "\n",
        "GridSearchCV allows us set grid (or multiple vectors) of hypermarameters to try with. The idea is to try and find a sweet spot (best performance measure) by adjusting the grid. K-fold CV also introduces a new hyperparameter, which affects the training results, namely the number of folds.\n",
        "\n",
        "GS uses K-fold Cross Validation (CV) to find the best performing model. Depending on the algorithm and chosen parameters the data is \"folded\" (divided into subsets) n times and each of these folds is used once for testing while n-1 folds are used for training. Cross validation is also useful when the data set size is limited and we would like to \"eat the cake and keep it\".\n",
        "\n",
        "For the task I have chosen Linear Support Vector Classifier. When classifying multiple classes and the number of classes in *n* LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training *n* models ([Scikit-learn](https://scikit-learn.org/stable/modules/svm.html#svm-classification)). At prediction time all the classi\ffers \"vote\", and item will be assigned to class with the lowest cost. Other possible models for text classification problem are for example K-Nearest-Neighbors and Multinomial Naive Bayes. Also classifiers can be compared with GS.\n",
        "\n",
        "A simple pipeline is built for both the preprocessor (vectorizer) and the classifier so that we are able to find the best hyperparameters for both of them at once.\n",
        "\n",
        "Sources:\n",
        "\n",
        "[GridSearchCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
        "\n",
        "[GridSearchCV example 1](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html) \n",
        "\n",
        "[GridSearchCV example 2](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html) \n",
        "\n",
        "[SVM documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
        "\n",
        "[GridSearchCV scoring parameters](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95XwkuFwVS8x"
      },
      "source": [
        "#### TfidfVectorizer with SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z5Iu2P5MLSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "4bae0f23-1b0b-4393-f00c-cf73807997e4"
      },
      "source": [
        "costs = np.logspace(-1, 1, num=5, endpoint = False)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vec', TfidfVectorizer()),\n",
        "    ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    #'vec__binary': (True, False), # Previous runs revealed this does not seem to matter\n",
        "    'vec__max_features': (20000, 50000),\n",
        "    'vec__ngram_range': ((1, 1), (1, 2), (1, 3)),  \n",
        "    'clf__C': (costs), \n",
        "}\n",
        "# find the best parameters for both the feature extraction and the classifier\n",
        "print(\"Running grid search...\")\n",
        "# n_jobs=-1: use as many cores as possible\n",
        "# cv=3: three folds (this is kind of little but it speeds things up)\n",
        "gridsearch = GridSearchCV(pipeline, parameters, cv=3, verbose=1, n_jobs=-1)\n",
        "gridsearch.fit(X_train, y_train)\n",
        "print(\"Grid search done!\")\n",
        "print()\n",
        "print(f\"Best score: {gridsearch.best_score_:0.2}\")\n",
        "print(\"Best of the observed hyperparameters:\")\n",
        "best_parameters = gridsearch.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "      print(f\"{param_name}: {best_parameters[param_name]}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running grid search...\n",
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 14.1min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-511c1c6bbac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# cv=3: three folds (this is kind of little but it speeds things up)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mgridsearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mgridsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Grid search done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPNkptft9ixO"
      },
      "source": [
        "So was the selected model clearly the best?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HbVgsom5Wir"
      },
      "source": [
        "# set column visibility in pandas df\n",
        "pd.set_option(\"max_colwidth\", None)\n",
        "\n",
        "# extract mean score for each parameter combination trained\n",
        "means = gridsearch.cv_results_['mean_test_score'] \n",
        "\n",
        "GSCV_results = pd.DataFrame(list(zip(means, gridsearch.cv_results_['params'])), \n",
        "               columns =['Score', 'Parameters']) \n",
        "# sort by the score\n",
        "GSCV_results.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
        "print(GSCV_results.head(7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY62tzzG-zVf"
      },
      "source": [
        "Not! This seemd to be a tight race."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJZ2_WEGBCSR"
      },
      "source": [
        "##### Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq6i4BgvTJ4l"
      },
      "source": [
        "# classifier=grid_search.best_estimator_\n",
        "# classifier.fit(feature_matrix_train, train_label)\n",
        "\n",
        "predictions = gridsearch.predict(X_test)\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "# conf = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "print(f\"Test accuracy: {acc:0.2f}\")\n",
        "print()\n",
        "# note here we have to feed in the test data not feature matrix since esitimator is a pipeline, not a classifier!\n",
        "plot_confusion_matrix(gridsearch.best_estimator_, X_test, y_test, cmap='Greens', values_format='d')  \n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.show()\n",
        "\n",
        "plot_confusion_matrix(gridsearch.best_estimator_, X_test, y_test, cmap='Blues', normalize='true')  \n",
        "plt.title(\"Normalized confusion matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cAuuKjW6yrk"
      },
      "source": [
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuSXvfofI_M1"
      },
      "source": [
        "The model handled both of the classes well. This can be seen from the confusion matrix and from the classification report where precision and recall are in balance for both os the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0eZsazk14xb"
      },
      "source": [
        "#### Simple Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1nyeGt9wES3"
      },
      "source": [
        "# redo the split to overwrite old variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.33)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEznt0u2KU3"
      },
      "source": [
        "To build a NN we need to\n",
        "\n",
        "1.   turn numpy vectors to tensors\n",
        "2.   know the shape of input layer (number of features)\n",
        "3.   know the shape of output layer (number of classes)\n",
        "\n",
        "TfidfVectorizer gives the 2nd one and LabelEncoder (for example) the 3rd one. (or just len(set(train_labels))\n",
        "\n",
        "\"Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class.\"(https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6vARoH6wv3L"
      },
      "source": [
        "# 1) np vectors to TF tensors\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "    coo = X.tocoo()\n",
        "    indices = np.mat([coo.row, coo.col]).transpose()\n",
        "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utEFKUq0ModO"
      },
      "source": [
        "Since vectorizer affects the shape of the NN, we do not optimize it as a part of the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YooLapRH3ABl"
      },
      "source": [
        "# 2) size of input \n",
        "vectorizer = TfidfVectorizer(max_features=100000)\n",
        "\n",
        "ft_matrix = vectorizer.fit_transform(X_train)\n",
        "ft_matrix.shape # so we need the second dimension for building the nn\n",
        "input_size = ft_matrix.shape[1]\n",
        "input_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HevFwocVx2l6"
      },
      "source": [
        "# 3) size_of_output_layer\n",
        "# use encoded labels when fitting the model and for testing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder() #Turns class labels into integers\n",
        "class_numbers_train = label_encoder.fit_transform(y_train)\n",
        "\n",
        "print(\"class_numbers shape=\", class_numbers_train.shape)\n",
        "print(\"class labels\", label_encoder.classes_) #this will let us translate back from indices to labels\n",
        "\n",
        "output_size = len(label_encoder.classes_)\n",
        "print(\"Shape of output layer:\", output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1GYj-1KZyJY"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "def build_sequential_nn(input_size=100, output_size=2, hiddenlayer_size=200, drop_out= 0.3, learning_rate=0.001): \n",
        "  # let's make 200 default sixe of the hiddenlayer\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape = (input_size, )))\n",
        "  model.add(Dense(hiddenlayer_size, activation = \"tanh\", ))\n",
        "  model.add(Dropout(rate=drop_out)) # Dropout regularizer to avoid over fitting\n",
        "  # model.add(Dense(output_size, activation = \"softmax\"))\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = build_sequential_nn(input_size=input_size, output_size=output_size)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbvQNJQmwpma"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "# from keras.callbacks import EarlyStopping\n",
        "import time\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('trans', FunctionTransformer(convert_sparse_matrix_to_sparse_tensor)), # wrapper for custom function\n",
        "    ('clf', KerasClassifier(build_fn=build_sequential_nn)), # wrapper for Keras model\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'clf__hiddenlayer_size': (200, 300), \n",
        "    'clf__input_size': ([input_size]), # GS sets ALL the params\n",
        "    'clf__output_size': ([output_size]),\n",
        "    'clf__batch_size': (64, 265),\n",
        "    'clf__drop_out': (0.2, 0.4),\n",
        "    'clf__epochs': (3, 5), # do not use early stopping callback, number of epochs is best treated as a hyper parameter: https://stackoverflow.com/questions/48127550/early-stopping-with-keras-and-sklearn-gridsearchcv-cross-validation\n",
        "    'clf__learning_rate': (0.001, 0.01) # 0.001 is default for Adam\n",
        "}\n",
        "\n",
        "t0=time.time()\n",
        "print(\"Running grid search...\")\n",
        "gridsearch = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=1) # n_jobs=-1: use as many cores as possible OR use GPU\n",
        "gridsearch.fit(ft_matrix, class_numbers_train)\n",
        "print()\n",
        "\n",
        "print(f\"Best score: {gridsearch.best_score_:0.2}\")\n",
        "\n",
        "print(\"Best of the observed hyperparameters:\")\n",
        "best_parameters = gridsearch.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "      print(f\"{param_name}: {best_parameters[param_name]}\")\n",
        "\n",
        "t1=time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VFeq0r0qxN"
      },
      "source": [
        "print(f\"Time elapsed {(t1-t0)/60:0.3} minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzuE9_f_5HjR"
      },
      "source": [
        "means = gridsearch.cv_results_['mean_test_score'] \n",
        "\n",
        "GSCV_results = pd.DataFrame(list(zip(means, gridsearch.cv_results_['params'])), \n",
        "               columns =['Score', 'Parameters']) \n",
        "# sort by the score\n",
        "GSCV_results.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
        "print(GSCV_results.head(7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-SCnMQ_GXtC"
      },
      "source": [
        "##### Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nh6sXJQS4o_"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# prepare test data\n",
        "ftm_test=vectorizer.transform(X_test) # model needs to be Sequential for predicting\n",
        "class_numbers_test = label_encoder.transform(y_test)\n",
        "\n",
        "# predict\n",
        "raw_predictions = gridsearch.predict(ftm_test)\n",
        "predictions=label_encoder.inverse_transform(raw_predictions)\n",
        "\n",
        "# results\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "print(f\"Test accuracy: {acc:0.2f}\")\n",
        "print()\n",
        "cf_mat = tf.math.confusion_matrix(\n",
        "    class_numbers_test, raw_predictions, num_classes=None, weights=None\n",
        ")\n",
        "\n",
        "def plot_cf_matrix(mat):\n",
        "  sns.heatmap(mat, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Greens')\n",
        "  plt.title(\"Confusion matrix for test data\", fontsize = 16)\n",
        "  plt.ylabel(\"True class\", fontsize = 14)\n",
        "  plt.xlabel(\"Predicted class\", fontsize = 14)\n",
        "\n",
        "plot_cf_matrix(cf_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_kDEsq0t25F"
      },
      "source": [
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
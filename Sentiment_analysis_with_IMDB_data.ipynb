{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_with_IMDB_data.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/Sentiment-analysis-with-IMDB-data/blob/main/Sentiment_analysis_with_IMDB_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAjU5sY1Mj0O"
      },
      "source": [
        "The purpose of this notebook is to experiment with hyperparameter search using Grid Search and sklearn Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oSt2eIvPfYW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import seaborn as sns\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras import optimizers\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeF_rZZMRRUX"
      },
      "source": [
        "## Load and inspect the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9mbjj9vv0Wv"
      },
      "source": [
        "The data is IMBD reviews made for Stanford University research project (https://www.aclweb.org/anthology/P11-1015/). To learn more about the data please visit the web page or read the README file printed below.\n",
        "\n",
        "For my purposes the test data is bi enough and I will use it for training, valdating and testing the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYMQPYZ5jUM0"
      },
      "source": [
        "%%bash\n",
        "wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1QE3NTLmAHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366fe757-354f-4715-deaf-473c7bfeec82"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb\n",
        "cat README | head -1000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Large Movie Review Dataset v1.0\n",
            "\n",
            "Overview\n",
            "\n",
            "This dataset contains movie reviews along with their associated binary\n",
            "sentiment polarity labels. It is intended to serve as a benchmark for\n",
            "sentiment classification. This document outlines how the dataset was\n",
            "gathered, and how to use the files provided. \n",
            "\n",
            "Dataset \n",
            "\n",
            "The core dataset contains 50,000 reviews split evenly into 25k train\n",
            "and 25k test sets. The overall distribution of labels is balanced (25k\n",
            "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
            "documents for unsupervised learning. \n",
            "\n",
            "In the entire collection, no more than 30 reviews are allowed for any\n",
            "given movie because reviews for the same movie tend to have correlated\n",
            "ratings. Further, the train and test sets contain a disjoint set of\n",
            "movies, so no significant performance is obtained by memorizing\n",
            "movie-unique terms and their associated with observed labels.  In the\n",
            "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
            "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
            "more neutral ratings are not included in the train/test sets. In the\n",
            "unsupervised set, reviews of any rating are included and there are an\n",
            "even number of reviews > 5 and <= 5.\n",
            "\n",
            "Files\n",
            "\n",
            "There are two top-level directories [train/, test/] corresponding to\n",
            "the training and test sets. Each contains [pos/, neg/] directories for\n",
            "the reviews with binary labels positive and negative. Within these\n",
            "directories, reviews are stored in text files named following the\n",
            "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
            "the star rating for that review on a 1-10 scale. For example, the file\n",
            "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
            "example with unique id 200 and star rating 8/10 from IMDb. The\n",
            "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
            "omitted for this portion of the dataset.\n",
            "\n",
            "We also include the IMDb URLs for each review in a separate\n",
            "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
            "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
            "are unable to link directly to the review, but only to the movie's\n",
            "review page.\n",
            "\n",
            "In addition to the review text files, we include already-tokenized bag\n",
            "of words (BoW) features that were used in our experiments. These \n",
            "are stored in .feat files in the train/test directories. Each .feat\n",
            "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
            "data.  The feature indices in these files start from 0, and the text\n",
            "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
            "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
            "(the) appears 7 times in that review.\n",
            "\n",
            "LIBSVM page for details on .feat file format:\n",
            "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
            "\n",
            "We also include [imdbEr.txt] which contains the expected rating for\n",
            "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
            "rating is a good way to get a sense for the average polarity of a word\n",
            "in the dataset.\n",
            "\n",
            "Citing the dataset\n",
            "\n",
            "When using this dataset please cite our ACL 2011 paper which\n",
            "introduces it. This paper also contains classification results which\n",
            "you may want to compare against.\n",
            "\n",
            "\n",
            "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "  month     = {June},\n",
            "  year      = {2011},\n",
            "  address   = {Portland, Oregon, USA},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "  pages     = {142--150},\n",
            "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "}\n",
            "\n",
            "References\n",
            "\n",
            "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
            "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
            "636-659.\n",
            "\n",
            "Contact\n",
            "\n",
            "For questions/comments/corrections please contact Andrew Maas\n",
            "amaas@cs.stanford.edu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSPALgVntPKT"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb/test/neg\n",
        "for f in *txt; do echo $f>> /content/neg_file_names.txt; done # appends filenames to file"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM3iXWLB6qwj"
      },
      "source": [
        "%%bash\n",
        "cd aclImdb/test/pos\n",
        "for f in *txt; do echo $f>> /content/pos_file_names.txt; done # appends filenames to file"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1wqR79f0woZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db7dfa9-ff26-404e-9a60-a1fa272cbdc6"
      },
      "source": [
        "def open_file_by_looping(path, file):\n",
        "  f=open(file)\n",
        "  filenames = f.readlines()\n",
        "  reviews = []\n",
        "  for filename in filenames:\n",
        "    fname = path + filename\n",
        "    fname =fname.rstrip()\n",
        "    f = open(fname)\n",
        "    review = f.readlines()\n",
        "    # print(type(review))\n",
        "    reviews.append(review) \n",
        "    f.close()\n",
        "  f.close()\n",
        "  flat_list = [item for sublist in reviews for item in sublist] # we have list of lists, it needs to be flattened\n",
        "  return flat_list\n",
        "\n",
        "neg_reviews = open_file_by_looping(\"aclImdb/test/neg/\", \"neg_file_names.txt\")\n",
        "print(\"Number of negative reviews:\", len(neg_reviews))\n",
        "\n",
        "pos_reviews = open_file_by_looping(\"aclImdb/test/pos/\", \"pos_file_names.txt\")\n",
        "print(\"Number of positive reviews:\", len(pos_reviews))\n",
        "\n",
        "reviews=neg_reviews+pos_reviews\n",
        "len(reviews)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of negative reviews: 12500\n",
            "Number of positive reviews: 12500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkqVNVyfVZ9L"
      },
      "source": [
        "Data quality affects the performance of all machine learning algorithms and neural networks. Poor data can not be improved even with a sophisticated algorithm. In this case our data is balanced (classes have equal 50 % share) and well behaving in many aspects.\n",
        "\n",
        "This is seldom true in real life applications. In these cases data balance must be taken care of with for example stratification or giving different weigths to different classes. If data is grouped or datarecords are not independent, even more caution should be given to the training process since this might lead to test data \"leaking\" into training data and thus highly optimistic model performance measeures. \n",
        "\n",
        "For these reasons one should familiarize her with a new dataset before rushing into further steps of modeling. If it is discovered that the data is imbalanced, grouped etc. we can fix the issues uprising from the nature of the data before we feed it to the algorithm, or at least take it in account when analysing the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSktZXAh7QMt",
        "outputId": "922810e4-1dbf-4513-d90b-1d11ab54b28e"
      },
      "source": [
        "reviews=neg_reviews+pos_reviews\n",
        "len(reviews)\n",
        "\n",
        "# make labels for the reviews:\n",
        "labels = ['neg']*len(neg_reviews) + ['pos']*len(pos_reviews)\n",
        "print(len(labels))\n",
        "\n",
        "# make shuffled indices and shuffle both of labels and reviews with them\n",
        "indices = list(range(len(labels)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "labels = [labels[index] for index in indices]\n",
        "reviews = [reviews[index] for index in indices]\n",
        "\n",
        "for label, text in zip(labels[:10], reviews[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "label: neg \n",
            "text: Has anyone else noticed that this version is basically a scene-by-scene remake of the 1933 version, with some of the scenes taken out? It makes me think less of a film that does that, showing a definite want of creativity. In all fairness, I tend to be biased in favor of Katharine Hepburn, but this version of the film seems like cinematic plagiarism. The 1933 version was nice and sweet, though a little awkward in presentation and transition at times, and then this version took the script, the music, and even a fair amount of the scene blocking from the earlier version. I don't understand the point of making the film again when the method of remaking it was to basically redo George Cukor's film with everything the same except the people working on it. \n",
            "\n",
            "label: pos \n",
            "text: A very interesting plot of the film based on the novel \"Waltz into Darkness\" of the writer Cornell Woolrich. It is a drama rather than a film noir, which tries to send a message that love changes your own life, i.e. your love to any person and the love you received from him/her. A wealthy man really changed his life for love, while his partner finally understood that he was the only one that loved her. Belmondo played well as usual, while a somewhat still young Michel Bouquet played his eternal role of a detective or police agent. Frankly Bouquet was not so impressive in this film, but less than that was the performance of Catherine Deneuve. She was not so convincingly in her role as a prostitute then lover/wife of Louis Mahé (Belmondo). For those who like to visit the world, the film offers the occasion to see part of the Ascension Island, and also Lyon city in France. \n",
            "\n",
            "label: pos \n",
            "text: Just ingenious enough to be plausible and still a lot of fun, this is a pure slice of the 1970s (Even the cops need haircuts badly!). Shot in and around London, the plot of the American ex-con who tries going straight but finds himself sent as an electrician to a bank in Mayfair, and then has the screws put on by crime lord David Niven, and finds himself plotting the crime of the century is well-handled.<br /><br />I liked its simplicity and even innocence, it harks back to a time when caper films where just that, a caper, and violence wasn't a part of the deal.<br /><br />All in all you could do a lot worse than watch this: it has enough twists and turns to give it some oomph and a cast that obviously had fun making it.<br /><br />Nicely made and watchable. \n",
            "\n",
            "label: pos \n",
            "text: The Western can be divided into many sub-genres. One of the broadest divisions is that between Town Westerns and Plains Westerns. Most Westerns are a mix of both, but at one end of the spectrum you have pictures like High Noon and Rio Bravo that take place almost entirely in a settlement, seldom venturing out into the real outdoors. At the other end you have ones like Wagon Master, where there is barely a homestead on view amid the wilderness.<br /><br />Director John Ford normally thrived on the \"bit of both\" Westerns, shooting the interiors with an emphasis on their being small and confined, and then contrasting this with the wide open exteriors, which appeared both exciting and dangerous. Wagon Master has a typical Frank Nugent script, with some interplay between seasoned oldsters and green youngsters, but still it presents Ford with some fresh challenges. In this picture, the dangers do not come from the harshness of the landscape, they come from within the group in the form of the Cleggses. What's more, the absence of real interior scenes means the outdoors could lose its impact over time.<br /><br />However, Ford was a real maestro when it came to manipulating space. He shoots scenes of the camp or the wagons so the frame is surrounded and we get that same sense of enclosure as we would in a genuine interior. Also, compared to his other Westerns, he does not in fact open out the space too much, having the wagon trail wend its way through canyons and passes rather than cross the stark and empty plains. One of the few moments where he does throw the landscape wide open is when the Indians are spotted and there is the possibility of a threat from outside.<br /><br />Wagon Master features some surprisingly effective moments of comic relief, and some great contributions from the quirky cast. Harry Carey Jr. was shaping up into a fine actor like his pa, and this is one of his better early roles. Joanne Dru was disappointing in She Wore a Yellow Ribbon, but she appears more at ease as a character with a bit of sass, and is actually fairly good here. Jane Darwell, who won an Oscar in the John Ford-directed Grapes of Wrath a decade earlier, appears here with sole function of performing a running gag in which she sounds a feeble old horn. Still, with her great timing and movement she makes the piece work. Francis Ford, in one of the many mute drunkard roles he played in his little brother's pictures, is at his cheeky best.<br /><br />And now we come to lead man Ben Johnson. Although he was by no means a bad actor, he was never going to become a big star like John Wayne. And yet, with his effortless horsemanship and easygoing drawl, he was one of the most authentically \"West\" players around. And this brings me onto my final point. This was apparently one of Ford's personal favourites, despite it seeming fairly unassuming. Wagon Master has no grand theme or dramatic intensity, it is simply the genre playing itself out. I think this is what Ford loved about it. It's a picture for the Ben Johnsons and the Harry Carey Jrs, not the John Waynes or the Henry Fondas. Small in scope, but worthy in its class. \n",
            "\n",
            "label: neg \n",
            "text: I'm not going to say the story of the movie as some people do. I'm pretty sure people who read this will know what the storyline is. I'm also not going to go on and on about everything thats wrong with this movie, because I'll be here for ages if I do. The storyline is typical, and the special effects are below today's standards. This is not a movie you should watch if you are a serious movie buff (as most of us here are) little things will annoy you the whole movie and ruin the experience. If your a casual movie watcher, who likes to have a good time when they are watching a flick, then this movie is perfect for you, lots of fun. It would also be a good movie to take a partner to. Just not for us movie buff's.<br /><br /> 5 out of 10 \n",
            "\n",
            "label: neg \n",
            "text: Brainless film about two girls and some guys they meet in an airport getting on the wrong late night shuttle bus and ending up in a whole world of trouble. Great twists and turns are totally, and I do mean totally wasted, in a film with a plot so incredibly stupid as to defy description. What is going on in a general sense is okay, I mean the idea of a guy kidnapping unattended girls for nefarious purposes is a good one. The problem is that the details are so beyond belief that I would be shocked if you don't turn off the film in utter disbelief. Gee, a guy who is suppose to be taking you home doesn't go any of the ways you know, and you stay on the bus? It get worse from there, think of every bad choice and this film has the characters make it, even to the point where they could just walk away, but never do. Whats annoying is that some of the twists and turns might have worked if there was something intelligent before it, but there is almost no intelligence anywhere in this film. Okay, maybe there is, the end, the end is clever. The end is the sort of thing that should freak you out. it should be the \"oh #$*@!!!!\" moment and become a classic of horror cinema. Instead it just lies there among the stupid ruins of a stupid movie. One of the most brainless films of the year. \n",
            "\n",
            "label: neg \n",
            "text: This starts off in Pennsylvania in 1913. A bunch of kids are killed in a mine explosion purposely set off by the mine's owner. Cut to 2006. Recently widowed Karen (Lori Heuring), her teenage daughter Sarah (Scout Taylor-Compton) and little girl Emma (Chloe Moretz) move to a remote house located near that mine. What they don't know is the ghosts of the little kids haunt the woods and kill anyone who's around after dark.<br /><br />Slow and boring \"horror\" movie. The premise is obvious and has been done to death already. Also there are huge gaps in logic in the story. It's never made clear why these kids just kill anybody or why they EAT the bodies afterwards (Yes--it's shown). They're dead already--why do they need food? And why haven't they gotten the main villain in the story long before? He was around the area. Why pick this time to attack him? Also the characters aren't the least bit likable. Sarah comes across the worst. It has a few saving graces. The location is beautiful and eerie at the same time, some of the killings were VERY bloody and brutal and the kids themselves looked spooky silently walking through the woods at night. But, all in all, I was bored and fighting to stay awake. You can skip this one. \n",
            "\n",
            "label: neg \n",
            "text: Pretty terrible, but not entirely unwatchable. Another review mentioned \"predictable\" - and that's almost an understatement. You can make a game out of guessing what the next line will be. Every character is either stereotype or archetypical. The good guy in a bad situation, the struggle between older and younger priest on acceptance and discipline, the repressed, sexually/emotionally deprived woman returning to the small town after failing in the big city, engaged to the hotheaded, feeble minded beau from youth, the unredeemable bad guys, two \"lost boys\" looking for a sense of family - they're all here, and none of them with even the remotest spin of something new. From the first few minutes you can figure out exactly what will happen by film's end. The story isn't entirely lame, but direction, acting (even from a cast with some talent) everything is thrown together without skill. As to the storyline, we've all seen it before in a movie called \"Sister Act.\" This is also one of those films where inattention to small details show up in an even more glaring light. (As example: the nurse and our hero drive into town but park several blocks away from their destinations (post office and hardware store) - yet both walk across empty parking lots for no apparent reason. Or the passage of morning to night during a scene that seemingly should occur in no more than half an hour. The movie is filled with that kind of stuff and then tags on an improbable denouement. \n",
            "\n",
            "label: neg \n",
            "text: This film infuriated me for the simple fact that it was made only because Shepherd was gay. The men who murdered him are clearly wicked. What happened to the poor man was truly horrible and a tragedy. However, where was Hollywood when four religious white kids were executed, after being forced to perform a host of sex acts on their killers and each other, by two evil black men in Wichita just two years ago? The celebrities only mug for the camera when it serves a political purpose. Also, Laramie is portrayed in a poor light by this pseudo-documentary, which of course is hardly surprising because they are the backward hicks who must be educated by omniscient and enlightened Californians. Still, it's always a treat to see Laura Linney. \n",
            "\n",
            "label: pos \n",
            "text: Blind Spot's images are great. The action draws you in completely, even though the movie is a bit long. By the end credits all that you can think about are the film's positive high-points. The lead actors have the most incredible screen presence. The story is heart-wrenching. The film score is nicely understated . Completely moving in its own powerful way. Not your standard melodramatic cuing. Trance-like moments add poetic resonance to the engrossing narration and terrific visual compositions. Hope you get a chance to see this film. It delves into some dark territory but you come out of the tunnel seeing nothing but white light. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XCLyD7zNxoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864e3cd5-d614-4db8-cb9b-29875c283d7c"
      },
      "source": [
        "# Remove HTML tags\n",
        "\n",
        "\n",
        "pattern1=r\"<br /><br />\" \n",
        "reviews = [re.sub(pattern1, \" \", item) for item in reviews]\n",
        "\n",
        "for label, text in zip(labels[:10], reviews[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: neg \n",
            "text: Has anyone else noticed that this version is basically a scene-by-scene remake of the 1933 version, with some of the scenes taken out? It makes me think less of a film that does that, showing a definite want of creativity. In all fairness, I tend to be biased in favor of Katharine Hepburn, but this version of the film seems like cinematic plagiarism. The 1933 version was nice and sweet, though a little awkward in presentation and transition at times, and then this version took the script, the music, and even a fair amount of the scene blocking from the earlier version. I don't understand the point of making the film again when the method of remaking it was to basically redo George Cukor's film with everything the same except the people working on it. \n",
            "\n",
            "label: pos \n",
            "text: A very interesting plot of the film based on the novel \"Waltz into Darkness\" of the writer Cornell Woolrich. It is a drama rather than a film noir, which tries to send a message that love changes your own life, i.e. your love to any person and the love you received from him/her. A wealthy man really changed his life for love, while his partner finally understood that he was the only one that loved her. Belmondo played well as usual, while a somewhat still young Michel Bouquet played his eternal role of a detective or police agent. Frankly Bouquet was not so impressive in this film, but less than that was the performance of Catherine Deneuve. She was not so convincingly in her role as a prostitute then lover/wife of Louis Mahé (Belmondo). For those who like to visit the world, the film offers the occasion to see part of the Ascension Island, and also Lyon city in France. \n",
            "\n",
            "label: pos \n",
            "text: Just ingenious enough to be plausible and still a lot of fun, this is a pure slice of the 1970s (Even the cops need haircuts badly!). Shot in and around London, the plot of the American ex-con who tries going straight but finds himself sent as an electrician to a bank in Mayfair, and then has the screws put on by crime lord David Niven, and finds himself plotting the crime of the century is well-handled. I liked its simplicity and even innocence, it harks back to a time when caper films where just that, a caper, and violence wasn't a part of the deal. All in all you could do a lot worse than watch this: it has enough twists and turns to give it some oomph and a cast that obviously had fun making it. Nicely made and watchable. \n",
            "\n",
            "label: pos \n",
            "text: The Western can be divided into many sub-genres. One of the broadest divisions is that between Town Westerns and Plains Westerns. Most Westerns are a mix of both, but at one end of the spectrum you have pictures like High Noon and Rio Bravo that take place almost entirely in a settlement, seldom venturing out into the real outdoors. At the other end you have ones like Wagon Master, where there is barely a homestead on view amid the wilderness. Director John Ford normally thrived on the \"bit of both\" Westerns, shooting the interiors with an emphasis on their being small and confined, and then contrasting this with the wide open exteriors, which appeared both exciting and dangerous. Wagon Master has a typical Frank Nugent script, with some interplay between seasoned oldsters and green youngsters, but still it presents Ford with some fresh challenges. In this picture, the dangers do not come from the harshness of the landscape, they come from within the group in the form of the Cleggses. What's more, the absence of real interior scenes means the outdoors could lose its impact over time. However, Ford was a real maestro when it came to manipulating space. He shoots scenes of the camp or the wagons so the frame is surrounded and we get that same sense of enclosure as we would in a genuine interior. Also, compared to his other Westerns, he does not in fact open out the space too much, having the wagon trail wend its way through canyons and passes rather than cross the stark and empty plains. One of the few moments where he does throw the landscape wide open is when the Indians are spotted and there is the possibility of a threat from outside. Wagon Master features some surprisingly effective moments of comic relief, and some great contributions from the quirky cast. Harry Carey Jr. was shaping up into a fine actor like his pa, and this is one of his better early roles. Joanne Dru was disappointing in She Wore a Yellow Ribbon, but she appears more at ease as a character with a bit of sass, and is actually fairly good here. Jane Darwell, who won an Oscar in the John Ford-directed Grapes of Wrath a decade earlier, appears here with sole function of performing a running gag in which she sounds a feeble old horn. Still, with her great timing and movement she makes the piece work. Francis Ford, in one of the many mute drunkard roles he played in his little brother's pictures, is at his cheeky best. And now we come to lead man Ben Johnson. Although he was by no means a bad actor, he was never going to become a big star like John Wayne. And yet, with his effortless horsemanship and easygoing drawl, he was one of the most authentically \"West\" players around. And this brings me onto my final point. This was apparently one of Ford's personal favourites, despite it seeming fairly unassuming. Wagon Master has no grand theme or dramatic intensity, it is simply the genre playing itself out. I think this is what Ford loved about it. It's a picture for the Ben Johnsons and the Harry Carey Jrs, not the John Waynes or the Henry Fondas. Small in scope, but worthy in its class. \n",
            "\n",
            "label: neg \n",
            "text: I'm not going to say the story of the movie as some people do. I'm pretty sure people who read this will know what the storyline is. I'm also not going to go on and on about everything thats wrong with this movie, because I'll be here for ages if I do. The storyline is typical, and the special effects are below today's standards. This is not a movie you should watch if you are a serious movie buff (as most of us here are) little things will annoy you the whole movie and ruin the experience. If your a casual movie watcher, who likes to have a good time when they are watching a flick, then this movie is perfect for you, lots of fun. It would also be a good movie to take a partner to. Just not for us movie buff's.  5 out of 10 \n",
            "\n",
            "label: neg \n",
            "text: Brainless film about two girls and some guys they meet in an airport getting on the wrong late night shuttle bus and ending up in a whole world of trouble. Great twists and turns are totally, and I do mean totally wasted, in a film with a plot so incredibly stupid as to defy description. What is going on in a general sense is okay, I mean the idea of a guy kidnapping unattended girls for nefarious purposes is a good one. The problem is that the details are so beyond belief that I would be shocked if you don't turn off the film in utter disbelief. Gee, a guy who is suppose to be taking you home doesn't go any of the ways you know, and you stay on the bus? It get worse from there, think of every bad choice and this film has the characters make it, even to the point where they could just walk away, but never do. Whats annoying is that some of the twists and turns might have worked if there was something intelligent before it, but there is almost no intelligence anywhere in this film. Okay, maybe there is, the end, the end is clever. The end is the sort of thing that should freak you out. it should be the \"oh #$*@!!!!\" moment and become a classic of horror cinema. Instead it just lies there among the stupid ruins of a stupid movie. One of the most brainless films of the year. \n",
            "\n",
            "label: neg \n",
            "text: This starts off in Pennsylvania in 1913. A bunch of kids are killed in a mine explosion purposely set off by the mine's owner. Cut to 2006. Recently widowed Karen (Lori Heuring), her teenage daughter Sarah (Scout Taylor-Compton) and little girl Emma (Chloe Moretz) move to a remote house located near that mine. What they don't know is the ghosts of the little kids haunt the woods and kill anyone who's around after dark. Slow and boring \"horror\" movie. The premise is obvious and has been done to death already. Also there are huge gaps in logic in the story. It's never made clear why these kids just kill anybody or why they EAT the bodies afterwards (Yes--it's shown). They're dead already--why do they need food? And why haven't they gotten the main villain in the story long before? He was around the area. Why pick this time to attack him? Also the characters aren't the least bit likable. Sarah comes across the worst. It has a few saving graces. The location is beautiful and eerie at the same time, some of the killings were VERY bloody and brutal and the kids themselves looked spooky silently walking through the woods at night. But, all in all, I was bored and fighting to stay awake. You can skip this one. \n",
            "\n",
            "label: neg \n",
            "text: Pretty terrible, but not entirely unwatchable. Another review mentioned \"predictable\" - and that's almost an understatement. You can make a game out of guessing what the next line will be. Every character is either stereotype or archetypical. The good guy in a bad situation, the struggle between older and younger priest on acceptance and discipline, the repressed, sexually/emotionally deprived woman returning to the small town after failing in the big city, engaged to the hotheaded, feeble minded beau from youth, the unredeemable bad guys, two \"lost boys\" looking for a sense of family - they're all here, and none of them with even the remotest spin of something new. From the first few minutes you can figure out exactly what will happen by film's end. The story isn't entirely lame, but direction, acting (even from a cast with some talent) everything is thrown together without skill. As to the storyline, we've all seen it before in a movie called \"Sister Act.\" This is also one of those films where inattention to small details show up in an even more glaring light. (As example: the nurse and our hero drive into town but park several blocks away from their destinations (post office and hardware store) - yet both walk across empty parking lots for no apparent reason. Or the passage of morning to night during a scene that seemingly should occur in no more than half an hour. The movie is filled with that kind of stuff and then tags on an improbable denouement. \n",
            "\n",
            "label: neg \n",
            "text: This film infuriated me for the simple fact that it was made only because Shepherd was gay. The men who murdered him are clearly wicked. What happened to the poor man was truly horrible and a tragedy. However, where was Hollywood when four religious white kids were executed, after being forced to perform a host of sex acts on their killers and each other, by two evil black men in Wichita just two years ago? The celebrities only mug for the camera when it serves a political purpose. Also, Laramie is portrayed in a poor light by this pseudo-documentary, which of course is hardly surprising because they are the backward hicks who must be educated by omniscient and enlightened Californians. Still, it's always a treat to see Laura Linney. \n",
            "\n",
            "label: pos \n",
            "text: Blind Spot's images are great. The action draws you in completely, even though the movie is a bit long. By the end credits all that you can think about are the film's positive high-points. The lead actors have the most incredible screen presence. The story is heart-wrenching. The film score is nicely understated . Completely moving in its own powerful way. Not your standard melodramatic cuing. Trance-like moments add poetic resonance to the engrossing narration and terrific visual compositions. Hope you get a chance to see this film. It delves into some dark territory but you come out of the tunnel seeing nothing but white light. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-bnE6jxnusp"
      },
      "source": [
        "# s = r\"\\s\\tWord\"\n",
        "# prog = re.compile(s)\n",
        "# prog\n",
        "\n",
        "# re.sub(some_regex, some_replacement.replace('\\\\', '\\\\\\\\'), input_string)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B9giOL-MsY9"
      },
      "source": [
        "# import re\n",
        "# pattern1=r\"<br /><br />\" # something fishy is going on with HTML tags. Get rid of them\n",
        "# s = \"\\\\'\"\n",
        "# print(s)\n",
        "# # pattern2=re.escape(r\"\\'\")\n",
        "# # print(\"p2:\", pattern2)\n",
        "# pattern3 = s\n",
        "# print(pattern3)\n",
        "# # print(pattern3.replace('\\\\', '\\\\\\\\'))\n",
        "# print(chr(39))\n",
        "\n",
        "# fixed = re.sub(pattern3, chr(39), reviews[8])\n",
        "# fixed_n = re.sub(pattern1, \" \", fixed)\n",
        "# fixed_n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox8EQgz3IEM1"
      },
      "source": [
        "## Train-Dev-Test split the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbPsc3SWIyKd",
        "outputId": "ec3835ed-7fd6-4b53-96a1-0664d37e8971"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.33)\n",
        "\n",
        "print(len(X_train), len(y_train))\n",
        "\n",
        "for label, text in zip(y_train[:10], X_train[:10]):\n",
        "  print(\"label:\", label, \"\\ntext:\", text, \"\\n\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16750 16750\n",
            "label: pos \n",
            "text: I felt cheated out of knowing the whole story. While there could be a twist, this twist was so significant, that I felt betrayed. I believe it could have used a better writer who could weave all the elements of the story together better. The writer could have revealed more of the 'twists' throughout the movie, rather than all at once at the end. That aside, I believe that the actors did very well with what they had, particularly Matt Damon, who actually had a little character in his character, little quirks that weren't egotistic or like a smooth criminal who always knows what he is doing. The other main characters were their own separate entities who just happened to converse with one another. The cohesiveness of the group in Ocean's Eleven was gone. \n",
            "\n",
            "label: neg \n",
            "text: Now don't get me wrong, I love seeing half naked chicks wiggling around. It's part of the fun of a Moroccan restaurant: ogling the belly dancers. But it doesn't make much of a plot. My first major problem is the music. I have the feeling that when Ann Rice wrote \"The Vampire Lestat\", the Cure was more the style of the music he would have liked (though I could be wrong). I know relating to current \"goth\" music might have seemed like a good idea, but they did a horrific job incorporating it. Lestat was an actor with presumably a pretty good singing voice. That they chose Jonathan Davis to be his stage voice is heartbreaking. Second, and someone else said it, mashing two very intricate books into one crappy movie is a bad idea. \"Lestat\" could have been a movie in it's own right, and a damn good one if done right. I honestly don't think \"Queen of the Damned\" lends itself to a movie very well. Though I would love to see a movie that incorporates a creation story, there's too much, how to word this, \"inaction\" in the book for it to be a very interesting movie. And the retelling they did soiled it pretty badly. Now mind you, it's been a long time since I've read it, I always thought \"Lestat\", \"Tale of the Body Thief\" and \"Memnoch the Devil\" were much more action packed and would have made better movies. I know a lot of people (hey, myself included) who like a lot of cheesy vampire crap that thought this was absolutely the worst of the genre to be a major motion picture. I tend to agree with them there. Aaliyah had a nice body though. \n",
            "\n",
            "label: pos \n",
            "text:  Headlines warn us of the current campaign to demonize drug users, note the nostalgia for Mussolini in Italy and remind us of our wont to profile likely terrorists. \"Focus\" reminds us of the evils of rampant fear and distrust and the anti-Semitism disguised as pro-Americanism of the WWII era. What goes around... Lawrence Newman (William C. Macy) becomes the unfortunate victim of hate crimes after he is mistaken for a Jew and these attacks increase when his bride, Gerty (Laura Dern) highlights the look. Newman rails against the false accusations when they cause him direct harm but he minds his own business. We see the rise in anti-Semitic attacks through his myopia, a condition not completely cured by eyeglasses. Macy's typical everyman role is featured again and the long road to realization that we are all connected nearly costs him his life.  What is it about the character Macy portrays to us? We can't choose to ignore the violence, hate, and bigotry because citizenship forces us to take sides. Newman's dilemma is that he has no alternative but to side with the mistreated and goes through hell to see it. And our sadness is that most of us need to be beaten up to realize the dangers surrounding us. Let's focus. \n",
            "\n",
            "label: pos \n",
            "text: Well it is a good movie. However, you have to admit that Van Dame's movies haven't been so great lately. On the other hand if you are a hardcore van Dame fan you should watch this as there is a huge improvement with fighting scenes. Also What i really liked is the music beautiful music. am sure this will kill time if you are action fan. As for Isaac Florentine the director of Undisputed 2, i don't think he did a good job as he did with Undisputed.I cant wait when one day Van Dame's movies hit the cinemas before the DVD. Maybe, one day:). In the end it worth the watch if you got no other action movies to watch, this could make your hour and half fun. \n",
            "\n",
            "label: neg \n",
            "text: I'm not picky with movies, oh I've seen so much crap I could watch anything. Maybe that was the reason I watched this one to the end. Im big fan of RPG games too, but this movie, its a disgrace to any self-respecting RPGer there is. The security-camera footage of a game-play would make it feel more realistic than this movie does. The lines, the cuts, the audio, everything is wrong. In some scenes you can see that it was filmed in some photo when !!!!!(spoilers ahead)!!!!!people running around does not disturb people sitting near computers. I mean would you continue your work if you got ninjas around you? oh and the jokes about pirates, that's the worst one yet in movies!!!!!(spoilers end)!!!!! At least first one felt like a documentary, now it looks like someones home video experiment. You can find better movies at youtube. Top line: Don't waste your time and money on this one, its as bad as it comes. \n",
            "\n",
            "label: pos \n",
            "text: When I saw this film, it reminded me of all the greatest dreams i had (mostly filled with robots) I can relate to Eledore's problems and I have a similarity to Shiro, and this is a great film to watch (if you're a Goth who is bitter and eccentric like me!). All in all, watch it before it's out of print! \n",
            "\n",
            "label: neg \n",
            "text: By the time this movie came out in 1996, director Mark Lester had been making tight, sharp little B action pictures for more than 20 years. He was responsible for the great \"Truck Stop Women\" from the '70s and several other little gems; unfortunately, he's also responsible for this dud. It's a shame to see the talented--and still smoking hot--Theresa Russell wasted yet again, but she's still the best actor in this picture. Eric Roberts shows up for a while, does his Eric Roberts thing, then goes away, a not altogether unwelcome occurrence in a picture with Eric Roberts in it. Frank Stallone actually isn't bad, which should give you an idea of how truly pathetic this picture is. As has been mentioned by other reviewers, the action scenes--which is the reason a picture like this gets made in the first place--are almost completely illogical and unrealistic, in addition to being somewhat inept. Other than some \"vintage\" clothes and a few old cars, there's no feel whatever for the 1930s, the era in which this film is set. A by-the-numbers script with irritating lapses in logic and little historical accuracy--this isn't a documentary, of course, but the filmmakers could have at least TRIED for a little authenticity--and performances that range from grade B to grade school relegate this cheap little quickie to the 4:00 a.m. Sunday slot on HBO, which is just where I saw it. \n",
            "\n",
            "label: pos \n",
            "text: 'Thriller' remains the greatest of the pop music promos to have a plot, great visuals, and a tip-top song to wrap the film around. Michael Jackson was at the top of the tree at this time (and not so altered in his plastic surgery regime for it to matter). Here he is in good form - the song is terrific, he leads the zombies in dance like no other. Ola Ray plays the girl who watches with incredulity as her sweet boyfriend (Jackson, natch) turns into a werewolf! Then to the pulsing rhythms of the opening line 'It's close to midnight', he stomps around the graveyard with the other zombies and creatures of the night. The crowning glory of all this is the fruity voice of the great horror star Vincent Price speaking in the middle of the record. Terrific. \n",
            "\n",
            "label: pos \n",
            "text: I loved this film, which I have just seen at the Philadelphia film festival. In March 2005 I went to India with 2 friends, and this movie was very real. I related to everything, and savored every moment. The characters are believable, the story poignant and the ending realistic, but not sentimental. I also enjoyed the discussion with the director after the showing. This movie shows very well the blending, but not complete mixing of 2 worlds (East and West). The supporting cast was wonderful, depicting the life a tourist encounters in India quite realistically. The humor is subtle, and at times dry, and this makes it all the more realistic, as it is woven into the daily escapades of the characters. It is so easy to identify with each of the situations portrayed. \n",
            "\n",
            "label: pos \n",
            "text: Eisenstein wasn't just one of the greatest soviet,russian, films'directors, but one of the great masters of the cinema, among Griffith, Murnau, Ford, Hitchcock, Welles, and others. One of the greatest things in all his films was the edition, very personal, and in this movie is exceptional. This was his first sound movie and the use of the musical score by great russian composer Serge Prokofiev in the sequence of the battle is a perfect contrast between music and image. Watching this film is like taking a class or lesson in Cinema, something that no many film directors can afford. I never get tired of watch REAL CLASSICS like this film. I hope that in near future more people will recognize a great work of art. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti6xr4TXGwab"
      },
      "source": [
        "## Preprocessing: Tfidf Vectorizer \n",
        "\n",
        "Since we are dealing with text data we need to transform it to format a basic SVM can handle. For that purpose I use sklearn TfidfVectorizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNbQMIaSzdU5"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=50000)\n",
        "fm_train = vectorizer.fit_transform(X_train)\n",
        "fm_test = vectorizer.transform(X_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuuJbYX_1-8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b4928a-effe-4872-f97c-74ee672a6fef"
      },
      "source": [
        "# input data size is limited by the vectorizer\n",
        "\n",
        "print(f\"We have {fm_train.shape[0]} rows and {fm_train.shape[1]} columns in the training data\")\n",
        "print(f\"And {fm_test.shape[0]} rows and {fm_test.shape[1]} columns in the training data\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 16750 rows and 50000 columns in the training data\n",
            "And 8250 rows and 50000 columns in the training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPXtqJ9ELYK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240b2523-848a-41cd-a2bd-6ed1cf47330e"
      },
      "source": [
        "# type of the input data is scipy.sparse.csr.csr_matrix\n",
        "print(type(fm_train))\n",
        "\n",
        "# What does it mean? It looks like this:\n",
        "print(fm_train[0:2:])\n",
        "\n",
        "# Each row of the sparse matrix contains the indices of the tfidf matrix \n",
        "#(for example (0, 12247) and the tfidf weight).\n",
        "# The row index is the document index (the number of the review) and the column \n",
        "# index is for the token "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "  (0, 19149)\t0.0992167195479343\n",
            "  (0, 14993)\t0.1600074301975623\n",
            "  (0, 29632)\t0.1302782985303928\n",
            "  (0, 19580)\t0.09199589365638809\n",
            "  (0, 2929)\t0.06439606350971956\n",
            "  (0, 29823)\t0.03404083356641864\n",
            "  (0, 11371)\t0.18630697660858034\n",
            "  (0, 43526)\t0.02320659273763631\n",
            "  (0, 20011)\t0.0958430234346515\n",
            "  (0, 23328)\t0.04094448417345909\n",
            "  (0, 15457)\t0.18887982576393136\n",
            "  (0, 36611)\t0.13084570628890424\n",
            "  (0, 30341)\t0.07121103101501734\n",
            "  (0, 42927)\t0.05008940481189398\n",
            "  (0, 48127)\t0.050550407010130714\n",
            "  (0, 9653)\t0.05680058014095944\n",
            "  (0, 26387)\t0.07806913559755978\n",
            "  (0, 30087)\t0.050779703724875246\n",
            "  (0, 14015)\t0.08585788901442588\n",
            "  (0, 22698)\t0.024397430663128016\n",
            "  (0, 20249)\t0.041623988044122325\n",
            "  (0, 23813)\t0.09670902396076592\n",
            "  (0, 2377)\t0.07180640948442756\n",
            "  (0, 11906)\t0.12529222327194778\n",
            "  (0, 38511)\t0.14799223424459568\n",
            "  :\t:\n",
            "  (1, 2377)\t0.04284990707141776\n",
            "  (1, 25068)\t0.046441925541656805\n",
            "  (1, 20712)\t0.024423972317116\n",
            "  (1, 21758)\t0.029512774836013816\n",
            "  (1, 19814)\t0.029186723043887794\n",
            "  (1, 43009)\t0.07221255691212998\n",
            "  (1, 48780)\t0.0358011043898748\n",
            "  (1, 48089)\t0.028847300341225478\n",
            "  (1, 46875)\t0.08154352214749117\n",
            "  (1, 13383)\t0.06970186931846209\n",
            "  (1, 28401)\t0.09764872616730359\n",
            "  (1, 28237)\t0.053514786095939404\n",
            "  (1, 48389)\t0.023646005502997225\n",
            "  (1, 5897)\t0.035144627535362946\n",
            "  (1, 20192)\t0.10152720102330956\n",
            "  (1, 22743)\t0.13092060772470276\n",
            "  (1, 42889)\t0.06359243808492747\n",
            "  (1, 47784)\t0.056338439726578364\n",
            "  (1, 43075)\t0.02862409264653082\n",
            "  (1, 5252)\t0.08211561046960535\n",
            "  (1, 11613)\t0.06396752656933358\n",
            "  (1, 42977)\t0.04805475477762938\n",
            "  (1, 40481)\t0.029208233307294156\n",
            "  (1, 42903)\t0.19732236868733527\n",
            "  (1, 29677)\t0.12346323812014495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RonQzooNynZJ",
        "outputId": "830b049a-cd5b-44d4-c6d9-f74191a91377"
      },
      "source": [
        "# Columns are mostly empty because most words in the vocabulary do not appear in every sentence\n",
        "# This is why sparse format is used instead of dense:\n",
        "print(fm_train[0:2].todense())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfeZp_3C0d3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1558d774-adeb-4b56-fae9-81ddc311531c"
      },
      "source": [
        "# In the vectorizer vocabulary we have the original words as key value pairs, where the \n",
        "# word is the key and matrix index is the value:\n",
        "\n",
        "for (idx, item) in enumerate(vectorizer.vocabulary_.items()):\n",
        "  print(\"Key:\", item[0], \"\\tValue:\",item[1])\n",
        "  if (idx==8):\n",
        "    break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Key: felt \tValue: 16799\n",
            "Key: cheated \tValue: 9757\n",
            "Key: out \tValue: 30112\n",
            "Key: of \tValue: 29677\n",
            "Key: knowing \tValue: 23807\n",
            "Key: the \tValue: 42903\n",
            "Key: whole \tValue: 48395\n",
            "Key: story \tValue: 40481\n",
            "Key: while \tValue: 48276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rihQsGsqKqOg"
      },
      "source": [
        "## Finding the best model with GridSearch Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "419GUOsFS_Zu"
      },
      "source": [
        "The model can be trained by exploring the hyperparameters one by one or with nested for-loops. It will how ever become a frustrating task to keep up with the hyperparameter combinations and obtained performence values. A more systematic way to do this is by using GridSearch (GS). \n",
        "\n",
        "GridSearchCV allows us set grid (or multiple vectors) of hypermarameters to try with. The idea is to try and find a sweet spot (best performance measure) by adjusting the grid. K-fold CV also introduces a new hyperparameter, which affects the training results, namely the number of folds.\n",
        "\n",
        "GS uses K-fold Cross Validation (CV) to find the best performing model. Depending on the algorithm and chosen parameters the data is \"folded\" (divided into subsets) n times and each of these folds is used once for testing while n-1 folds are used for training. Cross validation is also useful when the data set size is limited and we would like to \"eat the cake and keep it\".\n",
        "\n",
        "For the task I have chosen Linear Support Vector Classifier. When classifying multiple classes and the number of classes in *n* LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training *n* models ([Scikit-learn](https://scikit-learn.org/stable/modules/svm.html#svm-classification)). At prediction time all the classi\ffers \"vote\", and item will be assigned to class with the lowest cost. Other possible models for text classification problem are for example K-Nearest-Neighbors and Multinomial Naive Bayes. Also classifiers can be compared with GS.\n",
        "\n",
        "A simple pipeline is built for both the preprocessor (vectorizer) and the classifier so that we are able to find the best hyperparameters for both of them at once.\n",
        "\n",
        "Sources:\n",
        "\n",
        "[GridSearchCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
        "\n",
        "[GridSearchCV example 1](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html) \n",
        "\n",
        "[GridSearchCV example 2](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html) \n",
        "\n",
        "[SVM documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
        "\n",
        "[GridSearchCV scoring parameters](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95XwkuFwVS8x"
      },
      "source": [
        "#### TfidfVectorizer with SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z5Iu2P5MLSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89e948e0-8191-49e3-fbb9-41f13ac52e27"
      },
      "source": [
        "costs = np.logspace(-1, 1, num=5, endpoint = False)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vec', TfidfVectorizer()),\n",
        "    ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    #'vec__binary': (True, False), # Previous runs revealed this does not seem to matter\n",
        "    'vec__max_features': (10000, 30000),\n",
        "    'vec__ngram_range': ((1, 1), (1, 2), (1, 3)),  \n",
        "    'clf__C': (costs), \n",
        "}\n",
        "# find the best parameters for both the feature extraction and the classifier\n",
        "print(\"Running grid search...\")\n",
        "# n_jobs=-1: use as many cores as possible\n",
        "# cv=3: three folds (this is kind of little but it speeds things up)\n",
        "gridsearch = GridSearchCV(pipeline, parameters, cv=3, verbose=1, n_jobs=-1)\n",
        "gridsearch.fit(X_train, y_train)\n",
        "print(\"Grid search done!\")\n",
        "print()\n",
        "print(f\"Best score: {gridsearch.best_score_:0.2}\")\n",
        "print(\"Best of the observed hyperparameters:\")\n",
        "best_parameters = gridsearch.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "      print(f\"{param_name}: {best_parameters[param_name]}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running grid search...\n",
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 12.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Grid search done!\n",
            "\n",
            "Best score: 0.9\n",
            "Best of the observed hyperparameters:\n",
            "clf__C: 0.6309573444801934\n",
            "vec__max_features: 30000\n",
            "vec__ngram_range: (1, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPNkptft9ixO"
      },
      "source": [
        "So was the selected model clearly the best?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HbVgsom5Wir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05f994e-baf8-4b24-9e84-3d722183c3e8"
      },
      "source": [
        "# set column visibility in pandas df\n",
        "pd.set_option(\"max_colwidth\", None)\n",
        "\n",
        "# extract mean score for each parameter combination trained\n",
        "means = gridsearch.cv_results_['mean_test_score'] \n",
        "\n",
        "GSCV_results = pd.DataFrame(list(zip(means, gridsearch.cv_results_['params'])), \n",
        "               columns =['Score', 'Parameters']) \n",
        "# sort by the score\n",
        "GSCV_results.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
        "print(GSCV_results.head(7))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Score                                                                              Parameters\n",
            "16  0.902269  {'clf__C': 0.6309573444801934, 'vec__max_features': 30000, 'vec__ngram_range': (1, 2)}\n",
            "17  0.901552  {'clf__C': 0.6309573444801934, 'vec__max_features': 30000, 'vec__ngram_range': (1, 3)}\n",
            "10  0.900060   {'clf__C': 0.251188643150958, 'vec__max_features': 30000, 'vec__ngram_range': (1, 2)}\n",
            "22  0.899403   {'clf__C': 1.584893192461114, 'vec__max_features': 30000, 'vec__ngram_range': (1, 2)}\n",
            "11  0.898388   {'clf__C': 0.251188643150958, 'vec__max_features': 30000, 'vec__ngram_range': (1, 3)}\n",
            "23  0.895701   {'clf__C': 1.584893192461114, 'vec__max_features': 30000, 'vec__ngram_range': (1, 3)}\n",
            "7   0.894687   {'clf__C': 0.251188643150958, 'vec__max_features': 10000, 'vec__ngram_range': (1, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY62tzzG-zVf"
      },
      "source": [
        "Not! This seemd to be a tight race."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJZ2_WEGBCSR"
      },
      "source": [
        "##### Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq6i4BgvTJ4l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "3ab316be-7295-4d09-f93a-7e1cf4dfdbd9"
      },
      "source": [
        "# classifier=grid_search.best_estimator_\n",
        "# classifier.fit(feature_matrix_train, train_label)\n",
        "\n",
        "predictions = gridsearch.predict(X_test)\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "# conf = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "print(f\"Test accuracy: {acc:0.2f}\")\n",
        "print()\n",
        "# note here we have to feed in the test data not feature matrix since esitimator is a pipeline, not a classifier!\n",
        "plot_confusion_matrix(gridsearch.best_estimator_, X_test, y_test, cmap='Greens', values_format='d')  \n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.show()\n",
        "\n",
        "plot_confusion_matrix(gridsearch.best_estimator_, X_test, y_test, cmap='Blues', normalize='true')  \n",
        "plt.title(\"Normalized confusion matrix\")\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.91\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cAuuKjW6yrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1326dd20-d618-42ba-c2ab-2272fd6b62e6"
      },
      "source": [
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.92      0.89      0.91      4142\n",
            "         pos       0.90      0.92      0.91      4108\n",
            "\n",
            "    accuracy                           0.91      8250\n",
            "   macro avg       0.91      0.91      0.91      8250\n",
            "weighted avg       0.91      0.91      0.91      8250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuSXvfofI_M1"
      },
      "source": [
        "The model handled both of the classes well. This can be seen from the confusion matrix and from the classification report where precision and recall are in balance for both os the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0eZsazk14xb"
      },
      "source": [
        "#### Simple Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1nyeGt9wES3"
      },
      "source": [
        "# redo the split to overwrite old variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.33)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEznt0u2KU3"
      },
      "source": [
        "To build a NN we need to\n",
        "\n",
        "1.   turn numpy vectors to tensors\n",
        "2.   know the shape of input layer (number of features)\n",
        "3.   know the shape of output layer (number of classes)\n",
        "\n",
        "TfidfVectorizer gives the 2nd one and LabelEncoder (for example) the 3rd one. (or just len(set(train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6vARoH6wv3L"
      },
      "source": [
        "# 1) np vectors to TF tensors\n",
        "\n",
        "def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "    coo = X.tocoo()\n",
        "    indices = np.mat([coo.row, coo.col]).transpose()\n",
        "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utEFKUq0ModO"
      },
      "source": [
        "Since vectorizer affects the shape of the NN, we do not optimize it as a part of the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YooLapRH3ABl",
        "outputId": "1757acf4-b64f-4ecb-d87e-8db992866a23"
      },
      "source": [
        "# 2) size of input \n",
        "vectorizer = TfidfVectorizer(max_features=100000)\n",
        "\n",
        "ft_matrix = vectorizer.fit_transform(X_train)\n",
        "ft_matrix.shape # so we need the second dimension for building the nn\n",
        "input_size = ft_matrix.shape[1]\n",
        "print(\"Size of input layer:\", input_size)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input layer: 63066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HevFwocVx2l6",
        "outputId": "2b1be34d-f0bb-4652-a4e0-13fdcd4e1db8"
      },
      "source": [
        "# 3) size_of_output_layer\n",
        "# use encoded labels when fitting the model and for testing\n",
        "\n",
        "label_encoder = LabelEncoder() #Turns class labels into integers\n",
        "class_numbers_train = label_encoder.fit_transform(y_train)\n",
        "\n",
        "print(\"class_numbers shape=\", class_numbers_train.shape)\n",
        "print(\"class labels\", label_encoder.classes_) #this will let us translate back from indices to labels\n",
        "\n",
        "output_size = len(label_encoder.classes_)\n",
        "print(\"Size of output layer:\", output_size)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_numbers shape= (16750,)\n",
            "class labels ['neg' 'pos']\n",
            "Size of output layer: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1GYj-1KZyJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57501cdd-919b-4f1c-c979-2286cf28db0e"
      },
      "source": [
        "def build_sequential_nn(input_size=100, output_size=2, hiddenlayer_size=200, drop_out= 0.3, learning_rate=0.001): \n",
        "  # let's make 200 default sixe of the hiddenlayer\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape = (input_size, )))\n",
        "  model.add(Dense(hiddenlayer_size, activation = \"relu\", ))\n",
        "  model.add(Dropout(rate=drop_out)) # Dropout regularizer to avoid over fitting\n",
        "  model.add(Dense(output_size, activation = \"softmax\"))\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = build_sequential_nn(input_size=input_size, output_size=output_size)\n",
        "model.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_489\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_978 (Dense)            (None, 200)               12613400  \n",
            "_________________________________________________________________\n",
            "dropout_489 (Dropout)        (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_979 (Dense)            (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 12,613,802\n",
            "Trainable params: 12,613,802\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbvQNJQmwpma",
        "outputId": "6918aa41-7032-4636-c854-76a0f4105322"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('trans', FunctionTransformer(convert_sparse_matrix_to_sparse_tensor)), # wrapper for custom function\n",
        "    ('clf', KerasClassifier(build_fn=build_sequential_nn)), # wrapper for Keras model\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'clf__hiddenlayer_size': (200, 300), \n",
        "    'clf__input_size': ([input_size]), # GS sets ALL the params\n",
        "    'clf__output_size': ([output_size]),\n",
        "    'clf__batch_size': (64, 265),\n",
        "    'clf__drop_out': (0.2, 0.4), # regularizer to avoid over fitting\n",
        "    'clf__epochs': (2, 4), # do not use early stopping callback, number of epochs is best treated as a hyper parameter: https://stackoverflow.com/questions/48127550/early-stopping-with-keras-and-sklearn-gridsearchcv-cross-validation\n",
        "    'clf__learning_rate': (0.001, 0.01) # 0.001 is default for Adam\n",
        "}\n",
        "\n",
        "t0=time.time()\n",
        "print(\"Running grid search...\")\n",
        "gridsearch = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=1)\n",
        "gridsearch.fit(ft_matrix, class_numbers_train)\n",
        "print()\n",
        "\n",
        "print(f\"Best score: {gridsearch.best_score_:0.2}\")\n",
        "\n",
        "print(\"Best of the observed hyperparameters:\")\n",
        "best_parameters = gridsearch.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "      print(f\"{param_name}: {best_parameters[param_name]}\")\n",
        "\n",
        "t1=time.time()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running grid search...\n",
            "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5054 - accuracy: 0.8178\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0973 - accuracy: 0.9759\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2370 - accuracy: 0.9012\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 5ms/step - loss: 0.5092 - accuracy: 0.7994\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0991 - accuracy: 0.9723\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2397 - accuracy: 0.9048\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5076 - accuracy: 0.7879\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1019 - accuracy: 0.9713\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.8964\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5088 - accuracy: 0.7881\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1008 - accuracy: 0.9712\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2513 - accuracy: 0.8955\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5114 - accuracy: 0.7611\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1041 - accuracy: 0.9720\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9000\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3973 - accuracy: 0.8092\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0478 - accuracy: 0.9841\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8854\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3949 - accuracy: 0.8120\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0416 - accuracy: 0.9874\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.8824\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4007 - accuracy: 0.7985\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0416 - accuracy: 0.9859\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8875\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3863 - accuracy: 0.8239\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0400 - accuracy: 0.9875\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.8731\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3911 - accuracy: 0.8122\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0407 - accuracy: 0.9883\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8907\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5009 - accuracy: 0.7814\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0888 - accuracy: 0.9770\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2432 - accuracy: 0.8982\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4869 - accuracy: 0.7954\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0821 - accuracy: 0.9779\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2659 - accuracy: 0.8884\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4870 - accuracy: 0.8040\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0831 - accuracy: 0.9770\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2545 - accuracy: 0.8994\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4919 - accuracy: 0.7922\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0859 - accuracy: 0.9742\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8940\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4955 - accuracy: 0.7674\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0817 - accuracy: 0.9780\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2593 - accuracy: 0.8931\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4031 - accuracy: 0.8079\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0475 - accuracy: 0.9844\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8887\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3996 - accuracy: 0.8195\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0455 - accuracy: 0.9852\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8875\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3914 - accuracy: 0.8162\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0420 - accuracy: 0.9869\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8884\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3845 - accuracy: 0.8117\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0411 - accuracy: 0.9878\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8815\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3871 - accuracy: 0.8141\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0431 - accuracy: 0.9854\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8925\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5158 - accuracy: 0.7762\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0976 - accuracy: 0.9751\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0311 - accuracy: 0.9951\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0105 - accuracy: 0.9994\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.8943\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5133 - accuracy: 0.7834\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1013 - accuracy: 0.9750\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0303 - accuracy: 0.9957\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0108 - accuracy: 0.9993\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2783 - accuracy: 0.8940\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5109 - accuracy: 0.7861\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1017 - accuracy: 0.9709\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0274 - accuracy: 0.9970\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0095 - accuracy: 0.9998\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8985\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5125 - accuracy: 0.7711\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1014 - accuracy: 0.9709\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0263 - accuracy: 0.9966\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0094 - accuracy: 0.9997\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8904\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5091 - accuracy: 0.8034\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0976 - accuracy: 0.9754\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0288 - accuracy: 0.9966\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0104 - accuracy: 0.9989\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.8946\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3897 - accuracy: 0.8196\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0443 - accuracy: 0.9872\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0067 - accuracy: 0.9984\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 7.4188e-04 - accuracy: 0.9999\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.8845\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3841 - accuracy: 0.8187\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0400 - accuracy: 0.9871\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0065 - accuracy: 0.9982\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 7.4928e-04 - accuracy: 0.9999\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.8875\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.3920 - accuracy: 0.8102\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0432 - accuracy: 0.9884\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0052 - accuracy: 0.9985\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 6.1145e-04 - accuracy: 0.9999\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5372 - accuracy: 0.8881\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3987 - accuracy: 0.8141\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0413 - accuracy: 0.9873\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0054 - accuracy: 0.9993\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 6.8001e-04 - accuracy: 1.0000\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.8776\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3883 - accuracy: 0.8112\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0407 - accuracy: 0.9873\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0046 - accuracy: 0.9991\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 6.7299e-04 - accuracy: 0.9998\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.8931\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4889 - accuracy: 0.7975\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0835 - accuracy: 0.9771\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0200 - accuracy: 0.9981\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0064 - accuracy: 0.9998\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2859 - accuracy: 0.8952\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4906 - accuracy: 0.7992\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0849 - accuracy: 0.9772\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0199 - accuracy: 0.9982\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0064 - accuracy: 0.9999\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8958\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4853 - accuracy: 0.7957\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0838 - accuracy: 0.9760\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0196 - accuracy: 0.9974\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0065 - accuracy: 0.9997\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8937\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4922 - accuracy: 0.7792\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0787 - accuracy: 0.9798\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0203 - accuracy: 0.9979\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0071 - accuracy: 0.9991\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3069 - accuracy: 0.8922\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4933 - accuracy: 0.8037\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0856 - accuracy: 0.9752\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0213 - accuracy: 0.9970\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0063 - accuracy: 0.9999\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2854 - accuracy: 0.8970\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3763 - accuracy: 0.8294\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0393 - accuracy: 0.9873\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0062 - accuracy: 0.9984\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 4.9561e-04 - accuracy: 1.0000\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.8818\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3871 - accuracy: 0.8104\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0419 - accuracy: 0.9861\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0039 - accuracy: 0.9993\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 4.0193e-04 - accuracy: 1.0000\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5151 - accuracy: 0.8878\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3883 - accuracy: 0.8207\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0415 - accuracy: 0.9882\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0060 - accuracy: 0.9985\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 5.1005e-04 - accuracy: 1.0000\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.8845\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3926 - accuracy: 0.8119\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0407 - accuracy: 0.9884\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0076 - accuracy: 0.9981\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 6.8094e-04 - accuracy: 1.0000\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.8776\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3911 - accuracy: 0.8137\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0401 - accuracy: 0.9892\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0047 - accuracy: 0.9990\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 6.6437e-04 - accuracy: 0.9999\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.8899\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5212 - accuracy: 0.7803\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.1187 - accuracy: 0.9658\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9048\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5255 - accuracy: 0.7689\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1188 - accuracy: 0.9668\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.9042\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5305 - accuracy: 0.7678\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.1246 - accuracy: 0.9615\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.9039\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5295 - accuracy: 0.7709\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.1183 - accuracy: 0.9660\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.8979\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5286 - accuracy: 0.7805\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1198 - accuracy: 0.9681\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.9021\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.3937 - accuracy: 0.8135\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0657 - accuracy: 0.9758\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3053 - accuracy: 0.8854\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3846 - accuracy: 0.8183\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0619 - accuracy: 0.9794\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8788\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3989 - accuracy: 0.8109\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0629 - accuracy: 0.9795\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8863\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3883 - accuracy: 0.8187\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0636 - accuracy: 0.9810\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8794\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3937 - accuracy: 0.8158\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0656 - accuracy: 0.9785\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8922\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5056 - accuracy: 0.8109\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1010 - accuracy: 0.9700\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.9018\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5037 - accuracy: 0.8067\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0983 - accuracy: 0.9731\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.8970\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5033 - accuracy: 0.7920\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1008 - accuracy: 0.9682\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2460 - accuracy: 0.9060\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4996 - accuracy: 0.7940\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0988 - accuracy: 0.9724\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.8910\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5021 - accuracy: 0.7840\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0981 - accuracy: 0.9727\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.9000\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4150 - accuracy: 0.7961\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0629 - accuracy: 0.9780\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.8854\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3906 - accuracy: 0.8113\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0604 - accuracy: 0.9809\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8842\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3848 - accuracy: 0.8203\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0591 - accuracy: 0.9813\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8788\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4029 - accuracy: 0.8032\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0643 - accuracy: 0.9796\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8803\n",
            "Epoch 1/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3900 - accuracy: 0.8182\n",
            "Epoch 2/2\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0614 - accuracy: 0.9791\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8869\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5229 - accuracy: 0.7974\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.1201 - accuracy: 0.9660\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0397 - accuracy: 0.9944\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0156 - accuracy: 0.9985\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.9000\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.5222 - accuracy: 0.7841\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.1134 - accuracy: 0.9690\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0394 - accuracy: 0.9948\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0149 - accuracy: 0.9984\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.8970\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5255 - accuracy: 0.7746\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.1190 - accuracy: 0.9646\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0388 - accuracy: 0.9930\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0158 - accuracy: 0.9983\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.8973\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5266 - accuracy: 0.7802\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.1174 - accuracy: 0.9675\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0382 - accuracy: 0.9941\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0151 - accuracy: 0.9990\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8925\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.5231 - accuracy: 0.7825\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.1152 - accuracy: 0.9670\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0403 - accuracy: 0.9933\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0173 - accuracy: 0.9981\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.8961\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3907 - accuracy: 0.8092\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0651 - accuracy: 0.9766\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0167 - accuracy: 0.9960\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9995\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.8803\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.3871 - accuracy: 0.8166\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0573 - accuracy: 0.9820\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0124 - accuracy: 0.9968\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9996\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.8851\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.4038 - accuracy: 0.8044\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0687 - accuracy: 0.9778\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0155 - accuracy: 0.9946\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9995\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.8899\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.3900 - accuracy: 0.8115\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0543 - accuracy: 0.9831\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0124 - accuracy: 0.9968\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.0019 - accuracy: 0.9996\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.8839\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4054 - accuracy: 0.8070\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0639 - accuracy: 0.9800\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0149 - accuracy: 0.9965\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9994\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.8913\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5033 - accuracy: 0.7943\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1016 - accuracy: 0.9716\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0319 - accuracy: 0.9945\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0106 - accuracy: 0.9993\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2742 - accuracy: 0.8982\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5110 - accuracy: 0.7774\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1030 - accuracy: 0.9699\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0301 - accuracy: 0.9953\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0109 - accuracy: 0.9988\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.8964\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5087 - accuracy: 0.7848\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1049 - accuracy: 0.9686\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0306 - accuracy: 0.9959\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0102 - accuracy: 0.9996\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.8964\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5094 - accuracy: 0.7727\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1028 - accuracy: 0.9736\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0298 - accuracy: 0.9954\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0109 - accuracy: 0.9991\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8890\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5041 - accuracy: 0.7735\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1021 - accuracy: 0.9686\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0299 - accuracy: 0.9952\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0100 - accuracy: 0.9993\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8976\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4055 - accuracy: 0.8060\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0661 - accuracy: 0.9761\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0155 - accuracy: 0.9959\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0023 - accuracy: 0.9997\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.8833\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4101 - accuracy: 0.7993\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0654 - accuracy: 0.9796\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0154 - accuracy: 0.9954\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0029 - accuracy: 0.9992\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.8755\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3800 - accuracy: 0.8169\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0598 - accuracy: 0.9799\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0104 - accuracy: 0.9975\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0014 - accuracy: 0.9999\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.8907\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4020 - accuracy: 0.8066\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0601 - accuracy: 0.9819\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0111 - accuracy: 0.9971\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0022 - accuracy: 0.9996\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.8701\n",
            "Epoch 1/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3917 - accuracy: 0.8163\n",
            "Epoch 2/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0644 - accuracy: 0.9803\n",
            "Epoch 3/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0113 - accuracy: 0.9974\n",
            "Epoch 4/4\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0025 - accuracy: 0.9994\n",
            "53/53 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.8899\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6166 - accuracy: 0.7392\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2548 - accuracy: 0.9351\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2547 - accuracy: 0.9009\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6175 - accuracy: 0.7454\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2495 - accuracy: 0.9422\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2554 - accuracy: 0.8985\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6181 - accuracy: 0.7239\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2532 - accuracy: 0.9408\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.8988\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6160 - accuracy: 0.7732\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2533 - accuracy: 0.9393\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.8922\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6156 - accuracy: 0.7821\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2517 - accuracy: 0.9399\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2553 - accuracy: 0.9024\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4518 - accuracy: 0.7857\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0514 - accuracy: 0.9877\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2970 - accuracy: 0.8884\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4326 - accuracy: 0.7878\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.9895\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8940\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4460 - accuracy: 0.7866\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0463 - accuracy: 0.9886\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3060 - accuracy: 0.8985\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4461 - accuracy: 0.7735\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0442 - accuracy: 0.9892\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3257 - accuracy: 0.8913\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4281 - accuracy: 0.7901\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0475 - accuracy: 0.9893\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8931\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.5999 - accuracy: 0.7659\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9502\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9021\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6020 - accuracy: 0.7330\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2079 - accuracy: 0.9455\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2405 - accuracy: 0.9036\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.6017 - accuracy: 0.7306\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2089 - accuracy: 0.9471\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2432 - accuracy: 0.9015\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6040 - accuracy: 0.7756\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2108 - accuracy: 0.9451\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2513 - accuracy: 0.8970\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6017 - accuracy: 0.7579\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2106 - accuracy: 0.9460\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2413 - accuracy: 0.9006\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4498 - accuracy: 0.7777\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9869\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8896\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4237 - accuracy: 0.7900\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0438 - accuracy: 0.9896\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3411 - accuracy: 0.8934\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4173 - accuracy: 0.7983\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9926\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3503 - accuracy: 0.8907\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4236 - accuracy: 0.7984\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0434 - accuracy: 0.9893\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.8833\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4315 - accuracy: 0.7789\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0459 - accuracy: 0.9886\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8896\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6181 - accuracy: 0.7302\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2536 - accuracy: 0.9358\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1150 - accuracy: 0.9781\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0593 - accuracy: 0.9928\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9039\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6203 - accuracy: 0.7367\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2515 - accuracy: 0.9416\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1122 - accuracy: 0.9798\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0570 - accuracy: 0.9946\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2347 - accuracy: 0.9042\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 8ms/step - loss: 0.6113 - accuracy: 0.7777\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2450 - accuracy: 0.9431\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1099 - accuracy: 0.9808\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0557 - accuracy: 0.9929\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.9021\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6186 - accuracy: 0.7109\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2591 - accuracy: 0.9339\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1129 - accuracy: 0.9779\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0555 - accuracy: 0.9939\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2485 - accuracy: 0.8952\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6131 - accuracy: 0.7799\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2441 - accuracy: 0.9394\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1111 - accuracy: 0.9802\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0565 - accuracy: 0.9934\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2348 - accuracy: 0.9009\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4285 - accuracy: 0.8182\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0447 - accuracy: 0.9901\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9990\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 0.9998\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3904 - accuracy: 0.8854\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4479 - accuracy: 0.7872\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0502 - accuracy: 0.9863\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9992\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3904 - accuracy: 0.8913\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4339 - accuracy: 0.7936\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0488 - accuracy: 0.9880\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9990\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0012 - accuracy: 0.9999\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4129 - accuracy: 0.8907\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4617 - accuracy: 0.7763\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0507 - accuracy: 0.9870\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0069 - accuracy: 0.9990\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 0.9999\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8851\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4366 - accuracy: 0.7826\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0478 - accuracy: 0.9891\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9988\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3916 - accuracy: 0.8919\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.5989 - accuracy: 0.7792\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.2031 - accuracy: 0.9504\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0847 - accuracy: 0.9844\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0387 - accuracy: 0.9961\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.9009\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.6053 - accuracy: 0.7166\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2152 - accuracy: 0.9437\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.0860 - accuracy: 0.9844\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0399 - accuracy: 0.9958\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2411 - accuracy: 0.9006\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.6008 - accuracy: 0.7507\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2073 - accuracy: 0.9449\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.0839 - accuracy: 0.9858\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0393 - accuracy: 0.9961\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2436 - accuracy: 0.9021\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6079 - accuracy: 0.7185\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2114 - accuracy: 0.9481\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0830 - accuracy: 0.9847\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0404 - accuracy: 0.9962\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2535 - accuracy: 0.8937\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.6057 - accuracy: 0.7719\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.2136 - accuracy: 0.9442\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0849 - accuracy: 0.9862\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0404 - accuracy: 0.9959\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2412 - accuracy: 0.8988\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4391 - accuracy: 0.7838\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0526 - accuracy: 0.9867\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0070 - accuracy: 0.9990\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9999\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3870 - accuracy: 0.8904\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4266 - accuracy: 0.7839\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0460 - accuracy: 0.9892\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0062 - accuracy: 0.9990\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4017 - accuracy: 0.8916\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4258 - accuracy: 0.7914\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9893\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0044 - accuracy: 0.9993\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9998\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4397 - accuracy: 0.8893\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4271 - accuracy: 0.7857\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0464 - accuracy: 0.9900\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0054 - accuracy: 0.9986\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 9.3022e-04 - accuracy: 1.0000\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4371 - accuracy: 0.8833\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4212 - accuracy: 0.8078\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0481 - accuracy: 0.9899\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.0046 - accuracy: 0.9996\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3862 - accuracy: 0.8910\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6225 - accuracy: 0.7460\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2722 - accuracy: 0.9321\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.9045\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6257 - accuracy: 0.7168\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2756 - accuracy: 0.9343\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2617 - accuracy: 0.8994\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6291 - accuracy: 0.7342\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2810 - accuracy: 0.9259\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2638 - accuracy: 0.8973\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6297 - accuracy: 0.7020\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2785 - accuracy: 0.9307\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2678 - accuracy: 0.8955\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6253 - accuracy: 0.7083\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2770 - accuracy: 0.9276\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2672 - accuracy: 0.8961\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4442 - accuracy: 0.7827\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0607 - accuracy: 0.9844\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2965 - accuracy: 0.8893\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4431 - accuracy: 0.7851\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0607 - accuracy: 0.9833\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8896\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4372 - accuracy: 0.7874\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0611 - accuracy: 0.9826\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8970\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4375 - accuracy: 0.7865\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0579 - accuracy: 0.9848\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3573 - accuracy: 0.8851\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4391 - accuracy: 0.7978\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0595 - accuracy: 0.9849\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3106 - accuracy: 0.8961\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6192 - accuracy: 0.6919\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.2492 - accuracy: 0.9308\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2445 - accuracy: 0.9054\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.6119 - accuracy: 0.7413\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 10ms/step - loss: 0.2298 - accuracy: 0.9411\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2456 - accuracy: 0.9033\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6067 - accuracy: 0.7667\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2299 - accuracy: 0.9393\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2458 - accuracy: 0.9003\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6096 - accuracy: 0.7128\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.2243 - accuracy: 0.9450\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2511 - accuracy: 0.8967\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6092 - accuracy: 0.7620\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.2290 - accuracy: 0.9452\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2466 - accuracy: 0.9030\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4340 - accuracy: 0.7893\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0563 - accuracy: 0.9851\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3104 - accuracy: 0.8916\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4189 - accuracy: 0.8164\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0556 - accuracy: 0.9857\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3299 - accuracy: 0.8955\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4294 - accuracy: 0.7993\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 10ms/step - loss: 0.0541 - accuracy: 0.9851\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.8940\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4421 - accuracy: 0.7852\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0569 - accuracy: 0.9836\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3359 - accuracy: 0.8878\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4300 - accuracy: 0.7861\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 10ms/step - loss: 0.0554 - accuracy: 0.9856\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3112 - accuracy: 0.8922\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6208 - accuracy: 0.7766\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2725 - accuracy: 0.9305\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1309 - accuracy: 0.9714\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0679 - accuracy: 0.9913\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2310 - accuracy: 0.9066\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6189 - accuracy: 0.7660\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2715 - accuracy: 0.9328\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1304 - accuracy: 0.9716\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0683 - accuracy: 0.9894\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2319 - accuracy: 0.9036\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6253 - accuracy: 0.7423\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2788 - accuracy: 0.9314\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1323 - accuracy: 0.9679\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0717 - accuracy: 0.9894\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.2339 - accuracy: 0.9036\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6220 - accuracy: 0.7710\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2781 - accuracy: 0.9276\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.1323 - accuracy: 0.9737\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0701 - accuracy: 0.9896\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2434 - accuracy: 0.8967\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.6304 - accuracy: 0.7391\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.2868 - accuracy: 0.9308\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.1357 - accuracy: 0.9709\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0760 - accuracy: 0.9874\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2315 - accuracy: 0.9021\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 8ms/step - loss: 0.4436 - accuracy: 0.7842\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0613 - accuracy: 0.9830\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0114 - accuracy: 0.9975\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 0.9997\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3763 - accuracy: 0.8937\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4300 - accuracy: 0.8034\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0568 - accuracy: 0.9850\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0147 - accuracy: 0.9970\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0020 - accuracy: 0.9999\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4037 - accuracy: 0.8881\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4363 - accuracy: 0.8003\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0571 - accuracy: 0.9838\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9985\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4331 - accuracy: 0.8910\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.4486 - accuracy: 0.7823\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0626 - accuracy: 0.9841\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0105 - accuracy: 0.9982\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 0.9996\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4196 - accuracy: 0.8779\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 8ms/step - loss: 0.4478 - accuracy: 0.7744\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0633 - accuracy: 0.9817\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0097 - accuracy: 0.9988\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0027 - accuracy: 0.9997\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.3899 - accuracy: 0.8901\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.6104 - accuracy: 0.7657\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.2323 - accuracy: 0.9397\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.1020 - accuracy: 0.9794\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0539 - accuracy: 0.9929\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2308 - accuracy: 0.9054\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.6132 - accuracy: 0.7543\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.2359 - accuracy: 0.9399\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.1047 - accuracy: 0.9796\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0507 - accuracy: 0.9937\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2345 - accuracy: 0.9021\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6085 - accuracy: 0.7605\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.2278 - accuracy: 0.9406\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.1030 - accuracy: 0.9779\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0493 - accuracy: 0.9940\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2398 - accuracy: 0.9051\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.6166 - accuracy: 0.7115\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.2404 - accuracy: 0.9380\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.1039 - accuracy: 0.9777\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0512 - accuracy: 0.9941\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2506 - accuracy: 0.8958\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.6135 - accuracy: 0.7089\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.2362 - accuracy: 0.9386\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.1037 - accuracy: 0.9799\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.0527 - accuracy: 0.9932\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2332 - accuracy: 0.9006\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4364 - accuracy: 0.7843\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0570 - accuracy: 0.9855\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.0095 - accuracy: 0.9983\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9999\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.8890\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4269 - accuracy: 0.8033\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0525 - accuracy: 0.9854\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0094 - accuracy: 0.9977\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4254 - accuracy: 0.8869\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4284 - accuracy: 0.7966\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0617 - accuracy: 0.9822\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9980\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4172 - accuracy: 0.8928\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 9ms/step - loss: 0.4302 - accuracy: 0.7878\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0562 - accuracy: 0.9859\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0107 - accuracy: 0.9972\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9995\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.8812\n",
            "Epoch 1/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.4447 - accuracy: 0.7902\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0651 - accuracy: 0.9835\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0094 - accuracy: 0.9981\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.0021 - accuracy: 0.9998\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4137 - accuracy: 0.8922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed:  9.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "262/262 [==============================] - 2s 5ms/step - loss: 0.4910 - accuracy: 0.7975\n",
            "Epoch 2/2\n",
            "262/262 [==============================] - 1s 5ms/step - loss: 0.1070 - accuracy: 0.9687\n",
            "\n",
            "Best score: 0.9\n",
            "Best of the observed hyperparameters:\n",
            "clf__batch_size: 64\n",
            "clf__drop_out: 0.4\n",
            "clf__epochs: 2\n",
            "clf__hiddenlayer_size: 200\n",
            "clf__input_size: 63066\n",
            "clf__learning_rate: 0.001\n",
            "clf__output_size: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VFeq0r0qxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60730e18-719c-480d-f182-0cef39187c31"
      },
      "source": [
        "print(f\"Time elapsed {(t1-t0)/60:0.3} minutes\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed 9.72 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzuE9_f_5HjR",
        "outputId": "f674ca0e-293d-4289-f34f-9a9a04894cd4"
      },
      "source": [
        "means = gridsearch.cv_results_['mean_test_score'] \n",
        "\n",
        "GSCV_results = pd.DataFrame(list(zip(means, gridsearch.cv_results_['params'])), \n",
        "               columns =['Score', 'Parameters']) \n",
        "# sort by the score\n",
        "GSCV_results.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
        "print(GSCV_results.head(7))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Score                                                                                                                                                                    Parameters\n",
            "8   0.902567   {'clf__batch_size': 64, 'clf__drop_out': 0.4, 'clf__epochs': 2, 'clf__hiddenlayer_size': 200, 'clf__input_size': 63066, 'clf__learning_rate': 0.001, 'clf__output_size': 2}\n",
            "28  0.902507  {'clf__batch_size': 265, 'clf__drop_out': 0.4, 'clf__epochs': 4, 'clf__hiddenlayer_size': 200, 'clf__input_size': 63066, 'clf__learning_rate': 0.001, 'clf__output_size': 2}\n",
            "30  0.901791  {'clf__batch_size': 265, 'clf__drop_out': 0.4, 'clf__epochs': 4, 'clf__hiddenlayer_size': 300, 'clf__input_size': 63066, 'clf__learning_rate': 0.001, 'clf__output_size': 2}\n",
            "26  0.901731  {'clf__batch_size': 265, 'clf__drop_out': 0.4, 'clf__epochs': 2, 'clf__hiddenlayer_size': 300, 'clf__input_size': 63066, 'clf__learning_rate': 0.001, 'clf__output_size': 2}\n",
            "20  0.901254  {'clf__batch_size': 265, 'clf__drop_out': 0.2, 'clf__epochs': 4, 'clf__hiddenlayer_size': 200, 'clf__input_size': 63066, 'clf__learning_rate': 0.001, 'clf__output_size': 2}\n",
            "18  0.900955  {'clf__batch_size': 265, 'clf__drop_out': 0.2, 'clf__epochs': 2, 'clf__hiddenlayer_size': 300, 'clf__input_size': 63066, 'clf__learning_rate': 0.001, 'clf__output_size': 2}\n",
            "0   0.899582   {'clf__batch_size': 64, 'clf__drop_out': 0.2, 'clf__epochs': 2, 'clf__hiddenlayer_size': 200, 'clf__input_size': 63066, 'clf__learning_rate': 0.001, 'clf__output_size': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09lZrrzsOBj2"
      },
      "source": [
        "Accuracy scores are rather high from epoch to epoch excluding the first one. A more regularized model seems to be better, since models with less hidden layer neurons and higher drop out rate are ranked higher. Smaller learning rate dominates the bigger one. It must be noted, that differences between the model performances are minimal to say anything certain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-SCnMQ_GXtC"
      },
      "source": [
        "##### Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nh6sXJQS4o_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "8b2e5d6a-b4f4-44a4-ee6b-db5fa95beb5e"
      },
      "source": [
        "# prepare test data\n",
        "ftm_test=vectorizer.transform(X_test) # model needs to be Sequential for predicting\n",
        "class_numbers_test = label_encoder.transform(y_test)\n",
        "\n",
        "# predict\n",
        "raw_predictions = gridsearch.predict(ftm_test)\n",
        "predictions=label_encoder.inverse_transform(raw_predictions)\n",
        "\n",
        "# results\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "print(f\"Test accuracy: {acc:0.2f}\")\n",
        "print()\n",
        "cf_mat = tf.math.confusion_matrix(\n",
        "    class_numbers_test, raw_predictions, num_classes=None, weights=None\n",
        ")\n",
        "\n",
        "def plot_cf_matrix(mat):\n",
        "  sns.heatmap(mat, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Greens')\n",
        "  plt.title(\"Confusion matrix for test data\", fontsize = 16)\n",
        "  plt.ylabel(\"True class\", fontsize = 14)\n",
        "  plt.xlabel(\"Predicted class\", fontsize = 14)\n",
        "\n",
        "plot_cf_matrix(cf_mat)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.91\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_kDEsq0t25F",
        "outputId": "152a1501-69d5-4e42-e2d9-6d37c3c5682f"
      },
      "source": [
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.91      0.90      0.91      4146\n",
            "         pos       0.90      0.91      0.91      4104\n",
            "\n",
            "    accuracy                           0.91      8250\n",
            "   macro avg       0.91      0.91      0.91      8250\n",
            "weighted avg       0.91      0.91      0.91      8250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngco9xfDUiTg"
      },
      "source": [
        "Performance is rather similar with simpler SVM."
      ]
    }
  ]
}